2020-11-09 16:56:16.782835: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 16:56:16.790001: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 16:56:16.790131: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b19fa281c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 16:56:16.790149: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 16:56:16.791668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 16:56:16.808722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:56:16.808997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:56:16.811746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:56:16.814225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:56:16.814612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:56:16.817489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:56:16.818735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:56:16.824151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:56:16.827151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:56:16.827196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:56:16.976954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:56:16.977023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:56:16.977038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:56:16.981592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 16:56:16.983572: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b1a07a3680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 16:56:16.983601: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 16:56:16.985733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:56:16.985803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:56:16.985820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:56:16.985834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:56:16.987082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:56:16.987100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:56:16.987114: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:56:16.987128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:56:16.989937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:56:16.989968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:56:16.989978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:56:16.989987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:56:16.992852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 16:56:16.994583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:56:16.994636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:56:16.994653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:56:16.994667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:56:16.994681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:56:16.994694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:56:16.994708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:56:16.994721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:56:16.997501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:56:16.997526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:56:16.997536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:56:16.997545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:56:17.000352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F114
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
/glade/scratch/wchapman/Reforecast/F054
/glade/scratch/wchapman/Reforecast/F060
/glade/scratch/wchapman/Reforecast/F066
/glade/scratch/wchapman/Reforecast/F072
/glade/scratch/wchapman/Reforecast/F078
/glade/scratch/wchapman/Reforecast/F084
/glade/scratch/wchapman/Reforecast/F090
/glade/scratch/wchapman/Reforecast/F096
/glade/scratch/wchapman/Reforecast/F102
/glade/scratch/wchapman/Reforecast/F108
/glade/scratch/wchapman/Reforecast/F114
Training on
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc
['2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 157.7841, 154.4706
Mean and standard deviation for "p_sfc" = 985.2505, 61.8234
Mean and standard deviation for "u_tr_p" = 11.8700, 12.1231
Mean and standard deviation for "v_tr_p" = 1.8700, 13.3040
Mean and standard deviation for "Z_p" = 5580.6375, 200.5106
Mean and standard deviation for "IWV" = 13.4696, 7.8978
2020-11-09 16:56:19.260398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:56:19.260507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:56:19.260526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:56:19.260540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:56:19.260554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:56:19.260574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:56:19.260605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:56:19.260620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:56:19.263491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:56:19.265296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:56:19.265350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:56:19.265366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:56:19.265380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:56:19.265394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:56:19.265408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:56:19.265421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:56:19.265434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:56:19.268216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:56:19.268252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:56:19.268262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:56:19.268271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:56:19.271192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 16:56:20.897053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:56:21.898009: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 16:56:21.912999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 201.1545, 178.9212
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 364 samples, validate on 121 samples
Epoch 1/50
 50/364 [===>..........................] - ETA: 13s - loss: nan250/364 [===================>..........] - ETA: 1s - loss: nan 
Epoch 00001: val_loss did not improve from inf
364/364 [==============================] - 3s 8ms/sample - loss: nan - val_loss: nan
Epoch 2/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
364/364 [==============================] - 0s 290us/sample - loss: nan - val_loss: nan
Epoch 3/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
364/364 [==============================] - 0s 284us/sample - loss: nan - val_loss: nan
Epoch 4/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
364/364 [==============================] - 0s 284us/sample - loss: nan - val_loss: nan
Epoch 5/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
364/364 [==============================] - 0s 282us/sample - loss: nan - val_loss: nan
Epoch 6/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
364/364 [==============================] - 0s 283us/sample - loss: nan - val_loss: nan
Epoch 7/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
364/364 [==============================] - 0s 282us/sample - loss: nan - val_loss: nan
Epoch 8/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
364/364 [==============================] - 0s 283us/sample - loss: nan - val_loss: nan
Epoch 9/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
364/364 [==============================] - 0s 286us/sample - loss: nan - val_loss: nan
Epoch 10/50
 50/364 [===>..........................] - ETA: 0s - loss: nan300/364 [=======================>......] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
364/364 [==============================] - 0s 285us/sample - loss: nan - val_loss: nan
Epoch 00010: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 364 samples, validate on 121 samples
Epoch 1/50
 50/364 [===>..........................] - ETA: 4s - loss: 152.8061250/364 [===================>..........] - ETA: 0s - loss: 163.2675
Epoch 00001: val_loss improved from inf to 140.32060, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 1s 3ms/sample - loss: 161.1720 - val_loss: 140.3206
Epoch 2/50
 50/364 [===>..........................] - ETA: 0s - loss: 161.1502300/364 [=======================>......] - ETA: 0s - loss: 131.9416
Epoch 00002: val_loss improved from 140.32060 to 74.71749, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 365us/sample - loss: 121.3085 - val_loss: 74.7175
Epoch 3/50
 50/364 [===>..........................] - ETA: 0s - loss: 61.0536300/364 [=======================>......] - ETA: 0s - loss: 69.1311
Epoch 00003: val_loss improved from 74.71749 to 62.98108, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 359us/sample - loss: 68.6911 - val_loss: 62.9811
Epoch 4/50
 50/364 [===>..........................] - ETA: 0s - loss: 56.0685300/364 [=======================>......] - ETA: 0s - loss: 59.1737
Epoch 00004: val_loss did not improve from 62.98108
364/364 [==============================] - 0s 286us/sample - loss: 59.3010 - val_loss: 74.7545
Epoch 5/50
 50/364 [===>..........................] - ETA: 0s - loss: 55.8096300/364 [=======================>......] - ETA: 0s - loss: 54.6479
Epoch 00005: val_loss improved from 62.98108 to 60.92428, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 357us/sample - loss: 54.6137 - val_loss: 60.9243
Epoch 6/50
 50/364 [===>..........................] - ETA: 0s - loss: 55.3928300/364 [=======================>......] - ETA: 0s - loss: 50.8985
Epoch 00006: val_loss did not improve from 60.92428
364/364 [==============================] - 0s 286us/sample - loss: 50.8593 - val_loss: 73.7827
Epoch 7/50
 50/364 [===>..........................] - ETA: 0s - loss: 50.9036300/364 [=======================>......] - ETA: 0s - loss: 49.3005
Epoch 00007: val_loss did not improve from 60.92428

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
364/364 [==============================] - 0s 282us/sample - loss: 49.0541 - val_loss: 68.2093
Epoch 8/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.0511300/364 [=======================>......] - ETA: 0s - loss: 48.2552
Epoch 00008: val_loss did not improve from 60.92428
364/364 [==============================] - 0s 286us/sample - loss: 48.0860 - val_loss: 70.8800
Epoch 9/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.7278300/364 [=======================>......] - ETA: 0s - loss: 47.3260
Epoch 00009: val_loss did not improve from 60.92428

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
364/364 [==============================] - 0s 285us/sample - loss: 47.5825 - val_loss: 67.5361
Epoch 10/50
 50/364 [===>..........................] - ETA: 0s - loss: 49.8246300/364 [=======================>......] - ETA: 0s - loss: 47.2029
Epoch 00010: val_loss did not improve from 60.92428
364/364 [==============================] - 0s 285us/sample - loss: 47.4961 - val_loss: 66.5240
Epoch 11/50
 50/364 [===>..........................] - ETA: 0s - loss: 43.4184300/364 [=======================>......] - ETA: 0s - loss: 47.0728
Epoch 00011: val_loss did not improve from 60.92428

Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
364/364 [==============================] - 0s 284us/sample - loss: 47.2067 - val_loss: 65.6132
Epoch 12/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.1568300/364 [=======================>......] - ETA: 0s - loss: 46.8667
Epoch 00012: val_loss did not improve from 60.92428
364/364 [==============================] - 0s 282us/sample - loss: 47.1103 - val_loss: 64.5817
Epoch 13/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.0967300/364 [=======================>......] - ETA: 0s - loss: 45.5047
Epoch 00013: val_loss did not improve from 60.92428

Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
364/364 [==============================] - 0s 283us/sample - loss: 47.2140 - val_loss: 63.5054
Epoch 14/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.4912300/364 [=======================>......] - ETA: 0s - loss: 47.0923
Epoch 00014: val_loss did not improve from 60.92428
364/364 [==============================] - 0s 285us/sample - loss: 47.0125 - val_loss: 61.7635
Epoch 15/50
 50/364 [===>..........................] - ETA: 0s - loss: 45.1854300/364 [=======================>......] - ETA: 0s - loss: 46.9607
Epoch 00015: val_loss improved from 60.92428 to 60.36463, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 358us/sample - loss: 46.9971 - val_loss: 60.3646
Epoch 16/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.5921300/364 [=======================>......] - ETA: 0s - loss: 47.0998
Epoch 00016: val_loss improved from 60.36463 to 58.98361, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 366us/sample - loss: 46.9718 - val_loss: 58.9836
Epoch 17/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.2156300/364 [=======================>......] - ETA: 0s - loss: 46.4699
Epoch 00017: val_loss improved from 58.98361 to 57.86406, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 365us/sample - loss: 46.8893 - val_loss: 57.8641
Epoch 18/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.0998300/364 [=======================>......] - ETA: 0s - loss: 47.2794
Epoch 00018: val_loss improved from 57.86406 to 56.71748, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 358us/sample - loss: 46.9726 - val_loss: 56.7175
Epoch 19/50
 50/364 [===>..........................] - ETA: 0s - loss: 44.2994300/364 [=======================>......] - ETA: 0s - loss: 47.1058
Epoch 00019: val_loss improved from 56.71748 to 55.71612, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 361us/sample - loss: 47.1042 - val_loss: 55.7161
Epoch 20/50
 50/364 [===>..........................] - ETA: 0s - loss: 49.3565300/364 [=======================>......] - ETA: 0s - loss: 46.8349
Epoch 00020: val_loss improved from 55.71612 to 54.78172, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 360us/sample - loss: 46.9678 - val_loss: 54.7817
Epoch 21/50
 50/364 [===>..........................] - ETA: 0s - loss: 50.4163300/364 [=======================>......] - ETA: 0s - loss: 47.2899
Epoch 00021: val_loss improved from 54.78172 to 53.79225, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 363us/sample - loss: 47.0385 - val_loss: 53.7923
Epoch 22/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.0813300/364 [=======================>......] - ETA: 0s - loss: 46.8746
Epoch 00022: val_loss improved from 53.79225 to 52.77005, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 359us/sample - loss: 46.9181 - val_loss: 52.7701
Epoch 23/50
 50/364 [===>..........................] - ETA: 0s - loss: 44.8594300/364 [=======================>......] - ETA: 0s - loss: 47.3954
Epoch 00023: val_loss improved from 52.77005 to 52.03178, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 358us/sample - loss: 46.9920 - val_loss: 52.0318
Epoch 24/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.8477300/364 [=======================>......] - ETA: 0s - loss: 47.6944
Epoch 00024: val_loss improved from 52.03178 to 51.27449, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 362us/sample - loss: 46.9114 - val_loss: 51.2745
Epoch 25/50
 50/364 [===>..........................] - ETA: 0s - loss: 43.9298300/364 [=======================>......] - ETA: 0s - loss: 46.5322
Epoch 00025: val_loss improved from 51.27449 to 50.67866, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 363us/sample - loss: 46.8535 - val_loss: 50.6787
Epoch 26/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.6596300/364 [=======================>......] - ETA: 0s - loss: 47.3768
Epoch 00026: val_loss improved from 50.67866 to 50.24264, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 362us/sample - loss: 46.9516 - val_loss: 50.2426
Epoch 27/50
 50/364 [===>..........................] - ETA: 0s - loss: 45.7604300/364 [=======================>......] - ETA: 0s - loss: 46.4056
Epoch 00027: val_loss improved from 50.24264 to 49.79727, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 362us/sample - loss: 46.7969 - val_loss: 49.7973
Epoch 28/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.6791300/364 [=======================>......] - ETA: 0s - loss: 47.2321
Epoch 00028: val_loss improved from 49.79727 to 49.29772, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 360us/sample - loss: 46.8432 - val_loss: 49.2977
Epoch 29/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.3903300/364 [=======================>......] - ETA: 0s - loss: 47.4052
Epoch 00029: val_loss improved from 49.29772 to 48.72860, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 360us/sample - loss: 46.7890 - val_loss: 48.7286
Epoch 30/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.0682300/364 [=======================>......] - ETA: 0s - loss: 46.2165
Epoch 00030: val_loss improved from 48.72860 to 48.35764, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 363us/sample - loss: 46.7340 - val_loss: 48.3576
Epoch 31/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.8639300/364 [=======================>......] - ETA: 0s - loss: 46.5033
Epoch 00031: val_loss improved from 48.35764 to 48.02691, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 356us/sample - loss: 46.8796 - val_loss: 48.0269
Epoch 32/50
 50/364 [===>..........................] - ETA: 0s - loss: 49.8833300/364 [=======================>......] - ETA: 0s - loss: 47.1139
Epoch 00032: val_loss improved from 48.02691 to 47.54182, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 361us/sample - loss: 46.7783 - val_loss: 47.5418
Epoch 33/50
 50/364 [===>..........................] - ETA: 0s - loss: 48.3763300/364 [=======================>......] - ETA: 0s - loss: 46.5842
Epoch 00033: val_loss improved from 47.54182 to 47.30761, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 358us/sample - loss: 46.7663 - val_loss: 47.3076
Epoch 34/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.3763300/364 [=======================>......] - ETA: 0s - loss: 46.9255
Epoch 00034: val_loss improved from 47.30761 to 47.02159, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 363us/sample - loss: 46.7450 - val_loss: 47.0216
Epoch 35/50
 50/364 [===>..........................] - ETA: 0s - loss: 45.3841300/364 [=======================>......] - ETA: 0s - loss: 46.9369
Epoch 00035: val_loss improved from 47.02159 to 46.74619, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 360us/sample - loss: 46.7576 - val_loss: 46.7462
Epoch 36/50
 50/364 [===>..........................] - ETA: 0s - loss: 45.4491300/364 [=======================>......] - ETA: 0s - loss: 46.1764
Epoch 00036: val_loss improved from 46.74619 to 46.51358, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 367us/sample - loss: 46.7166 - val_loss: 46.5136
Epoch 37/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.0391300/364 [=======================>......] - ETA: 0s - loss: 46.1743
Epoch 00037: val_loss improved from 46.51358 to 46.26440, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 371us/sample - loss: 46.6930 - val_loss: 46.2644
Epoch 38/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.7558300/364 [=======================>......] - ETA: 0s - loss: 46.8142
Epoch 00038: val_loss improved from 46.26440 to 45.97877, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 365us/sample - loss: 46.6187 - val_loss: 45.9788
Epoch 39/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.0479300/364 [=======================>......] - ETA: 0s - loss: 46.8549
Epoch 00039: val_loss improved from 45.97877 to 45.78859, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 362us/sample - loss: 46.6250 - val_loss: 45.7886
Epoch 40/50
 50/364 [===>..........................] - ETA: 0s - loss: 49.0676300/364 [=======================>......] - ETA: 0s - loss: 46.8854
Epoch 00040: val_loss improved from 45.78859 to 45.66723, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 369us/sample - loss: 46.6213 - val_loss: 45.6672
Epoch 41/50
 50/364 [===>..........................] - ETA: 0s - loss: 45.6179300/364 [=======================>......] - ETA: 0s - loss: 46.4957
Epoch 00041: val_loss improved from 45.66723 to 45.56363, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 377us/sample - loss: 46.6139 - val_loss: 45.5636
Epoch 42/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.4763300/364 [=======================>......] - ETA: 0s - loss: 46.5622
Epoch 00042: val_loss improved from 45.56363 to 45.45293, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 369us/sample - loss: 46.6460 - val_loss: 45.4529
Epoch 43/50
 50/364 [===>..........................] - ETA: 0s - loss: 45.8567300/364 [=======================>......] - ETA: 0s - loss: 46.4274
Epoch 00043: val_loss improved from 45.45293 to 45.27901, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 371us/sample - loss: 46.6922 - val_loss: 45.2790
Epoch 44/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.8501250/364 [===================>..........] - ETA: 0s - loss: 46.2062
Epoch 00044: val_loss improved from 45.27901 to 45.11005, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 380us/sample - loss: 46.6428 - val_loss: 45.1101
Epoch 45/50
 50/364 [===>..........................] - ETA: 0s - loss: 49.9637300/364 [=======================>......] - ETA: 0s - loss: 47.3255
Epoch 00045: val_loss improved from 45.11005 to 45.03661, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 373us/sample - loss: 46.6697 - val_loss: 45.0366
Epoch 46/50
 50/364 [===>..........................] - ETA: 0s - loss: 44.6406300/364 [=======================>......] - ETA: 0s - loss: 46.5871
Epoch 00046: val_loss did not improve from 45.03661
364/364 [==============================] - 0s 292us/sample - loss: 46.4615 - val_loss: 45.0367
Epoch 47/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.8827300/364 [=======================>......] - ETA: 0s - loss: 46.7843
Epoch 00047: val_loss improved from 45.03661 to 44.94621, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 367us/sample - loss: 46.4944 - val_loss: 44.9462
Epoch 48/50
 50/364 [===>..........................] - ETA: 0s - loss: 42.7019300/364 [=======================>......] - ETA: 0s - loss: 46.6441
Epoch 00048: val_loss improved from 44.94621 to 44.85612, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 385us/sample - loss: 46.4894 - val_loss: 44.8561
Epoch 49/50
 50/364 [===>..........................] - ETA: 0s - loss: 46.4305300/364 [=======================>......] - ETA: 0s - loss: 46.3901
Epoch 00049: val_loss improved from 44.85612 to 44.75404, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 372us/sample - loss: 46.4288 - val_loss: 44.7540
Epoch 50/50
 50/364 [===>..........................] - ETA: 0s - loss: 47.6901300/364 [=======================>......] - ETA: 0s - loss: 46.4937
Epoch 00050: val_loss improved from 44.75404 to 44.67758, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
364/364 [==============================] - 0s 367us/sample - loss: 46.4449 - val_loss: 44.6776
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 485 samples, validate on 121 samples
Epoch 1/50
 50/485 [==>...........................] - ETA: 5s - loss: 173.5039250/485 [==============>...............] - ETA: 0s - loss: 160.3326
Epoch 00001: val_loss improved from inf to 130.41880, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
485/485 [==============================] - 1s 3ms/sample - loss: 153.8388 - val_loss: 130.4188
Epoch 2/50
 50/485 [==>...........................] - ETA: 0s - loss: 130.3274300/485 [=================>............] - ETA: 0s - loss: 102.9845
Epoch 00002: val_loss improved from 130.41880 to 53.08919, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
485/485 [==============================] - 0s 330us/sample - loss: 88.9159 - val_loss: 53.0892
Epoch 3/50
 50/485 [==>...........................] - ETA: 0s - loss: 55.2369300/485 [=================>............] - ETA: 0s - loss: 53.6666
Epoch 00003: val_loss did not improve from 53.08919
485/485 [==============================] - 0s 271us/sample - loss: 54.9095 - val_loss: 57.6613
Epoch 4/50
 50/485 [==>...........................] - ETA: 0s - loss: 46.9151300/485 [=================>............] - ETA: 0s - loss: 50.2974
Epoch 00004: val_loss did not improve from 53.08919

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
485/485 [==============================] - 0s 266us/sample - loss: 50.1946 - val_loss: 61.3240
Epoch 5/50
 50/485 [==>...........................] - ETA: 0s - loss: 52.4253300/485 [=================>............] - ETA: 0s - loss: 49.4582
Epoch 00005: val_loss did not improve from 53.08919
485/485 [==============================] - 0s 264us/sample - loss: 48.7524 - val_loss: 58.6712
Epoch 6/50
 50/485 [==>...........................] - ETA: 0s - loss: 45.9226300/485 [=================>............] - ETA: 0s - loss: 47.5866
Epoch 00006: val_loss did not improve from 53.08919

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
485/485 [==============================] - 0s 266us/sample - loss: 47.4660 - val_loss: 57.5859
Epoch 7/50
 50/485 [==>...........................] - ETA: 0s - loss: 47.8160300/485 [=================>............] - ETA: 0s - loss: 47.0735
Epoch 00007: val_loss did not improve from 53.08919
485/485 [==============================] - 0s 264us/sample - loss: 47.1941 - val_loss: 58.1395
Epoch 8/50
 50/485 [==>...........................] - ETA: 0s - loss: 46.0192300/485 [=================>............] - ETA: 0s - loss: 46.6888
Epoch 00008: val_loss did not improve from 53.08919

Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
485/485 [==============================] - 0s 265us/sample - loss: 46.8190 - val_loss: 57.7926
Epoch 9/50
 50/485 [==>...........................] - ETA: 0s - loss: 45.0300300/485 [=================>............] - ETA: 0s - loss: 47.7417
Epoch 00009: val_loss did not improve from 53.08919
485/485 [==============================] - 0s 265us/sample - loss: 46.7376 - val_loss: 56.7629
Epoch 10/50
 50/485 [==>...........................] - ETA: 0s - loss: 43.6403300/485 [=================>............] - ETA: 0s - loss: 46.1042
Epoch 00010: val_loss did not improve from 53.08919

Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
485/485 [==============================] - 0s 266us/sample - loss: 46.5975 - val_loss: 55.9598
Epoch 11/50
 50/485 [==>...........................] - ETA: 0s - loss: 44.9105300/485 [=================>............] - ETA: 0s - loss: 47.3443
Epoch 00011: val_loss did not improve from 53.08919
485/485 [==============================] - 0s 265us/sample - loss: 46.6111 - val_loss: 54.8297
Epoch 12/50
 50/485 [==>...........................] - ETA: 0s - loss: 43.7581300/485 [=================>............] - ETA: 0s - loss: 47.2297
Epoch 00012: val_loss did not improve from 53.08919

Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
485/485 [==============================] - 0s 265us/sample - loss: 46.6783 - val_loss: 53.8862
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
Epoch 00012: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F114/validate/F114_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/test/F114_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F114/train/F114_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
Traceback (most recent call last):
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "Run_CNN_FineTune_ByYear.py", line 347, in <module>
    model.load_weights(Wsave_name)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 234, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1187, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 99, in NewCheckpointReader
    error_translator(e)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F114/StartingYear2014/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
