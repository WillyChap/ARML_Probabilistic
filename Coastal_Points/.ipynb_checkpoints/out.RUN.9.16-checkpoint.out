2020-11-09 21:04:22.920774: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 21:04:23.322852: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 21:04:23.323123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ff85aa81d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:04:23.323149: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 21:04:23.344417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 21:04:23.450702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:04:23.466807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:04:23.669199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:04:23.721738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:04:23.871079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:04:24.032024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:04:24.157800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:04:24.274457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:04:24.277862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:04:24.277994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:04:24.457191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:04:24.457257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:04:24.457286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:04:24.462657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:04:24.480996: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ff868237b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:04:24.481033: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 21:04:24.485051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:04:24.485150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:04:24.485170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:04:24.485187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:04:24.488603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:04:24.488625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:04:24.488641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:04:24.488657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:04:24.499046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:04:24.499104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:04:24.499116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:04:24.499126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:04:24.502227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:04:24.505203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:04:24.505284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:04:24.505302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:04:24.505318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:04:24.505333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:04:24.505348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:04:24.505364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:04:24.505379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:04:24.508360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:04:24.508399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:04:24.508411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:04:24.508420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:04:24.511420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 160.0385, 153.3016
Mean and standard deviation for "p_sfc" = 984.0085, 62.0153
Mean and standard deviation for "u_tr_p" = 12.7144, 12.3078
Mean and standard deviation for "v_tr_p" = 1.3817, 13.2843
Mean and standard deviation for "Z_p" = 5573.0352, 203.6569
Mean and standard deviation for "IWV" = 13.4128, 7.6963
2020-11-09 21:04:44.369741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:04:44.370245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:04:44.370270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:04:44.370286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:04:44.370301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:04:44.370316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:04:44.370331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:04:44.370347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:04:44.373432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:04:44.471614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:04:44.471741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:04:44.471760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:04:44.471776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:04:44.471792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:04:44.482472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:04:44.482527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:04:44.482549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:04:44.487406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:04:44.487461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:04:44.487474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:04:44.487484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:04:44.490546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:04:53.456483: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:04:57.444142: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 21:04:57.492807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 203.3162, 182.2535
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 1819 samples, validate on 121 samples
Epoch 1/50
  50/1819 [..............................] - ETA: 4:17 - loss: 154.1465 100/1819 [>.............................] - ETA: 2:06 - loss: 153.9058 300/1819 [===>..........................] - ETA: 37s - loss: 156.9678  500/1819 [=======>......................] - ETA: 19s - loss: 151.1685 700/1819 [==========>...................] - ETA: 12s - loss: 137.8717 900/1819 [=============>................] - ETA: 7s - loss: 120.0528 1100/1819 [=================>............] - ETA: 4s - loss: 109.28401300/1819 [====================>.........] - ETA: 3s - loss: 99.7615 1500/1819 [=======================>......] - ETA: 1s - loss: 92.76171700/1819 [===========================>..] - ETA: 0s - loss: 86.4609
Epoch 00001: val_loss improved from inf to 68.30667, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 8s 5ms/sample - loss: 83.3342 - val_loss: 68.3067
Epoch 2/50
  50/1819 [..............................] - ETA: 0s - loss: 37.8853 250/1819 [===>..........................] - ETA: 0s - loss: 38.0336 450/1819 [======>.......................] - ETA: 0s - loss: 36.7582 650/1819 [=========>....................] - ETA: 0s - loss: 35.8703 850/1819 [=============>................] - ETA: 0s - loss: 35.30621050/1819 [================>.............] - ETA: 0s - loss: 34.69461250/1819 [===================>..........] - ETA: 0s - loss: 34.32121450/1819 [======================>.......] - ETA: 0s - loss: 33.85621650/1819 [==========================>...] - ETA: 0s - loss: 33.5976
Epoch 00002: val_loss did not improve from 68.30667
1819/1819 [==============================] - 1s 288us/sample - loss: 33.3394 - val_loss: 74.5000
Epoch 3/50
  50/1819 [..............................] - ETA: 0s - loss: 28.9093 250/1819 [===>..........................] - ETA: 0s - loss: 29.8317 450/1819 [======>.......................] - ETA: 0s - loss: 29.5548 650/1819 [=========>....................] - ETA: 0s - loss: 29.4793 850/1819 [=============>................] - ETA: 0s - loss: 29.41061050/1819 [================>.............] - ETA: 0s - loss: 29.28951250/1819 [===================>..........] - ETA: 0s - loss: 29.36851450/1819 [======================>.......] - ETA: 0s - loss: 29.33851650/1819 [==========================>...] - ETA: 0s - loss: 29.2220
Epoch 00003: val_loss improved from 68.30667 to 54.08917, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 316us/sample - loss: 29.0203 - val_loss: 54.0892
Epoch 4/50
  50/1819 [..............................] - ETA: 0s - loss: 27.8103 250/1819 [===>..........................] - ETA: 0s - loss: 27.2068 450/1819 [======>.......................] - ETA: 0s - loss: 27.9171 600/1819 [========>.....................] - ETA: 0s - loss: 28.2406 800/1819 [============>.................] - ETA: 0s - loss: 28.08221000/1819 [===============>..............] - ETA: 0s - loss: 27.97301200/1819 [==================>...........] - ETA: 0s - loss: 27.92521400/1819 [======================>.......] - ETA: 0s - loss: 27.79231600/1819 [=========================>....] - ETA: 0s - loss: 27.67151800/1819 [============================>.] - ETA: 0s - loss: 27.5808
Epoch 00004: val_loss improved from 54.08917 to 38.12332, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 327us/sample - loss: 27.5795 - val_loss: 38.1233
Epoch 5/50
  50/1819 [..............................] - ETA: 0s - loss: 26.2591 250/1819 [===>..........................] - ETA: 0s - loss: 26.9713 450/1819 [======>.......................] - ETA: 0s - loss: 27.1829 650/1819 [=========>....................] - ETA: 0s - loss: 27.1727 850/1819 [=============>................] - ETA: 0s - loss: 27.25891050/1819 [================>.............] - ETA: 0s - loss: 27.23801250/1819 [===================>..........] - ETA: 0s - loss: 27.03411450/1819 [======================>.......] - ETA: 0s - loss: 27.05781650/1819 [==========================>...] - ETA: 0s - loss: 26.9214
Epoch 00005: val_loss improved from 38.12332 to 30.12654, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 304us/sample - loss: 26.9757 - val_loss: 30.1265
Epoch 6/50
  50/1819 [..............................] - ETA: 0s - loss: 27.6980 250/1819 [===>..........................] - ETA: 0s - loss: 27.4144 450/1819 [======>.......................] - ETA: 0s - loss: 26.9905 650/1819 [=========>....................] - ETA: 0s - loss: 27.0010 850/1819 [=============>................] - ETA: 0s - loss: 26.87441050/1819 [================>.............] - ETA: 0s - loss: 27.09961250/1819 [===================>..........] - ETA: 0s - loss: 26.93371450/1819 [======================>.......] - ETA: 0s - loss: 27.01031650/1819 [==========================>...] - ETA: 0s - loss: 27.0081
Epoch 00006: val_loss did not improve from 30.12654
1819/1819 [==============================] - 1s 286us/sample - loss: 27.0517 - val_loss: 31.5942
Epoch 7/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4395 250/1819 [===>..........................] - ETA: 0s - loss: 26.7910 450/1819 [======>.......................] - ETA: 0s - loss: 27.2044 650/1819 [=========>....................] - ETA: 0s - loss: 26.6535 850/1819 [=============>................] - ETA: 0s - loss: 26.65071050/1819 [================>.............] - ETA: 0s - loss: 26.87561250/1819 [===================>..........] - ETA: 0s - loss: 26.98841450/1819 [======================>.......] - ETA: 0s - loss: 26.74931650/1819 [==========================>...] - ETA: 0s - loss: 26.6617
Epoch 00007: val_loss improved from 30.12654 to 27.80551, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 302us/sample - loss: 26.6662 - val_loss: 27.8055
Epoch 8/50
  50/1819 [..............................] - ETA: 0s - loss: 23.6501 250/1819 [===>..........................] - ETA: 0s - loss: 25.5981 450/1819 [======>.......................] - ETA: 0s - loss: 26.1253 650/1819 [=========>....................] - ETA: 0s - loss: 26.3214 850/1819 [=============>................] - ETA: 0s - loss: 26.20551050/1819 [================>.............] - ETA: 0s - loss: 26.23631250/1819 [===================>..........] - ETA: 0s - loss: 26.43141450/1819 [======================>.......] - ETA: 0s - loss: 26.56831650/1819 [==========================>...] - ETA: 0s - loss: 26.9402
Epoch 00008: val_loss improved from 27.80551 to 27.74739, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 302us/sample - loss: 27.0686 - val_loss: 27.7474
Epoch 9/50
  50/1819 [..............................] - ETA: 0s - loss: 28.8749 250/1819 [===>..........................] - ETA: 0s - loss: 28.4655 450/1819 [======>.......................] - ETA: 0s - loss: 27.8070 650/1819 [=========>....................] - ETA: 0s - loss: 27.2642 850/1819 [=============>................] - ETA: 0s - loss: 26.96631050/1819 [================>.............] - ETA: 0s - loss: 26.93821250/1819 [===================>..........] - ETA: 0s - loss: 26.92151450/1819 [======================>.......] - ETA: 0s - loss: 26.79491650/1819 [==========================>...] - ETA: 0s - loss: 26.7738
Epoch 00009: val_loss improved from 27.74739 to 25.84356, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 304us/sample - loss: 26.8486 - val_loss: 25.8436
Epoch 10/50
  50/1819 [..............................] - ETA: 0s - loss: 24.8504 250/1819 [===>..........................] - ETA: 0s - loss: 25.7637 450/1819 [======>.......................] - ETA: 0s - loss: 25.6053 650/1819 [=========>....................] - ETA: 0s - loss: 25.9396 850/1819 [=============>................] - ETA: 0s - loss: 25.91261050/1819 [================>.............] - ETA: 0s - loss: 26.26811250/1819 [===================>..........] - ETA: 0s - loss: 26.29171450/1819 [======================>.......] - ETA: 0s - loss: 26.38821650/1819 [==========================>...] - ETA: 0s - loss: 26.5309
Epoch 00010: val_loss improved from 25.84356 to 24.38812, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 303us/sample - loss: 26.6375 - val_loss: 24.3881
Epoch 11/50
  50/1819 [..............................] - ETA: 0s - loss: 24.4943 250/1819 [===>..........................] - ETA: 0s - loss: 26.7384 450/1819 [======>.......................] - ETA: 0s - loss: 27.2936 650/1819 [=========>....................] - ETA: 0s - loss: 27.4263 850/1819 [=============>................] - ETA: 0s - loss: 27.15351050/1819 [================>.............] - ETA: 0s - loss: 26.99661250/1819 [===================>..........] - ETA: 0s - loss: 27.05171450/1819 [======================>.......] - ETA: 0s - loss: 26.82991650/1819 [==========================>...] - ETA: 0s - loss: 26.8604
Epoch 00011: val_loss improved from 24.38812 to 24.23186, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 302us/sample - loss: 26.8828 - val_loss: 24.2319
Epoch 12/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4302 250/1819 [===>..........................] - ETA: 0s - loss: 25.6747 450/1819 [======>.......................] - ETA: 0s - loss: 25.4960 650/1819 [=========>....................] - ETA: 0s - loss: 26.0415 850/1819 [=============>................] - ETA: 0s - loss: 26.20871050/1819 [================>.............] - ETA: 0s - loss: 26.79731250/1819 [===================>..........] - ETA: 0s - loss: 26.85181450/1819 [======================>.......] - ETA: 0s - loss: 26.72211650/1819 [==========================>...] - ETA: 0s - loss: 26.7680
Epoch 00012: val_loss did not improve from 24.23186
1819/1819 [==============================] - 1s 287us/sample - loss: 26.7959 - val_loss: 25.6563
Epoch 13/50
  50/1819 [..............................] - ETA: 0s - loss: 26.3217 250/1819 [===>..........................] - ETA: 0s - loss: 26.7232 450/1819 [======>.......................] - ETA: 0s - loss: 26.9199 650/1819 [=========>....................] - ETA: 0s - loss: 26.5498 850/1819 [=============>................] - ETA: 0s - loss: 26.21131050/1819 [================>.............] - ETA: 0s - loss: 26.18861250/1819 [===================>..........] - ETA: 0s - loss: 26.32211450/1819 [======================>.......] - ETA: 0s - loss: 26.19231650/1819 [==========================>...] - ETA: 0s - loss: 26.1734
Epoch 00013: val_loss improved from 24.23186 to 23.91057, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 304us/sample - loss: 26.2582 - val_loss: 23.9106
Epoch 14/50
  50/1819 [..............................] - ETA: 0s - loss: 24.3678 250/1819 [===>..........................] - ETA: 0s - loss: 27.0925 450/1819 [======>.......................] - ETA: 0s - loss: 27.2149 650/1819 [=========>....................] - ETA: 0s - loss: 27.2410 850/1819 [=============>................] - ETA: 0s - loss: 26.77841050/1819 [================>.............] - ETA: 0s - loss: 26.69461250/1819 [===================>..........] - ETA: 0s - loss: 26.51731450/1819 [======================>.......] - ETA: 0s - loss: 26.45571650/1819 [==========================>...] - ETA: 0s - loss: 26.4202
Epoch 00014: val_loss did not improve from 23.91057
1819/1819 [==============================] - 1s 282us/sample - loss: 26.2934 - val_loss: 25.7627
Epoch 15/50
  50/1819 [..............................] - ETA: 0s - loss: 26.9419 250/1819 [===>..........................] - ETA: 0s - loss: 25.6520 450/1819 [======>.......................] - ETA: 0s - loss: 26.1388 650/1819 [=========>....................] - ETA: 0s - loss: 26.0292 850/1819 [=============>................] - ETA: 0s - loss: 26.27131050/1819 [================>.............] - ETA: 0s - loss: 26.33941250/1819 [===================>..........] - ETA: 0s - loss: 26.41181450/1819 [======================>.......] - ETA: 0s - loss: 26.48121650/1819 [==========================>...] - ETA: 0s - loss: 26.3685
Epoch 00015: val_loss did not improve from 23.91057

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1819/1819 [==============================] - 1s 285us/sample - loss: 26.2589 - val_loss: 25.5863
Epoch 16/50
  50/1819 [..............................] - ETA: 0s - loss: 23.9112 250/1819 [===>..........................] - ETA: 0s - loss: 27.1838 450/1819 [======>.......................] - ETA: 0s - loss: 27.0170 650/1819 [=========>....................] - ETA: 0s - loss: 26.6704 850/1819 [=============>................] - ETA: 0s - loss: 26.74771050/1819 [================>.............] - ETA: 0s - loss: 26.52791250/1819 [===================>..........] - ETA: 0s - loss: 26.53511450/1819 [======================>.......] - ETA: 0s - loss: 26.43871650/1819 [==========================>...] - ETA: 0s - loss: 26.3383
Epoch 00016: val_loss did not improve from 23.91057
1819/1819 [==============================] - 0s 272us/sample - loss: 26.3884 - val_loss: 24.1502
Epoch 17/50
  50/1819 [..............................] - ETA: 0s - loss: 25.7955 250/1819 [===>..........................] - ETA: 0s - loss: 25.5476 450/1819 [======>.......................] - ETA: 0s - loss: 25.8221 650/1819 [=========>....................] - ETA: 0s - loss: 25.9077 850/1819 [=============>................] - ETA: 0s - loss: 25.91711050/1819 [================>.............] - ETA: 0s - loss: 25.97321250/1819 [===================>..........] - ETA: 0s - loss: 25.84261450/1819 [======================>.......] - ETA: 0s - loss: 25.83681650/1819 [==========================>...] - ETA: 0s - loss: 25.7886
Epoch 00017: val_loss improved from 23.91057 to 23.35124, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 293us/sample - loss: 25.8382 - val_loss: 23.3512
Epoch 18/50
  50/1819 [..............................] - ETA: 0s - loss: 26.7233 250/1819 [===>..........................] - ETA: 0s - loss: 26.7442 500/1819 [=======>......................] - ETA: 0s - loss: 26.0636 700/1819 [==========>...................] - ETA: 0s - loss: 26.1466 900/1819 [=============>................] - ETA: 0s - loss: 26.09791100/1819 [=================>............] - ETA: 0s - loss: 25.91881300/1819 [====================>.........] - ETA: 0s - loss: 25.87871500/1819 [=======================>......] - ETA: 0s - loss: 25.86981750/1819 [===========================>..] - ETA: 0s - loss: 25.8325
Epoch 00018: val_loss improved from 23.35124 to 23.15749, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 281us/sample - loss: 25.8413 - val_loss: 23.1575
Epoch 19/50
  50/1819 [..............................] - ETA: 0s - loss: 24.0450 250/1819 [===>..........................] - ETA: 0s - loss: 25.7729 450/1819 [======>.......................] - ETA: 0s - loss: 26.5104 650/1819 [=========>....................] - ETA: 0s - loss: 26.3194 850/1819 [=============>................] - ETA: 0s - loss: 26.23291100/1819 [=================>............] - ETA: 0s - loss: 26.27991350/1819 [=====================>........] - ETA: 0s - loss: 26.15831600/1819 [=========================>....] - ETA: 0s - loss: 25.9869
Epoch 00019: val_loss did not improve from 23.15749
1819/1819 [==============================] - 0s 263us/sample - loss: 25.9243 - val_loss: 23.4514
Epoch 20/50
  50/1819 [..............................] - ETA: 0s - loss: 24.8886 250/1819 [===>..........................] - ETA: 0s - loss: 26.1063 450/1819 [======>.......................] - ETA: 0s - loss: 25.9195 700/1819 [==========>...................] - ETA: 0s - loss: 26.0313 900/1819 [=============>................] - ETA: 0s - loss: 25.94541150/1819 [=================>............] - ETA: 0s - loss: 26.06021350/1819 [=====================>........] - ETA: 0s - loss: 25.93781550/1819 [========================>.....] - ETA: 0s - loss: 26.04421750/1819 [===========================>..] - ETA: 0s - loss: 25.9450
Epoch 00020: val_loss did not improve from 23.15749

Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1819/1819 [==============================] - 0s 263us/sample - loss: 25.9206 - val_loss: 24.6265
Epoch 21/50
  50/1819 [..............................] - ETA: 0s - loss: 26.6279 250/1819 [===>..........................] - ETA: 0s - loss: 26.7360 450/1819 [======>.......................] - ETA: 0s - loss: 26.1768 650/1819 [=========>....................] - ETA: 0s - loss: 26.1756 850/1819 [=============>................] - ETA: 0s - loss: 26.17861050/1819 [================>.............] - ETA: 0s - loss: 26.26361250/1819 [===================>..........] - ETA: 0s - loss: 26.29111450/1819 [======================>.......] - ETA: 0s - loss: 26.03281650/1819 [==========================>...] - ETA: 0s - loss: 26.0363
Epoch 00021: val_loss improved from 23.15749 to 23.12615, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 301us/sample - loss: 26.0039 - val_loss: 23.1262
Epoch 22/50
  50/1819 [..............................] - ETA: 0s - loss: 25.9886 250/1819 [===>..........................] - ETA: 0s - loss: 25.4404 450/1819 [======>.......................] - ETA: 0s - loss: 25.2089 650/1819 [=========>....................] - ETA: 0s - loss: 25.6254 850/1819 [=============>................] - ETA: 0s - loss: 25.60361050/1819 [================>.............] - ETA: 0s - loss: 25.73511250/1819 [===================>..........] - ETA: 0s - loss: 25.78141450/1819 [======================>.......] - ETA: 0s - loss: 25.73181650/1819 [==========================>...] - ETA: 0s - loss: 25.7193
Epoch 00022: val_loss improved from 23.12615 to 23.06944, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 335us/sample - loss: 25.6829 - val_loss: 23.0694
Epoch 23/50
  50/1819 [..............................] - ETA: 0s - loss: 25.2788 250/1819 [===>..........................] - ETA: 0s - loss: 25.8251 450/1819 [======>.......................] - ETA: 0s - loss: 26.0188 650/1819 [=========>....................] - ETA: 0s - loss: 25.6203 850/1819 [=============>................] - ETA: 0s - loss: 25.71951050/1819 [================>.............] - ETA: 0s - loss: 25.86611250/1819 [===================>..........] - ETA: 0s - loss: 25.96171450/1819 [======================>.......] - ETA: 0s - loss: 25.84231650/1819 [==========================>...] - ETA: 0s - loss: 25.8219
Epoch 00023: val_loss did not improve from 23.06944
1819/1819 [==============================] - 0s 273us/sample - loss: 25.7905 - val_loss: 23.1374
Epoch 24/50
  50/1819 [..............................] - ETA: 0s - loss: 25.7813 250/1819 [===>..........................] - ETA: 0s - loss: 24.5783 450/1819 [======>.......................] - ETA: 0s - loss: 24.7682 650/1819 [=========>....................] - ETA: 0s - loss: 24.8906 850/1819 [=============>................] - ETA: 0s - loss: 25.21631050/1819 [================>.............] - ETA: 0s - loss: 25.37161250/1819 [===================>..........] - ETA: 0s - loss: 25.65711450/1819 [======================>.......] - ETA: 0s - loss: 25.70421700/1819 [===========================>..] - ETA: 0s - loss: 25.6900
Epoch 00024: val_loss did not improve from 23.06944

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1819/1819 [==============================] - 0s 272us/sample - loss: 25.6580 - val_loss: 23.2160
Epoch 25/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9552 250/1819 [===>..........................] - ETA: 0s - loss: 25.3805 500/1819 [=======>......................] - ETA: 0s - loss: 25.7666 700/1819 [==========>...................] - ETA: 0s - loss: 26.0531 900/1819 [=============>................] - ETA: 0s - loss: 25.97621100/1819 [=================>............] - ETA: 0s - loss: 25.68861300/1819 [====================>.........] - ETA: 0s - loss: 25.72921500/1819 [=======================>......] - ETA: 0s - loss: 25.86471700/1819 [===========================>..] - ETA: 0s - loss: 25.8707
Epoch 00025: val_loss improved from 23.06944 to 23.01365, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 292us/sample - loss: 25.8069 - val_loss: 23.0137
Epoch 26/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9602 250/1819 [===>..........................] - ETA: 0s - loss: 25.4910 500/1819 [=======>......................] - ETA: 0s - loss: 25.6128 700/1819 [==========>...................] - ETA: 0s - loss: 25.6981 900/1819 [=============>................] - ETA: 0s - loss: 25.56951100/1819 [=================>............] - ETA: 0s - loss: 25.73381300/1819 [====================>.........] - ETA: 0s - loss: 25.67681500/1819 [=======================>......] - ETA: 0s - loss: 25.74091700/1819 [===========================>..] - ETA: 0s - loss: 25.6908
Epoch 00026: val_loss improved from 23.01365 to 22.99228, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 291us/sample - loss: 25.6609 - val_loss: 22.9923
Epoch 27/50
  50/1819 [..............................] - ETA: 0s - loss: 27.6182 250/1819 [===>..........................] - ETA: 0s - loss: 25.5578 450/1819 [======>.......................] - ETA: 0s - loss: 26.4120 650/1819 [=========>....................] - ETA: 0s - loss: 26.2352 850/1819 [=============>................] - ETA: 0s - loss: 25.94631050/1819 [================>.............] - ETA: 0s - loss: 25.86071250/1819 [===================>..........] - ETA: 0s - loss: 25.95401450/1819 [======================>.......] - ETA: 0s - loss: 25.81621650/1819 [==========================>...] - ETA: 0s - loss: 25.8407
Epoch 00027: val_loss did not improve from 22.99228
1819/1819 [==============================] - 1s 278us/sample - loss: 25.7557 - val_loss: 23.2241
Epoch 28/50
  50/1819 [..............................] - ETA: 0s - loss: 25.7855 250/1819 [===>..........................] - ETA: 0s - loss: 25.7567 450/1819 [======>.......................] - ETA: 0s - loss: 25.5291 650/1819 [=========>....................] - ETA: 0s - loss: 25.3261 850/1819 [=============>................] - ETA: 0s - loss: 25.43931050/1819 [================>.............] - ETA: 0s - loss: 25.61351250/1819 [===================>..........] - ETA: 0s - loss: 25.69491450/1819 [======================>.......] - ETA: 0s - loss: 25.69151650/1819 [==========================>...] - ETA: 0s - loss: 25.6890
Epoch 00028: val_loss did not improve from 22.99228

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1819/1819 [==============================] - 0s 274us/sample - loss: 25.6654 - val_loss: 22.9928
Epoch 29/50
  50/1819 [..............................] - ETA: 0s - loss: 25.2028 250/1819 [===>..........................] - ETA: 0s - loss: 25.6097 450/1819 [======>.......................] - ETA: 0s - loss: 25.7671 650/1819 [=========>....................] - ETA: 0s - loss: 25.7542 850/1819 [=============>................] - ETA: 0s - loss: 25.62771050/1819 [================>.............] - ETA: 0s - loss: 25.69321250/1819 [===================>..........] - ETA: 0s - loss: 25.68821450/1819 [======================>.......] - ETA: 0s - loss: 25.77101650/1819 [==========================>...] - ETA: 0s - loss: 25.7940
Epoch 00029: val_loss did not improve from 22.99228
1819/1819 [==============================] - 0s 272us/sample - loss: 25.7110 - val_loss: 22.9954
Epoch 30/50
  50/1819 [..............................] - ETA: 0s - loss: 25.7511 250/1819 [===>..........................] - ETA: 0s - loss: 24.7420 450/1819 [======>.......................] - ETA: 0s - loss: 25.2563 650/1819 [=========>....................] - ETA: 0s - loss: 25.7031 850/1819 [=============>................] - ETA: 0s - loss: 25.90411050/1819 [================>.............] - ETA: 0s - loss: 25.80511250/1819 [===================>..........] - ETA: 0s - loss: 25.66631500/1819 [=======================>......] - ETA: 0s - loss: 25.55681700/1819 [===========================>..] - ETA: 0s - loss: 25.6189
Epoch 00030: val_loss did not improve from 22.99228

Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1819/1819 [==============================] - 0s 269us/sample - loss: 25.6503 - val_loss: 22.9967
Epoch 31/50
  50/1819 [..............................] - ETA: 0s - loss: 24.7671 250/1819 [===>..........................] - ETA: 0s - loss: 25.8207 450/1819 [======>.......................] - ETA: 0s - loss: 25.7340 650/1819 [=========>....................] - ETA: 0s - loss: 26.0432 850/1819 [=============>................] - ETA: 0s - loss: 25.95161050/1819 [================>.............] - ETA: 0s - loss: 26.02451250/1819 [===================>..........] - ETA: 0s - loss: 26.06561450/1819 [======================>.......] - ETA: 0s - loss: 25.86451650/1819 [==========================>...] - ETA: 0s - loss: 25.7443
Epoch 00031: val_loss did not improve from 22.99228
1819/1819 [==============================] - 1s 289us/sample - loss: 25.7078 - val_loss: 22.9928
Epoch 32/50
  50/1819 [..............................] - ETA: 0s - loss: 24.2289 250/1819 [===>..........................] - ETA: 0s - loss: 25.6932 450/1819 [======>.......................] - ETA: 0s - loss: 25.5323 650/1819 [=========>....................] - ETA: 0s - loss: 25.4733 850/1819 [=============>................] - ETA: 0s - loss: 25.80901050/1819 [================>.............] - ETA: 0s - loss: 25.81521250/1819 [===================>..........] - ETA: 0s - loss: 25.74111450/1819 [======================>.......] - ETA: 0s - loss: 25.73241650/1819 [==========================>...] - ETA: 0s - loss: 25.7463
Epoch 00032: val_loss improved from 22.99228 to 22.98493, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 310us/sample - loss: 25.6421 - val_loss: 22.9849
Epoch 33/50
  50/1819 [..............................] - ETA: 0s - loss: 26.4361 250/1819 [===>..........................] - ETA: 0s - loss: 26.2390 450/1819 [======>.......................] - ETA: 0s - loss: 26.2612 650/1819 [=========>....................] - ETA: 0s - loss: 25.7978 850/1819 [=============>................] - ETA: 0s - loss: 25.90681050/1819 [================>.............] - ETA: 0s - loss: 25.72521250/1819 [===================>..........] - ETA: 0s - loss: 25.78911450/1819 [======================>.......] - ETA: 0s - loss: 25.65171650/1819 [==========================>...] - ETA: 0s - loss: 25.6952
Epoch 00033: val_loss did not improve from 22.98493
1819/1819 [==============================] - 1s 307us/sample - loss: 25.6814 - val_loss: 22.9965
Epoch 34/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3523 250/1819 [===>..........................] - ETA: 0s - loss: 25.9261 450/1819 [======>.......................] - ETA: 0s - loss: 26.2568 650/1819 [=========>....................] - ETA: 0s - loss: 26.1315 850/1819 [=============>................] - ETA: 0s - loss: 25.92811050/1819 [================>.............] - ETA: 0s - loss: 25.74451250/1819 [===================>..........] - ETA: 0s - loss: 25.71421450/1819 [======================>.......] - ETA: 0s - loss: 25.68141650/1819 [==========================>...] - ETA: 0s - loss: 25.7564
Epoch 00034: val_loss did not improve from 22.98493

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-05.
1819/1819 [==============================] - 1s 299us/sample - loss: 25.7747 - val_loss: 23.0020
Epoch 35/50
  50/1819 [..............................] - ETA: 0s - loss: 26.7146 250/1819 [===>..........................] - ETA: 0s - loss: 25.9247 450/1819 [======>.......................] - ETA: 0s - loss: 26.1714 650/1819 [=========>....................] - ETA: 0s - loss: 25.7768 850/1819 [=============>................] - ETA: 0s - loss: 25.70051050/1819 [================>.............] - ETA: 0s - loss: 25.56021250/1819 [===================>..........] - ETA: 0s - loss: 25.53031450/1819 [======================>.......] - ETA: 0s - loss: 25.55961650/1819 [==========================>...] - ETA: 0s - loss: 25.5402
Epoch 00035: val_loss did not improve from 22.98493
1819/1819 [==============================] - 1s 301us/sample - loss: 25.6666 - val_loss: 22.9975
Epoch 36/50
  50/1819 [..............................] - ETA: 0s - loss: 26.6964 250/1819 [===>..........................] - ETA: 0s - loss: 25.6669 450/1819 [======>.......................] - ETA: 0s - loss: 25.7101 650/1819 [=========>....................] - ETA: 0s - loss: 25.8604 850/1819 [=============>................] - ETA: 0s - loss: 25.79771050/1819 [================>.............] - ETA: 0s - loss: 25.83411250/1819 [===================>..........] - ETA: 0s - loss: 25.76571400/1819 [======================>.......] - ETA: 0s - loss: 25.71661600/1819 [=========================>....] - ETA: 0s - loss: 25.70061800/1819 [============================>.] - ETA: 0s - loss: 25.6167
Epoch 00036: val_loss did not improve from 22.98493
1819/1819 [==============================] - 1s 303us/sample - loss: 25.6320 - val_loss: 22.9860
Epoch 37/50
  50/1819 [..............................] - ETA: 0s - loss: 26.7108 250/1819 [===>..........................] - ETA: 0s - loss: 25.7822 450/1819 [======>.......................] - ETA: 0s - loss: 25.7663 650/1819 [=========>....................] - ETA: 0s - loss: 25.8913 850/1819 [=============>................] - ETA: 0s - loss: 25.72721050/1819 [================>.............] - ETA: 0s - loss: 25.76331250/1819 [===================>..........] - ETA: 0s - loss: 25.65571450/1819 [======================>.......] - ETA: 0s - loss: 25.69501650/1819 [==========================>...] - ETA: 0s - loss: 25.6462
Epoch 00037: val_loss did not improve from 22.98493
1819/1819 [==============================] - 1s 303us/sample - loss: 25.6461 - val_loss: 22.9927
Epoch 38/50
  50/1819 [..............................] - ETA: 0s - loss: 25.0260 250/1819 [===>..........................] - ETA: 0s - loss: 25.8406 450/1819 [======>.......................] - ETA: 0s - loss: 25.9890 650/1819 [=========>....................] - ETA: 0s - loss: 25.9670 850/1819 [=============>................] - ETA: 0s - loss: 25.76471050/1819 [================>.............] - ETA: 0s - loss: 25.68751250/1819 [===================>..........] - ETA: 0s - loss: 25.59491450/1819 [======================>.......] - ETA: 0s - loss: 25.56001650/1819 [==========================>...] - ETA: 0s - loss: 25.5957
Epoch 00038: val_loss did not improve from 22.98493
1819/1819 [==============================] - 1s 297us/sample - loss: 25.6072 - val_loss: 23.0011
Epoch 39/50
  50/1819 [..............................] - ETA: 0s - loss: 24.2977 250/1819 [===>..........................] - ETA: 0s - loss: 26.3259 450/1819 [======>.......................] - ETA: 0s - loss: 26.0148 650/1819 [=========>....................] - ETA: 0s - loss: 26.0209 850/1819 [=============>................] - ETA: 0s - loss: 25.81951050/1819 [================>.............] - ETA: 0s - loss: 25.91831250/1819 [===================>..........] - ETA: 0s - loss: 25.73811450/1819 [======================>.......] - ETA: 0s - loss: 25.66521650/1819 [==========================>...] - ETA: 0s - loss: 25.5669
Epoch 00039: val_loss did not improve from 22.98493
1819/1819 [==============================] - 1s 300us/sample - loss: 25.6005 - val_loss: 22.9851
Epoch 40/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3859 250/1819 [===>..........................] - ETA: 0s - loss: 25.9530 450/1819 [======>.......................] - ETA: 0s - loss: 26.0451 650/1819 [=========>....................] - ETA: 0s - loss: 25.5766 850/1819 [=============>................] - ETA: 0s - loss: 25.45761050/1819 [================>.............] - ETA: 0s - loss: 25.66781250/1819 [===================>..........] - ETA: 0s - loss: 25.61141450/1819 [======================>.......] - ETA: 0s - loss: 25.59371650/1819 [==========================>...] - ETA: 0s - loss: 25.6414
Epoch 00040: val_loss improved from 22.98493 to 22.98359, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 314us/sample - loss: 25.6217 - val_loss: 22.9836
Epoch 41/50
  50/1819 [..............................] - ETA: 0s - loss: 26.3579 250/1819 [===>..........................] - ETA: 0s - loss: 25.6284 450/1819 [======>.......................] - ETA: 0s - loss: 25.2472 650/1819 [=========>....................] - ETA: 0s - loss: 25.4104 850/1819 [=============>................] - ETA: 0s - loss: 25.63331050/1819 [================>.............] - ETA: 0s - loss: 25.68131250/1819 [===================>..........] - ETA: 0s - loss: 25.84331450/1819 [======================>.......] - ETA: 0s - loss: 25.79471650/1819 [==========================>...] - ETA: 0s - loss: 25.6952
Epoch 00041: val_loss improved from 22.98359 to 22.97805, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 310us/sample - loss: 25.6286 - val_loss: 22.9781
Epoch 42/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9818 250/1819 [===>..........................] - ETA: 0s - loss: 24.9656 450/1819 [======>.......................] - ETA: 0s - loss: 25.6770 650/1819 [=========>....................] - ETA: 0s - loss: 25.6948 850/1819 [=============>................] - ETA: 0s - loss: 25.51211050/1819 [================>.............] - ETA: 0s - loss: 25.66151250/1819 [===================>..........] - ETA: 0s - loss: 25.50921450/1819 [======================>.......] - ETA: 0s - loss: 25.59491650/1819 [==========================>...] - ETA: 0s - loss: 25.6078
Epoch 00042: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 292us/sample - loss: 25.6666 - val_loss: 22.9908
Epoch 43/50
  50/1819 [..............................] - ETA: 0s - loss: 24.4863 250/1819 [===>..........................] - ETA: 0s - loss: 25.3149 450/1819 [======>.......................] - ETA: 0s - loss: 25.6640 650/1819 [=========>....................] - ETA: 0s - loss: 25.7790 850/1819 [=============>................] - ETA: 0s - loss: 25.65051050/1819 [================>.............] - ETA: 0s - loss: 25.65631150/1819 [=================>............] - ETA: 0s - loss: 25.80901350/1819 [=====================>........] - ETA: 0s - loss: 25.84321550/1819 [========================>.....] - ETA: 0s - loss: 25.77351750/1819 [===========================>..] - ETA: 0s - loss: 25.7294
Epoch 00043: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 319us/sample - loss: 25.7433 - val_loss: 22.9840
Epoch 44/50
  50/1819 [..............................] - ETA: 0s - loss: 25.0354 250/1819 [===>..........................] - ETA: 0s - loss: 25.1383 450/1819 [======>.......................] - ETA: 0s - loss: 25.5795 650/1819 [=========>....................] - ETA: 0s - loss: 25.6068 850/1819 [=============>................] - ETA: 0s - loss: 25.51841050/1819 [================>.............] - ETA: 0s - loss: 25.58341250/1819 [===================>..........] - ETA: 0s - loss: 25.77281450/1819 [======================>.......] - ETA: 0s - loss: 25.73381650/1819 [==========================>...] - ETA: 0s - loss: 25.6789
Epoch 00044: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 293us/sample - loss: 25.6317 - val_loss: 22.9798
Epoch 45/50
  50/1819 [..............................] - ETA: 0s - loss: 25.8766 250/1819 [===>..........................] - ETA: 0s - loss: 25.4398 450/1819 [======>.......................] - ETA: 0s - loss: 25.4531 650/1819 [=========>....................] - ETA: 0s - loss: 25.2657 850/1819 [=============>................] - ETA: 0s - loss: 25.35851050/1819 [================>.............] - ETA: 0s - loss: 25.27641250/1819 [===================>..........] - ETA: 0s - loss: 25.35581450/1819 [======================>.......] - ETA: 0s - loss: 25.49461650/1819 [==========================>...] - ETA: 0s - loss: 25.4616
Epoch 00045: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 298us/sample - loss: 25.5379 - val_loss: 22.9831
Epoch 46/50
  50/1819 [..............................] - ETA: 0s - loss: 26.8155 250/1819 [===>..........................] - ETA: 0s - loss: 25.7861 450/1819 [======>.......................] - ETA: 0s - loss: 26.0414 650/1819 [=========>....................] - ETA: 0s - loss: 25.7551 850/1819 [=============>................] - ETA: 0s - loss: 25.82481050/1819 [================>.............] - ETA: 0s - loss: 25.73581250/1819 [===================>..........] - ETA: 0s - loss: 25.84711450/1819 [======================>.......] - ETA: 0s - loss: 25.75551650/1819 [==========================>...] - ETA: 0s - loss: 25.6170
Epoch 00046: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 294us/sample - loss: 25.6255 - val_loss: 22.9782
Epoch 47/50
  50/1819 [..............................] - ETA: 0s - loss: 24.5353 250/1819 [===>..........................] - ETA: 0s - loss: 25.4642 450/1819 [======>.......................] - ETA: 0s - loss: 25.5582 650/1819 [=========>....................] - ETA: 0s - loss: 25.6111 850/1819 [=============>................] - ETA: 0s - loss: 25.72541050/1819 [================>.............] - ETA: 0s - loss: 25.65101250/1819 [===================>..........] - ETA: 0s - loss: 25.40821450/1819 [======================>.......] - ETA: 0s - loss: 25.40401650/1819 [==========================>...] - ETA: 0s - loss: 25.5011
Epoch 00047: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 287us/sample - loss: 25.5206 - val_loss: 22.9881
Epoch 48/50
  50/1819 [..............................] - ETA: 0s - loss: 26.7147 250/1819 [===>..........................] - ETA: 0s - loss: 26.0590 450/1819 [======>.......................] - ETA: 0s - loss: 26.1240 650/1819 [=========>....................] - ETA: 0s - loss: 25.9392 850/1819 [=============>................] - ETA: 0s - loss: 25.67931050/1819 [================>.............] - ETA: 0s - loss: 25.80671250/1819 [===================>..........] - ETA: 0s - loss: 25.86951450/1819 [======================>.......] - ETA: 0s - loss: 25.73801650/1819 [==========================>...] - ETA: 0s - loss: 25.7865
Epoch 00048: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 296us/sample - loss: 25.7378 - val_loss: 22.9785
Epoch 49/50
  50/1819 [..............................] - ETA: 0s - loss: 29.7400 250/1819 [===>..........................] - ETA: 0s - loss: 25.6306 450/1819 [======>.......................] - ETA: 0s - loss: 25.6270 650/1819 [=========>....................] - ETA: 0s - loss: 25.5886 850/1819 [=============>................] - ETA: 0s - loss: 25.37931050/1819 [================>.............] - ETA: 0s - loss: 25.46651250/1819 [===================>..........] - ETA: 0s - loss: 25.52131450/1819 [======================>.......] - ETA: 0s - loss: 25.55831650/1819 [==========================>...] - ETA: 0s - loss: 25.5683
Epoch 00049: val_loss did not improve from 22.97805
1819/1819 [==============================] - 1s 292us/sample - loss: 25.6037 - val_loss: 22.9870
Epoch 50/50
  50/1819 [..............................] - ETA: 0s - loss: 25.8208 250/1819 [===>..........................] - ETA: 0s - loss: 25.3598 450/1819 [======>.......................] - ETA: 0s - loss: 25.4664 650/1819 [=========>....................] - ETA: 0s - loss: 25.3997 850/1819 [=============>................] - ETA: 0s - loss: 25.54271050/1819 [================>.............] - ETA: 0s - loss: 25.73741250/1819 [===================>..........] - ETA: 0s - loss: 25.68071450/1819 [======================>.......] - ETA: 0s - loss: 25.67641650/1819 [==========================>...] - ETA: 0s - loss: 25.7138
Epoch 00050: val_loss improved from 22.97805 to 22.97395, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1819/1819 [==============================] - 1s 311us/sample - loss: 25.6241 - val_loss: 22.9740
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 1819 samples, validate on 121 samples
Epoch 1/50
  50/1819 [..............................] - ETA: 4:06 - loss: 156.5493 150/1819 [=>............................] - ETA: 1:17 - loss: 157.6578 350/1819 [====>.........................] - ETA: 29s - loss: 155.6924  550/1819 [========>.....................] - ETA: 16s - loss: 150.9514 750/1819 [===========>..................] - ETA: 10s - loss: 140.5917 950/1819 [==============>...............] - ETA: 6s - loss: 122.7457 1150/1819 [=================>............] - ETA: 4s - loss: 112.56121350/1819 [=====================>........] - ETA: 2s - loss: 102.62811550/1819 [========================>.....] - ETA: 1s - loss: 95.9528 1750/1819 [===========================>..] - ETA: 0s - loss: 89.7952
Epoch 00001: val_loss improved from inf to 54.49858, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 8s 4ms/sample - loss: 87.8559 - val_loss: 54.4986
Epoch 2/50
  50/1819 [..............................] - ETA: 0s - loss: 40.5401 250/1819 [===>..........................] - ETA: 0s - loss: 39.5123 450/1819 [======>.......................] - ETA: 0s - loss: 38.0254 650/1819 [=========>....................] - ETA: 0s - loss: 37.4562 850/1819 [=============>................] - ETA: 0s - loss: 37.48561050/1819 [================>.............] - ETA: 0s - loss: 37.35801250/1819 [===================>..........] - ETA: 0s - loss: 37.07861450/1819 [======================>.......] - ETA: 0s - loss: 36.52911650/1819 [==========================>...] - ETA: 0s - loss: 36.2391
Epoch 00002: val_loss did not improve from 54.49858
1819/1819 [==============================] - 1s 292us/sample - loss: 35.8144 - val_loss: 67.9573
Epoch 3/50
  50/1819 [..............................] - ETA: 0s - loss: 31.2660 250/1819 [===>..........................] - ETA: 0s - loss: 31.2261 450/1819 [======>.......................] - ETA: 0s - loss: 30.5547 650/1819 [=========>....................] - ETA: 0s - loss: 30.5392 850/1819 [=============>................] - ETA: 0s - loss: 30.43441050/1819 [================>.............] - ETA: 0s - loss: 30.20501250/1819 [===================>..........] - ETA: 0s - loss: 30.20691450/1819 [======================>.......] - ETA: 0s - loss: 30.11251650/1819 [==========================>...] - ETA: 0s - loss: 29.9900
Epoch 00003: val_loss did not improve from 54.49858

Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1819/1819 [==============================] - 1s 292us/sample - loss: 29.8973 - val_loss: 58.8242
Epoch 4/50
  50/1819 [..............................] - ETA: 0s - loss: 27.2789 250/1819 [===>..........................] - ETA: 0s - loss: 28.9866 450/1819 [======>.......................] - ETA: 0s - loss: 28.6912 650/1819 [=========>....................] - ETA: 0s - loss: 28.5915 850/1819 [=============>................] - ETA: 0s - loss: 28.71291050/1819 [================>.............] - ETA: 0s - loss: 28.59191250/1819 [===================>..........] - ETA: 0s - loss: 28.58491450/1819 [======================>.......] - ETA: 0s - loss: 28.45791650/1819 [==========================>...] - ETA: 0s - loss: 28.3495
Epoch 00004: val_loss improved from 54.49858 to 46.51748, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 326us/sample - loss: 28.3007 - val_loss: 46.5175
Epoch 5/50
  50/1819 [..............................] - ETA: 0s - loss: 32.1493 250/1819 [===>..........................] - ETA: 0s - loss: 29.6981 450/1819 [======>.......................] - ETA: 0s - loss: 28.9342 650/1819 [=========>....................] - ETA: 0s - loss: 28.5605 850/1819 [=============>................] - ETA: 0s - loss: 28.49791050/1819 [================>.............] - ETA: 0s - loss: 28.09411250/1819 [===================>..........] - ETA: 0s - loss: 27.98721450/1819 [======================>.......] - ETA: 0s - loss: 28.02411650/1819 [==========================>...] - ETA: 0s - loss: 27.9177
Epoch 00005: val_loss improved from 46.51748 to 39.90836, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 314us/sample - loss: 27.8739 - val_loss: 39.9084
Epoch 6/50
  50/1819 [..............................] - ETA: 0s - loss: 28.1609 250/1819 [===>..........................] - ETA: 0s - loss: 27.8155 450/1819 [======>.......................] - ETA: 0s - loss: 27.5706 650/1819 [=========>....................] - ETA: 0s - loss: 27.4171 850/1819 [=============>................] - ETA: 0s - loss: 27.58771050/1819 [================>.............] - ETA: 0s - loss: 27.54261250/1819 [===================>..........] - ETA: 0s - loss: 27.41301450/1819 [======================>.......] - ETA: 0s - loss: 27.58861650/1819 [==========================>...] - ETA: 0s - loss: 27.5471
Epoch 00006: val_loss improved from 39.90836 to 30.18420, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 311us/sample - loss: 27.6490 - val_loss: 30.1842
Epoch 7/50
  50/1819 [..............................] - ETA: 0s - loss: 26.4342 250/1819 [===>..........................] - ETA: 0s - loss: 27.5385 450/1819 [======>.......................] - ETA: 0s - loss: 27.9996 650/1819 [=========>....................] - ETA: 0s - loss: 27.9699 850/1819 [=============>................] - ETA: 0s - loss: 27.99451050/1819 [================>.............] - ETA: 0s - loss: 27.73981250/1819 [===================>..........] - ETA: 0s - loss: 27.53291450/1819 [======================>.......] - ETA: 0s - loss: 27.61281650/1819 [==========================>...] - ETA: 0s - loss: 27.5357
Epoch 00007: val_loss improved from 30.18420 to 28.94978, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 311us/sample - loss: 27.5606 - val_loss: 28.9498
Epoch 8/50
  50/1819 [..............................] - ETA: 0s - loss: 28.2996 250/1819 [===>..........................] - ETA: 0s - loss: 26.8188 450/1819 [======>.......................] - ETA: 0s - loss: 26.5267 650/1819 [=========>....................] - ETA: 0s - loss: 26.6791 850/1819 [=============>................] - ETA: 0s - loss: 26.67941050/1819 [================>.............] - ETA: 0s - loss: 26.78001250/1819 [===================>..........] - ETA: 0s - loss: 26.80891450/1819 [======================>.......] - ETA: 0s - loss: 26.88231650/1819 [==========================>...] - ETA: 0s - loss: 26.9007
Epoch 00008: val_loss improved from 28.94978 to 25.61794, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 315us/sample - loss: 26.9925 - val_loss: 25.6179
Epoch 9/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4765 250/1819 [===>..........................] - ETA: 0s - loss: 27.0384 450/1819 [======>.......................] - ETA: 0s - loss: 26.4688 650/1819 [=========>....................] - ETA: 0s - loss: 26.6715 850/1819 [=============>................] - ETA: 0s - loss: 26.79531050/1819 [================>.............] - ETA: 0s - loss: 27.07301250/1819 [===================>..........] - ETA: 0s - loss: 26.85711450/1819 [======================>.......] - ETA: 0s - loss: 26.89691650/1819 [==========================>...] - ETA: 0s - loss: 26.8336
Epoch 00009: val_loss improved from 25.61794 to 24.77154, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 320us/sample - loss: 26.8351 - val_loss: 24.7715
Epoch 10/50
  50/1819 [..............................] - ETA: 0s - loss: 24.6140 250/1819 [===>..........................] - ETA: 0s - loss: 26.2313 450/1819 [======>.......................] - ETA: 0s - loss: 26.5866 650/1819 [=========>....................] - ETA: 0s - loss: 26.6910 850/1819 [=============>................] - ETA: 0s - loss: 26.94301050/1819 [================>.............] - ETA: 0s - loss: 26.87271250/1819 [===================>..........] - ETA: 0s - loss: 27.09291450/1819 [======================>.......] - ETA: 0s - loss: 27.00941650/1819 [==========================>...] - ETA: 0s - loss: 26.89841800/1819 [============================>.] - ETA: 0s - loss: 26.8266
Epoch 00010: val_loss did not improve from 24.77154
1819/1819 [==============================] - 1s 296us/sample - loss: 26.8154 - val_loss: 25.2961
Epoch 11/50
  50/1819 [..............................] - ETA: 0s - loss: 26.4267 250/1819 [===>..........................] - ETA: 0s - loss: 26.2462 450/1819 [======>.......................] - ETA: 0s - loss: 27.2795 650/1819 [=========>....................] - ETA: 0s - loss: 27.0898 850/1819 [=============>................] - ETA: 0s - loss: 27.12841050/1819 [================>.............] - ETA: 0s - loss: 27.27721250/1819 [===================>..........] - ETA: 0s - loss: 27.27251450/1819 [======================>.......] - ETA: 0s - loss: 27.14711650/1819 [==========================>...] - ETA: 0s - loss: 26.9613
Epoch 00011: val_loss improved from 24.77154 to 24.75468, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 308us/sample - loss: 26.9372 - val_loss: 24.7547
Epoch 12/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6854 250/1819 [===>..........................] - ETA: 0s - loss: 26.8523 450/1819 [======>.......................] - ETA: 0s - loss: 27.2004 650/1819 [=========>....................] - ETA: 0s - loss: 26.8976 850/1819 [=============>................] - ETA: 0s - loss: 26.95301050/1819 [================>.............] - ETA: 0s - loss: 26.99421250/1819 [===================>..........] - ETA: 0s - loss: 26.77211400/1819 [======================>.......] - ETA: 0s - loss: 26.85181550/1819 [========================>.....] - ETA: 0s - loss: 26.86781750/1819 [===========================>..] - ETA: 0s - loss: 26.8218
Epoch 00012: val_loss did not improve from 24.75468
1819/1819 [==============================] - 1s 311us/sample - loss: 26.7948 - val_loss: 25.1469
Epoch 13/50
  50/1819 [..............................] - ETA: 0s - loss: 23.8212 250/1819 [===>..........................] - ETA: 0s - loss: 26.7278 450/1819 [======>.......................] - ETA: 0s - loss: 26.6674 650/1819 [=========>....................] - ETA: 0s - loss: 26.6910 850/1819 [=============>................] - ETA: 0s - loss: 26.58101050/1819 [================>.............] - ETA: 0s - loss: 26.47261250/1819 [===================>..........] - ETA: 0s - loss: 26.27891450/1819 [======================>.......] - ETA: 0s - loss: 26.24151650/1819 [==========================>...] - ETA: 0s - loss: 26.3769
Epoch 00013: val_loss improved from 24.75468 to 23.46545, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 309us/sample - loss: 26.4246 - val_loss: 23.4654
Epoch 14/50
  50/1819 [..............................] - ETA: 0s - loss: 27.0880 250/1819 [===>..........................] - ETA: 0s - loss: 26.4317 450/1819 [======>.......................] - ETA: 0s - loss: 26.5148 650/1819 [=========>....................] - ETA: 0s - loss: 26.6872 850/1819 [=============>................] - ETA: 0s - loss: 26.59291050/1819 [================>.............] - ETA: 0s - loss: 26.39461250/1819 [===================>..........] - ETA: 0s - loss: 26.72651450/1819 [======================>.......] - ETA: 0s - loss: 26.87221650/1819 [==========================>...] - ETA: 0s - loss: 26.7925
Epoch 00014: val_loss did not improve from 23.46545
1819/1819 [==============================] - 1s 290us/sample - loss: 26.7214 - val_loss: 25.3889
Epoch 15/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9455 250/1819 [===>..........................] - ETA: 0s - loss: 26.8794 450/1819 [======>.......................] - ETA: 0s - loss: 27.2014 650/1819 [=========>....................] - ETA: 0s - loss: 26.9418 850/1819 [=============>................] - ETA: 0s - loss: 26.60701050/1819 [================>.............] - ETA: 0s - loss: 26.39811250/1819 [===================>..........] - ETA: 0s - loss: 26.47161450/1819 [======================>.......] - ETA: 0s - loss: 26.64141650/1819 [==========================>...] - ETA: 0s - loss: 26.5871
Epoch 00015: val_loss improved from 23.46545 to 23.40770, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 310us/sample - loss: 26.4455 - val_loss: 23.4077
Epoch 16/50
  50/1819 [..............................] - ETA: 0s - loss: 30.8813 250/1819 [===>..........................] - ETA: 0s - loss: 27.1710 450/1819 [======>.......................] - ETA: 0s - loss: 26.8832 650/1819 [=========>....................] - ETA: 0s - loss: 26.4852 850/1819 [=============>................] - ETA: 0s - loss: 26.41821050/1819 [================>.............] - ETA: 0s - loss: 26.32001250/1819 [===================>..........] - ETA: 0s - loss: 26.24961450/1819 [======================>.......] - ETA: 0s - loss: 26.34301650/1819 [==========================>...] - ETA: 0s - loss: 26.1873
Epoch 00016: val_loss improved from 23.40770 to 23.31077, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 311us/sample - loss: 26.2758 - val_loss: 23.3108
Epoch 17/50
  50/1819 [..............................] - ETA: 0s - loss: 26.6610 250/1819 [===>..........................] - ETA: 0s - loss: 27.3401 450/1819 [======>.......................] - ETA: 0s - loss: 27.0916 650/1819 [=========>....................] - ETA: 0s - loss: 26.2891 850/1819 [=============>................] - ETA: 0s - loss: 26.27871050/1819 [================>.............] - ETA: 0s - loss: 26.38081250/1819 [===================>..........] - ETA: 0s - loss: 26.34281450/1819 [======================>.......] - ETA: 0s - loss: 26.29001650/1819 [==========================>...] - ETA: 0s - loss: 26.2662
Epoch 00017: val_loss did not improve from 23.31077
1819/1819 [==============================] - 1s 299us/sample - loss: 26.1496 - val_loss: 23.5302
Epoch 18/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3655 250/1819 [===>..........................] - ETA: 0s - loss: 25.9073 450/1819 [======>.......................] - ETA: 0s - loss: 26.1747 650/1819 [=========>....................] - ETA: 0s - loss: 26.4650 850/1819 [=============>................] - ETA: 0s - loss: 26.58331050/1819 [================>.............] - ETA: 0s - loss: 26.27141250/1819 [===================>..........] - ETA: 0s - loss: 26.22191450/1819 [======================>.......] - ETA: 0s - loss: 26.14001650/1819 [==========================>...] - ETA: 0s - loss: 26.2293
Epoch 00018: val_loss did not improve from 23.31077

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1819/1819 [==============================] - 1s 295us/sample - loss: 26.0978 - val_loss: 23.3351
Epoch 19/50
  50/1819 [..............................] - ETA: 0s - loss: 25.9748 250/1819 [===>..........................] - ETA: 0s - loss: 26.3780 450/1819 [======>.......................] - ETA: 0s - loss: 26.0143 650/1819 [=========>....................] - ETA: 0s - loss: 25.8038 850/1819 [=============>................] - ETA: 0s - loss: 26.07001050/1819 [================>.............] - ETA: 0s - loss: 26.03051250/1819 [===================>..........] - ETA: 0s - loss: 26.06011450/1819 [======================>.......] - ETA: 0s - loss: 26.12631650/1819 [==========================>...] - ETA: 0s - loss: 26.1902
Epoch 00019: val_loss improved from 23.31077 to 23.24239, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 318us/sample - loss: 26.1039 - val_loss: 23.2424
Epoch 20/50
  50/1819 [..............................] - ETA: 0s - loss: 26.3171 250/1819 [===>..........................] - ETA: 0s - loss: 26.2878 450/1819 [======>.......................] - ETA: 0s - loss: 25.7182 650/1819 [=========>....................] - ETA: 0s - loss: 25.5611 850/1819 [=============>................] - ETA: 0s - loss: 25.71351050/1819 [================>.............] - ETA: 0s - loss: 26.02781250/1819 [===================>..........] - ETA: 0s - loss: 26.21251450/1819 [======================>.......] - ETA: 0s - loss: 26.27971650/1819 [==========================>...] - ETA: 0s - loss: 26.2240
Epoch 00020: val_loss did not improve from 23.24239
1819/1819 [==============================] - 1s 291us/sample - loss: 26.1778 - val_loss: 23.5129
Epoch 21/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9535 250/1819 [===>..........................] - ETA: 0s - loss: 25.5241 450/1819 [======>.......................] - ETA: 0s - loss: 25.7496 650/1819 [=========>....................] - ETA: 0s - loss: 25.8237 850/1819 [=============>................] - ETA: 0s - loss: 25.73011050/1819 [================>.............] - ETA: 0s - loss: 25.75401250/1819 [===================>..........] - ETA: 0s - loss: 25.84211450/1819 [======================>.......] - ETA: 0s - loss: 25.78971650/1819 [==========================>...] - ETA: 0s - loss: 25.8706
Epoch 00021: val_loss improved from 23.24239 to 23.08213, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 314us/sample - loss: 25.9562 - val_loss: 23.0821
Epoch 22/50
  50/1819 [..............................] - ETA: 0s - loss: 28.5771 250/1819 [===>..........................] - ETA: 0s - loss: 26.2395 450/1819 [======>.......................] - ETA: 0s - loss: 26.2368 650/1819 [=========>....................] - ETA: 0s - loss: 26.1290 850/1819 [=============>................] - ETA: 0s - loss: 25.80041050/1819 [================>.............] - ETA: 0s - loss: 25.86381250/1819 [===================>..........] - ETA: 0s - loss: 26.00041450/1819 [======================>.......] - ETA: 0s - loss: 26.01781650/1819 [==========================>...] - ETA: 0s - loss: 26.0910
Epoch 00022: val_loss improved from 23.08213 to 23.01744, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 311us/sample - loss: 26.1391 - val_loss: 23.0174
Epoch 23/50
  50/1819 [..............................] - ETA: 0s - loss: 26.0685 250/1819 [===>..........................] - ETA: 0s - loss: 25.6670 450/1819 [======>.......................] - ETA: 0s - loss: 26.2277 650/1819 [=========>....................] - ETA: 0s - loss: 25.9685 850/1819 [=============>................] - ETA: 0s - loss: 26.01741050/1819 [================>.............] - ETA: 0s - loss: 26.08891250/1819 [===================>..........] - ETA: 0s - loss: 26.15581450/1819 [======================>.......] - ETA: 0s - loss: 26.11581650/1819 [==========================>...] - ETA: 0s - loss: 26.0800
Epoch 00023: val_loss did not improve from 23.01744
1819/1819 [==============================] - 1s 292us/sample - loss: 26.1430 - val_loss: 23.4020
Epoch 24/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9323 250/1819 [===>..........................] - ETA: 0s - loss: 25.1789 450/1819 [======>.......................] - ETA: 0s - loss: 25.3766 650/1819 [=========>....................] - ETA: 0s - loss: 25.6377 850/1819 [=============>................] - ETA: 0s - loss: 25.60771050/1819 [================>.............] - ETA: 0s - loss: 25.54221250/1819 [===================>..........] - ETA: 0s - loss: 25.69271450/1819 [======================>.......] - ETA: 0s - loss: 25.80751650/1819 [==========================>...] - ETA: 0s - loss: 25.8175
Epoch 00024: val_loss did not improve from 23.01744

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1819/1819 [==============================] - 1s 291us/sample - loss: 25.8730 - val_loss: 23.0226
Epoch 25/50
  50/1819 [..............................] - ETA: 0s - loss: 27.2269 250/1819 [===>..........................] - ETA: 0s - loss: 26.1766 450/1819 [======>.......................] - ETA: 0s - loss: 25.6604 650/1819 [=========>....................] - ETA: 0s - loss: 25.6780 850/1819 [=============>................] - ETA: 0s - loss: 25.81971050/1819 [================>.............] - ETA: 0s - loss: 25.73891250/1819 [===================>..........] - ETA: 0s - loss: 25.78311450/1819 [======================>.......] - ETA: 0s - loss: 25.83391650/1819 [==========================>...] - ETA: 0s - loss: 25.9947
Epoch 00025: val_loss improved from 23.01744 to 22.97158, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 313us/sample - loss: 25.9252 - val_loss: 22.9716
Epoch 26/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6088 250/1819 [===>..........................] - ETA: 0s - loss: 26.0358 450/1819 [======>.......................] - ETA: 0s - loss: 26.2489 650/1819 [=========>....................] - ETA: 0s - loss: 26.4477 850/1819 [=============>................] - ETA: 0s - loss: 26.21901050/1819 [================>.............] - ETA: 0s - loss: 26.07271250/1819 [===================>..........] - ETA: 0s - loss: 26.08121450/1819 [======================>.......] - ETA: 0s - loss: 26.07281650/1819 [==========================>...] - ETA: 0s - loss: 26.0923
Epoch 00026: val_loss did not improve from 22.97158
1819/1819 [==============================] - 1s 296us/sample - loss: 25.9627 - val_loss: 23.0608
Epoch 27/50
  50/1819 [..............................] - ETA: 0s - loss: 27.6470 250/1819 [===>..........................] - ETA: 0s - loss: 26.6913 450/1819 [======>.......................] - ETA: 0s - loss: 26.3415 650/1819 [=========>....................] - ETA: 0s - loss: 26.2090 850/1819 [=============>................] - ETA: 0s - loss: 26.14191050/1819 [================>.............] - ETA: 0s - loss: 26.15341250/1819 [===================>..........] - ETA: 0s - loss: 26.12231450/1819 [======================>.......] - ETA: 0s - loss: 25.94641650/1819 [==========================>...] - ETA: 0s - loss: 25.8747
Epoch 00027: val_loss did not improve from 22.97158

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1819/1819 [==============================] - 1s 294us/sample - loss: 25.9507 - val_loss: 23.0017
Epoch 28/50
  50/1819 [..............................] - ETA: 0s - loss: 27.9422 250/1819 [===>..........................] - ETA: 0s - loss: 26.6849 400/1819 [=====>........................] - ETA: 0s - loss: 26.8239 550/1819 [========>.....................] - ETA: 0s - loss: 26.3028 750/1819 [===========>..................] - ETA: 0s - loss: 26.1761 950/1819 [==============>...............] - ETA: 0s - loss: 26.28881150/1819 [=================>............] - ETA: 0s - loss: 26.19621350/1819 [=====================>........] - ETA: 0s - loss: 25.96881550/1819 [========================>.....] - ETA: 0s - loss: 25.87371750/1819 [===========================>..] - ETA: 0s - loss: 25.9109
Epoch 00028: val_loss did not improve from 22.97158
1819/1819 [==============================] - 1s 318us/sample - loss: 25.8458 - val_loss: 22.9915
Epoch 29/50
  50/1819 [..............................] - ETA: 0s - loss: 26.1794 250/1819 [===>..........................] - ETA: 0s - loss: 26.4312 450/1819 [======>.......................] - ETA: 0s - loss: 26.4049 650/1819 [=========>....................] - ETA: 0s - loss: 26.2573 850/1819 [=============>................] - ETA: 0s - loss: 26.18451050/1819 [================>.............] - ETA: 0s - loss: 26.30101250/1819 [===================>..........] - ETA: 0s - loss: 26.17901450/1819 [======================>.......] - ETA: 0s - loss: 25.98631650/1819 [==========================>...] - ETA: 0s - loss: 25.9745
Epoch 00029: val_loss did not improve from 22.97158

Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1819/1819 [==============================] - 1s 294us/sample - loss: 25.9225 - val_loss: 23.0387
Epoch 30/50
  50/1819 [..............................] - ETA: 0s - loss: 25.9573 250/1819 [===>..........................] - ETA: 0s - loss: 25.5475 450/1819 [======>.......................] - ETA: 0s - loss: 25.7186 650/1819 [=========>....................] - ETA: 0s - loss: 26.1300 850/1819 [=============>................] - ETA: 0s - loss: 26.13751050/1819 [================>.............] - ETA: 0s - loss: 26.10241250/1819 [===================>..........] - ETA: 0s - loss: 26.07791450/1819 [======================>.......] - ETA: 0s - loss: 26.00481650/1819 [==========================>...] - ETA: 0s - loss: 26.0187
Epoch 00030: val_loss did not improve from 22.97158
1819/1819 [==============================] - 1s 294us/sample - loss: 25.9474 - val_loss: 22.9849
Epoch 31/50
  50/1819 [..............................] - ETA: 0s - loss: 26.2218 250/1819 [===>..........................] - ETA: 0s - loss: 25.0734 450/1819 [======>.......................] - ETA: 0s - loss: 25.4862 650/1819 [=========>....................] - ETA: 0s - loss: 25.9608 850/1819 [=============>................] - ETA: 0s - loss: 25.81311050/1819 [================>.............] - ETA: 0s - loss: 25.86561250/1819 [===================>..........] - ETA: 0s - loss: 25.94311450/1819 [======================>.......] - ETA: 0s - loss: 25.81541650/1819 [==========================>...] - ETA: 0s - loss: 25.9031
Epoch 00031: val_loss did not improve from 22.97158

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-05.
1819/1819 [==============================] - 1s 289us/sample - loss: 25.9116 - val_loss: 22.9791
Epoch 32/50
  50/1819 [..............................] - ETA: 0s - loss: 25.2946 250/1819 [===>..........................] - ETA: 0s - loss: 25.1602 450/1819 [======>.......................] - ETA: 0s - loss: 25.4812 650/1819 [=========>....................] - ETA: 0s - loss: 25.7513 850/1819 [=============>................] - ETA: 0s - loss: 25.79451050/1819 [================>.............] - ETA: 0s - loss: 25.88561250/1819 [===================>..........] - ETA: 0s - loss: 25.84961450/1819 [======================>.......] - ETA: 0s - loss: 25.72001650/1819 [==========================>...] - ETA: 0s - loss: 25.7199
Epoch 00032: val_loss did not improve from 22.97158
1819/1819 [==============================] - 1s 295us/sample - loss: 25.8057 - val_loss: 22.9780
Epoch 33/50
  50/1819 [..............................] - ETA: 0s - loss: 25.7122 250/1819 [===>..........................] - ETA: 0s - loss: 26.0438 450/1819 [======>.......................] - ETA: 0s - loss: 25.6006 650/1819 [=========>....................] - ETA: 0s - loss: 25.5860 850/1819 [=============>................] - ETA: 0s - loss: 25.63721050/1819 [================>.............] - ETA: 0s - loss: 25.71801250/1819 [===================>..........] - ETA: 0s - loss: 25.89421450/1819 [======================>.......] - ETA: 0s - loss: 25.95251650/1819 [==========================>...] - ETA: 0s - loss: 25.8639
Epoch 00033: val_loss improved from 22.97158 to 22.96042, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 316us/sample - loss: 25.8863 - val_loss: 22.9604
Epoch 34/50
  50/1819 [..............................] - ETA: 0s - loss: 24.4559 250/1819 [===>..........................] - ETA: 0s - loss: 25.7362 450/1819 [======>.......................] - ETA: 0s - loss: 26.5084 650/1819 [=========>....................] - ETA: 0s - loss: 25.9548 850/1819 [=============>................] - ETA: 0s - loss: 25.79001050/1819 [================>.............] - ETA: 0s - loss: 25.86331250/1819 [===================>..........] - ETA: 0s - loss: 25.92131450/1819 [======================>.......] - ETA: 0s - loss: 25.74961650/1819 [==========================>...] - ETA: 0s - loss: 25.9393
Epoch 00034: val_loss did not improve from 22.96042
1819/1819 [==============================] - 1s 294us/sample - loss: 25.8868 - val_loss: 22.9772
Epoch 35/50
  50/1819 [..............................] - ETA: 0s - loss: 26.1320 250/1819 [===>..........................] - ETA: 0s - loss: 25.0680 450/1819 [======>.......................] - ETA: 0s - loss: 26.0411 650/1819 [=========>....................] - ETA: 0s - loss: 26.0348 850/1819 [=============>................] - ETA: 0s - loss: 26.01331050/1819 [================>.............] - ETA: 0s - loss: 25.99251250/1819 [===================>..........] - ETA: 0s - loss: 25.93551450/1819 [======================>.......] - ETA: 0s - loss: 25.82251650/1819 [==========================>...] - ETA: 0s - loss: 25.9278
Epoch 00035: val_loss did not improve from 22.96042
1819/1819 [==============================] - 1s 297us/sample - loss: 25.9025 - val_loss: 22.9753
Epoch 36/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6944 250/1819 [===>..........................] - ETA: 0s - loss: 26.5274 450/1819 [======>.......................] - ETA: 0s - loss: 25.8264 650/1819 [=========>....................] - ETA: 0s - loss: 25.6913 850/1819 [=============>................] - ETA: 0s - loss: 25.71451050/1819 [================>.............] - ETA: 0s - loss: 25.68911250/1819 [===================>..........] - ETA: 0s - loss: 25.89381450/1819 [======================>.......] - ETA: 0s - loss: 25.81861650/1819 [==========================>...] - ETA: 0s - loss: 25.8157
Epoch 00036: val_loss improved from 22.96042 to 22.94976, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 313us/sample - loss: 25.7949 - val_loss: 22.9498
Epoch 37/50
  50/1819 [..............................] - ETA: 0s - loss: 26.2973 250/1819 [===>..........................] - ETA: 0s - loss: 25.8462 450/1819 [======>.......................] - ETA: 0s - loss: 25.6211 650/1819 [=========>....................] - ETA: 0s - loss: 26.0201 850/1819 [=============>................] - ETA: 0s - loss: 25.91071050/1819 [================>.............] - ETA: 0s - loss: 25.86681250/1819 [===================>..........] - ETA: 0s - loss: 25.80081450/1819 [======================>.......] - ETA: 0s - loss: 25.99911650/1819 [==========================>...] - ETA: 0s - loss: 25.9378
Epoch 00037: val_loss did not improve from 22.94976
1819/1819 [==============================] - 1s 292us/sample - loss: 25.8867 - val_loss: 22.9664
Epoch 38/50
  50/1819 [..............................] - ETA: 0s - loss: 26.3893 250/1819 [===>..........................] - ETA: 0s - loss: 25.5112 450/1819 [======>.......................] - ETA: 0s - loss: 25.6517 650/1819 [=========>....................] - ETA: 0s - loss: 25.5883 850/1819 [=============>................] - ETA: 0s - loss: 25.55081050/1819 [================>.............] - ETA: 0s - loss: 25.45991250/1819 [===================>..........] - ETA: 0s - loss: 25.56291350/1819 [=====================>........] - ETA: 0s - loss: 25.57111550/1819 [========================>.....] - ETA: 0s - loss: 25.83631750/1819 [===========================>..] - ETA: 0s - loss: 25.8522
Epoch 00038: val_loss did not improve from 22.94976
1819/1819 [==============================] - 1s 303us/sample - loss: 25.8538 - val_loss: 22.9584
Epoch 39/50
  50/1819 [..............................] - ETA: 0s - loss: 26.1960 250/1819 [===>..........................] - ETA: 0s - loss: 25.1012 450/1819 [======>.......................] - ETA: 0s - loss: 25.4551 650/1819 [=========>....................] - ETA: 0s - loss: 25.6373 850/1819 [=============>................] - ETA: 0s - loss: 25.64171050/1819 [================>.............] - ETA: 0s - loss: 25.69511250/1819 [===================>..........] - ETA: 0s - loss: 25.75331450/1819 [======================>.......] - ETA: 0s - loss: 25.82991650/1819 [==========================>...] - ETA: 0s - loss: 25.8267
Epoch 00039: val_loss improved from 22.94976 to 22.94482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1819/1819 [==============================] - 1s 301us/sample - loss: 25.9389 - val_loss: 22.9448
Epoch 40/50
  50/1819 [..............................] - ETA: 0s - loss: 25.1690 250/1819 [===>..........................] - ETA: 0s - loss: 26.7476 450/1819 [======>.......................] - ETA: 0s - loss: 26.3472 650/1819 [=========>....................] - ETA: 0s - loss: 26.1899 850/1819 [=============>................] - ETA: 0s - loss: 26.02671050/1819 [================>.............] - ETA: 0s - loss: 25.99691250/1819 [===================>..........] - ETA: 0s - loss: 26.03851450/1819 [======================>.......] - ETA: 0s - loss: 25.95981650/1819 [==========================>...] - ETA: 0s - loss: 25.8884
Epoch 00040: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 282us/sample - loss: 25.8401 - val_loss: 22.9659
Epoch 41/50
  50/1819 [..............................] - ETA: 0s - loss: 24.8562 250/1819 [===>..........................] - ETA: 0s - loss: 26.0998 500/1819 [=======>......................] - ETA: 0s - loss: 26.2086 750/1819 [===========>..................] - ETA: 0s - loss: 25.90451000/1819 [===============>..............] - ETA: 0s - loss: 26.01521250/1819 [===================>..........] - ETA: 0s - loss: 26.13171500/1819 [=======================>......] - ETA: 0s - loss: 26.03091750/1819 [===========================>..] - ETA: 0s - loss: 25.8364
Epoch 00041: val_loss did not improve from 22.94482
1819/1819 [==============================] - 0s 255us/sample - loss: 25.8899 - val_loss: 22.9692
Epoch 42/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3412 250/1819 [===>..........................] - ETA: 0s - loss: 26.4605 450/1819 [======>.......................] - ETA: 0s - loss: 26.3106 650/1819 [=========>....................] - ETA: 0s - loss: 26.1440 850/1819 [=============>................] - ETA: 0s - loss: 25.93961050/1819 [================>.............] - ETA: 0s - loss: 25.99511250/1819 [===================>..........] - ETA: 0s - loss: 26.13221450/1819 [======================>.......] - ETA: 0s - loss: 25.97461650/1819 [==========================>...] - ETA: 0s - loss: 25.8587
Epoch 00042: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 275us/sample - loss: 25.8831 - val_loss: 22.9613
Epoch 43/50
  50/1819 [..............................] - ETA: 0s - loss: 26.4717 250/1819 [===>..........................] - ETA: 0s - loss: 25.9163 450/1819 [======>.......................] - ETA: 0s - loss: 25.5769 650/1819 [=========>....................] - ETA: 0s - loss: 25.7773 850/1819 [=============>................] - ETA: 0s - loss: 25.54951050/1819 [================>.............] - ETA: 0s - loss: 25.66731250/1819 [===================>..........] - ETA: 0s - loss: 25.70621450/1819 [======================>.......] - ETA: 0s - loss: 25.83921650/1819 [==========================>...] - ETA: 0s - loss: 25.8808
Epoch 00043: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 286us/sample - loss: 25.8960 - val_loss: 22.9643
Epoch 44/50
  50/1819 [..............................] - ETA: 0s - loss: 24.5844 250/1819 [===>..........................] - ETA: 0s - loss: 25.4943 450/1819 [======>.......................] - ETA: 0s - loss: 25.4426 650/1819 [=========>....................] - ETA: 0s - loss: 25.5639 850/1819 [=============>................] - ETA: 0s - loss: 25.44151050/1819 [================>.............] - ETA: 0s - loss: 25.60531250/1819 [===================>..........] - ETA: 0s - loss: 25.77591450/1819 [======================>.......] - ETA: 0s - loss: 25.81131650/1819 [==========================>...] - ETA: 0s - loss: 25.8659
Epoch 00044: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 293us/sample - loss: 25.8492 - val_loss: 22.9544
Epoch 45/50
  50/1819 [..............................] - ETA: 0s - loss: 26.9254 250/1819 [===>..........................] - ETA: 0s - loss: 25.9475 450/1819 [======>.......................] - ETA: 0s - loss: 25.8535 650/1819 [=========>....................] - ETA: 0s - loss: 25.7034 850/1819 [=============>................] - ETA: 0s - loss: 25.69081050/1819 [================>.............] - ETA: 0s - loss: 25.77631250/1819 [===================>..........] - ETA: 0s - loss: 25.79111450/1819 [======================>.......] - ETA: 0s - loss: 25.78091650/1819 [==========================>...] - ETA: 0s - loss: 25.8141
Epoch 00045: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 296us/sample - loss: 25.7133 - val_loss: 22.9599
Epoch 46/50
  50/1819 [..............................] - ETA: 0s - loss: 26.2445 250/1819 [===>..........................] - ETA: 0s - loss: 27.0333 450/1819 [======>.......................] - ETA: 0s - loss: 26.7864 650/1819 [=========>....................] - ETA: 0s - loss: 26.3926 850/1819 [=============>................] - ETA: 0s - loss: 26.07621050/1819 [================>.............] - ETA: 0s - loss: 25.94181250/1819 [===================>..........] - ETA: 0s - loss: 26.02621450/1819 [======================>.......] - ETA: 0s - loss: 25.98551650/1819 [==========================>...] - ETA: 0s - loss: 25.9518
Epoch 00046: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 292us/sample - loss: 25.9234 - val_loss: 22.9721
Epoch 47/50
  50/1819 [..............................] - ETA: 0s - loss: 24.2547 250/1819 [===>..........................] - ETA: 0s - loss: 25.2543 450/1819 [======>.......................] - ETA: 0s - loss: 25.5540 650/1819 [=========>....................] - ETA: 0s - loss: 25.6125 850/1819 [=============>................] - ETA: 0s - loss: 25.76151050/1819 [================>.............] - ETA: 0s - loss: 25.82321250/1819 [===================>..........] - ETA: 0s - loss: 25.92091450/1819 [======================>.......] - ETA: 0s - loss: 25.78271600/1819 [=========================>....] - ETA: 0s - loss: 25.76201800/1819 [============================>.] - ETA: 0s - loss: 25.7105
Epoch 00047: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 318us/sample - loss: 25.7506 - val_loss: 22.9628
Epoch 48/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3378 250/1819 [===>..........................] - ETA: 0s - loss: 26.6192 450/1819 [======>.......................] - ETA: 0s - loss: 26.2035 650/1819 [=========>....................] - ETA: 0s - loss: 26.1724 850/1819 [=============>................] - ETA: 0s - loss: 26.21331050/1819 [================>.............] - ETA: 0s - loss: 26.08221250/1819 [===================>..........] - ETA: 0s - loss: 25.94501450/1819 [======================>.......] - ETA: 0s - loss: 25.85241650/1819 [==========================>...] - ETA: 0s - loss: 25.9398
Epoch 00048: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 292us/sample - loss: 25.9427 - val_loss: 22.9659
Epoch 49/50
  50/1819 [..............................] - ETA: 0s - loss: 26.2455 250/1819 [===>..........................] - ETA: 0s - loss: 25.8017 450/1819 [======>.......................] - ETA: 0s - loss: 25.7309 650/1819 [=========>....................] - ETA: 0s - loss: 25.7119 850/1819 [=============>................] - ETA: 0s - loss: 25.83631050/1819 [================>.............] - ETA: 0s - loss: 25.98171250/1819 [===================>..........] - ETA: 0s - loss: 25.93211450/1819 [======================>.......] - ETA: 0s - loss: 25.90971650/1819 [==========================>...] - ETA: 0s - loss: 25.8291
Epoch 00049: val_loss did not improve from 22.94482
1819/1819 [==============================] - 1s 289us/sample - loss: 25.7563 - val_loss: 22.9484
Epoch 00049: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 1940 samples, validate on 121 samples
Epoch 1/50
  50/1940 [..............................] - ETA: 46s - loss: 158.0314 200/1940 [==>...........................] - ETA: 11s - loss: 157.4516 400/1940 [=====>........................] - ETA: 5s - loss: 155.1577  600/1940 [========>.....................] - ETA: 3s - loss: 152.3719 800/1940 [===========>..................] - ETA: 2s - loss: 146.73051000/1940 [==============>...............] - ETA: 1s - loss: 134.32931200/1940 [=================>............] - ETA: 0s - loss: 121.35151400/1940 [====================>.........] - ETA: 0s - loss: 110.75741600/1940 [=======================>......] - ETA: 0s - loss: 102.48431800/1940 [==========================>...] - ETA: 0s - loss: 95.9356 
Epoch 00001: val_loss improved from inf to 60.14426, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 2s 1ms/sample - loss: 91.8946 - val_loss: 60.1443
Epoch 2/50
  50/1940 [..............................] - ETA: 0s - loss: 41.0269 250/1940 [==>...........................] - ETA: 0s - loss: 39.6031 450/1940 [=====>........................] - ETA: 0s - loss: 38.0480 650/1940 [=========>....................] - ETA: 0s - loss: 38.1789 850/1940 [============>.................] - ETA: 0s - loss: 37.28871050/1940 [===============>..............] - ETA: 0s - loss: 36.79541250/1940 [==================>...........] - ETA: 0s - loss: 36.55731450/1940 [=====================>........] - ETA: 0s - loss: 36.07011650/1940 [========================>.....] - ETA: 0s - loss: 35.57651850/1940 [===========================>..] - ETA: 0s - loss: 35.2046
Epoch 00002: val_loss did not improve from 60.14426
1940/1940 [==============================] - 1s 289us/sample - loss: 35.0809 - val_loss: 62.1588
Epoch 3/50
  50/1940 [..............................] - ETA: 0s - loss: 29.5819 250/1940 [==>...........................] - ETA: 0s - loss: 31.0285 450/1940 [=====>........................] - ETA: 0s - loss: 30.9888 650/1940 [=========>....................] - ETA: 0s - loss: 30.5293 850/1940 [============>.................] - ETA: 0s - loss: 30.51571050/1940 [===============>..............] - ETA: 0s - loss: 30.22991250/1940 [==================>...........] - ETA: 0s - loss: 30.41541450/1940 [=====================>........] - ETA: 0s - loss: 30.49791650/1940 [========================>.....] - ETA: 0s - loss: 30.45531850/1940 [===========================>..] - ETA: 0s - loss: 30.4609
Epoch 00003: val_loss improved from 60.14426 to 45.02815, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 298us/sample - loss: 30.4148 - val_loss: 45.0282
Epoch 4/50
  50/1940 [..............................] - ETA: 0s - loss: 29.8719 250/1940 [==>...........................] - ETA: 0s - loss: 28.9594 500/1940 [======>.......................] - ETA: 0s - loss: 28.3885 700/1940 [=========>....................] - ETA: 0s - loss: 28.2131 950/1940 [=============>................] - ETA: 0s - loss: 28.20521200/1940 [=================>............] - ETA: 0s - loss: 28.30771450/1940 [=====================>........] - ETA: 0s - loss: 28.58041700/1940 [=========================>....] - ETA: 0s - loss: 28.6899
Epoch 00004: val_loss improved from 45.02815 to 40.30952, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 273us/sample - loss: 28.6178 - val_loss: 40.3095
Epoch 5/50
  50/1940 [..............................] - ETA: 0s - loss: 32.0652 250/1940 [==>...........................] - ETA: 0s - loss: 28.1474 500/1940 [======>.......................] - ETA: 0s - loss: 27.9684 750/1940 [==========>...................] - ETA: 0s - loss: 28.3887 900/1940 [============>.................] - ETA: 0s - loss: 28.43541150/1940 [================>.............] - ETA: 0s - loss: 28.37981400/1940 [====================>.........] - ETA: 0s - loss: 28.12831650/1940 [========================>.....] - ETA: 0s - loss: 27.87581900/1940 [============================>.] - ETA: 0s - loss: 27.6563
Epoch 00005: val_loss improved from 40.30952 to 32.48107, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 285us/sample - loss: 27.6510 - val_loss: 32.4811
Epoch 6/50
  50/1940 [..............................] - ETA: 0s - loss: 27.1892 300/1940 [===>..........................] - ETA: 0s - loss: 27.4091 550/1940 [=======>......................] - ETA: 0s - loss: 26.9934 800/1940 [===========>..................] - ETA: 0s - loss: 27.62981050/1940 [===============>..............] - ETA: 0s - loss: 27.65931300/1940 [===================>..........] - ETA: 0s - loss: 27.54711550/1940 [======================>.......] - ETA: 0s - loss: 27.42701800/1940 [==========================>...] - ETA: 0s - loss: 27.2838
Epoch 00006: val_loss improved from 32.48107 to 26.48225, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 259us/sample - loss: 27.3123 - val_loss: 26.4823
Epoch 7/50
  50/1940 [..............................] - ETA: 0s - loss: 27.3601 300/1940 [===>..........................] - ETA: 0s - loss: 28.2608 550/1940 [=======>......................] - ETA: 0s - loss: 27.7663 800/1940 [===========>..................] - ETA: 0s - loss: 27.62521050/1940 [===============>..............] - ETA: 0s - loss: 27.37811300/1940 [===================>..........] - ETA: 0s - loss: 27.45321550/1940 [======================>.......] - ETA: 0s - loss: 27.29821800/1940 [==========================>...] - ETA: 0s - loss: 27.1975
Epoch 00007: val_loss improved from 26.48225 to 26.18490, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 269us/sample - loss: 27.1362 - val_loss: 26.1849
Epoch 8/50
  50/1940 [..............................] - ETA: 0s - loss: 25.4829 250/1940 [==>...........................] - ETA: 0s - loss: 26.5769 450/1940 [=====>........................] - ETA: 0s - loss: 26.9631 650/1940 [=========>....................] - ETA: 0s - loss: 26.6801 850/1940 [============>.................] - ETA: 0s - loss: 26.80671050/1940 [===============>..............] - ETA: 0s - loss: 26.77521250/1940 [==================>...........] - ETA: 0s - loss: 26.81161450/1940 [=====================>........] - ETA: 0s - loss: 26.95541650/1940 [========================>.....] - ETA: 0s - loss: 27.17531850/1940 [===========================>..] - ETA: 0s - loss: 27.2266
Epoch 00008: val_loss improved from 26.18490 to 25.95848, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 310us/sample - loss: 27.2101 - val_loss: 25.9585
Epoch 9/50
  50/1940 [..............................] - ETA: 0s - loss: 24.9405 250/1940 [==>...........................] - ETA: 0s - loss: 26.0548 450/1940 [=====>........................] - ETA: 0s - loss: 26.1968 650/1940 [=========>....................] - ETA: 0s - loss: 26.0052 850/1940 [============>.................] - ETA: 0s - loss: 26.30071050/1940 [===============>..............] - ETA: 0s - loss: 26.44441250/1940 [==================>...........] - ETA: 0s - loss: 26.54021450/1940 [=====================>........] - ETA: 0s - loss: 26.53681650/1940 [========================>.....] - ETA: 0s - loss: 26.45291850/1940 [===========================>..] - ETA: 0s - loss: 26.4058
Epoch 00009: val_loss improved from 25.95848 to 24.38368, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 400us/sample - loss: 26.5040 - val_loss: 24.3837
Epoch 10/50
  50/1940 [..............................] - ETA: 0s - loss: 26.5611 250/1940 [==>...........................] - ETA: 0s - loss: 25.9493 450/1940 [=====>........................] - ETA: 0s - loss: 26.4336 650/1940 [=========>....................] - ETA: 0s - loss: 26.3284 850/1940 [============>.................] - ETA: 0s - loss: 26.14581050/1940 [===============>..............] - ETA: 0s - loss: 26.23171250/1940 [==================>...........] - ETA: 0s - loss: 26.24381450/1940 [=====================>........] - ETA: 0s - loss: 26.18491650/1940 [========================>.....] - ETA: 0s - loss: 26.38641850/1940 [===========================>..] - ETA: 0s - loss: 26.5728
Epoch 00010: val_loss did not improve from 24.38368
1940/1940 [==============================] - 1s 331us/sample - loss: 26.7208 - val_loss: 25.3027
Epoch 11/50
  50/1940 [..............................] - ETA: 0s - loss: 28.6090 250/1940 [==>...........................] - ETA: 0s - loss: 26.5930 450/1940 [=====>........................] - ETA: 0s - loss: 26.4733 650/1940 [=========>....................] - ETA: 0s - loss: 27.1029 850/1940 [============>.................] - ETA: 0s - loss: 27.14001050/1940 [===============>..............] - ETA: 0s - loss: 27.27971250/1940 [==================>...........] - ETA: 0s - loss: 27.17531450/1940 [=====================>........] - ETA: 0s - loss: 27.02301650/1940 [========================>.....] - ETA: 0s - loss: 27.15991850/1940 [===========================>..] - ETA: 0s - loss: 27.1178
Epoch 00011: val_loss did not improve from 24.38368

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1940/1940 [==============================] - 1s 309us/sample - loss: 27.0232 - val_loss: 26.4458
Epoch 12/50
  50/1940 [..............................] - ETA: 0s - loss: 25.8397 250/1940 [==>...........................] - ETA: 0s - loss: 26.6332 450/1940 [=====>........................] - ETA: 0s - loss: 26.2210 650/1940 [=========>....................] - ETA: 0s - loss: 26.1349 850/1940 [============>.................] - ETA: 0s - loss: 26.20811050/1940 [===============>..............] - ETA: 0s - loss: 26.28461250/1940 [==================>...........] - ETA: 0s - loss: 26.20111450/1940 [=====================>........] - ETA: 0s - loss: 26.15541650/1940 [========================>.....] - ETA: 0s - loss: 26.16411850/1940 [===========================>..] - ETA: 0s - loss: 26.1871
Epoch 00012: val_loss improved from 24.38368 to 23.43334, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 310us/sample - loss: 26.1193 - val_loss: 23.4333
Epoch 13/50
  50/1940 [..............................] - ETA: 0s - loss: 25.3722 250/1940 [==>...........................] - ETA: 0s - loss: 26.1629 450/1940 [=====>........................] - ETA: 0s - loss: 26.4190 650/1940 [=========>....................] - ETA: 0s - loss: 26.1311 850/1940 [============>.................] - ETA: 0s - loss: 25.84581050/1940 [===============>..............] - ETA: 0s - loss: 25.89261250/1940 [==================>...........] - ETA: 0s - loss: 25.82011450/1940 [=====================>........] - ETA: 0s - loss: 25.81881650/1940 [========================>.....] - ETA: 0s - loss: 25.76821850/1940 [===========================>..] - ETA: 0s - loss: 25.8856
Epoch 00013: val_loss did not improve from 23.43334
1940/1940 [==============================] - 1s 304us/sample - loss: 25.8766 - val_loss: 23.6939
Epoch 14/50
  50/1940 [..............................] - ETA: 0s - loss: 26.8221 250/1940 [==>...........................] - ETA: 0s - loss: 26.3810 450/1940 [=====>........................] - ETA: 0s - loss: 25.7814 650/1940 [=========>....................] - ETA: 0s - loss: 25.7305 850/1940 [============>.................] - ETA: 0s - loss: 26.06601050/1940 [===============>..............] - ETA: 0s - loss: 26.15571250/1940 [==================>...........] - ETA: 0s - loss: 26.08271450/1940 [=====================>........] - ETA: 0s - loss: 25.99871650/1940 [========================>.....] - ETA: 0s - loss: 26.09801850/1940 [===========================>..] - ETA: 0s - loss: 26.1856
Epoch 00014: val_loss improved from 23.43334 to 23.23114, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 313us/sample - loss: 26.2212 - val_loss: 23.2311
Epoch 15/50
  50/1940 [..............................] - ETA: 0s - loss: 26.2059 250/1940 [==>...........................] - ETA: 0s - loss: 26.2362 450/1940 [=====>........................] - ETA: 0s - loss: 26.5965 650/1940 [=========>....................] - ETA: 0s - loss: 26.6040 850/1940 [============>.................] - ETA: 0s - loss: 26.49431050/1940 [===============>..............] - ETA: 0s - loss: 26.51121250/1940 [==================>...........] - ETA: 0s - loss: 26.40221450/1940 [=====================>........] - ETA: 0s - loss: 26.35321650/1940 [========================>.....] - ETA: 0s - loss: 26.30881850/1940 [===========================>..] - ETA: 0s - loss: 26.4410
Epoch 00015: val_loss did not improve from 23.23114
1940/1940 [==============================] - 1s 300us/sample - loss: 26.3974 - val_loss: 23.2692
Epoch 16/50
  50/1940 [..............................] - ETA: 0s - loss: 25.5279 250/1940 [==>...........................] - ETA: 0s - loss: 26.8869 450/1940 [=====>........................] - ETA: 0s - loss: 26.3362 650/1940 [=========>....................] - ETA: 0s - loss: 26.3204 850/1940 [============>.................] - ETA: 0s - loss: 25.89401050/1940 [===============>..............] - ETA: 0s - loss: 25.86071250/1940 [==================>...........] - ETA: 0s - loss: 25.88791450/1940 [=====================>........] - ETA: 0s - loss: 25.79181650/1940 [========================>.....] - ETA: 0s - loss: 25.79921850/1940 [===========================>..] - ETA: 0s - loss: 25.8072
Epoch 00016: val_loss improved from 23.23114 to 23.09297, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 316us/sample - loss: 25.8075 - val_loss: 23.0930
Epoch 17/50
  50/1940 [..............................] - ETA: 0s - loss: 24.9559 250/1940 [==>...........................] - ETA: 0s - loss: 25.6685 450/1940 [=====>........................] - ETA: 0s - loss: 25.7497 650/1940 [=========>....................] - ETA: 0s - loss: 25.9154 850/1940 [============>.................] - ETA: 0s - loss: 25.70351050/1940 [===============>..............] - ETA: 0s - loss: 25.69221250/1940 [==================>...........] - ETA: 0s - loss: 25.60391450/1940 [=====================>........] - ETA: 0s - loss: 25.52931650/1940 [========================>.....] - ETA: 0s - loss: 25.58841850/1940 [===========================>..] - ETA: 0s - loss: 25.6468
Epoch 00017: val_loss improved from 23.09297 to 23.03039, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 305us/sample - loss: 25.6572 - val_loss: 23.0304
Epoch 18/50
  50/1940 [..............................] - ETA: 0s - loss: 26.3965 250/1940 [==>...........................] - ETA: 0s - loss: 25.3580 450/1940 [=====>........................] - ETA: 0s - loss: 26.1073 650/1940 [=========>....................] - ETA: 0s - loss: 25.8524 850/1940 [============>.................] - ETA: 0s - loss: 25.78481050/1940 [===============>..............] - ETA: 0s - loss: 25.91381250/1940 [==================>...........] - ETA: 0s - loss: 25.98591450/1940 [=====================>........] - ETA: 0s - loss: 25.92491650/1940 [========================>.....] - ETA: 0s - loss: 25.93881850/1940 [===========================>..] - ETA: 0s - loss: 25.9254
Epoch 00018: val_loss improved from 23.03039 to 23.02903, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 304us/sample - loss: 25.9574 - val_loss: 23.0290
Epoch 19/50
  50/1940 [..............................] - ETA: 0s - loss: 27.9275 250/1940 [==>...........................] - ETA: 0s - loss: 25.6721 450/1940 [=====>........................] - ETA: 0s - loss: 25.8084 650/1940 [=========>....................] - ETA: 0s - loss: 25.7276 850/1940 [============>.................] - ETA: 0s - loss: 25.72801050/1940 [===============>..............] - ETA: 0s - loss: 25.70811250/1940 [==================>...........] - ETA: 0s - loss: 25.79181450/1940 [=====================>........] - ETA: 0s - loss: 25.80101650/1940 [========================>.....] - ETA: 0s - loss: 25.80231850/1940 [===========================>..] - ETA: 0s - loss: 25.7471
Epoch 00019: val_loss improved from 23.02903 to 22.97198, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 308us/sample - loss: 25.7344 - val_loss: 22.9720
Epoch 20/50
  50/1940 [..............................] - ETA: 0s - loss: 26.1489 250/1940 [==>...........................] - ETA: 0s - loss: 25.3424 450/1940 [=====>........................] - ETA: 0s - loss: 25.7929 650/1940 [=========>....................] - ETA: 0s - loss: 25.4935 850/1940 [============>.................] - ETA: 0s - loss: 25.47621050/1940 [===============>..............] - ETA: 0s - loss: 25.48971250/1940 [==================>...........] - ETA: 0s - loss: 25.59781450/1940 [=====================>........] - ETA: 0s - loss: 25.53361650/1940 [========================>.....] - ETA: 0s - loss: 25.46751850/1940 [===========================>..] - ETA: 0s - loss: 25.5263
Epoch 00020: val_loss improved from 22.97198 to 22.82728, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 306us/sample - loss: 25.5470 - val_loss: 22.8273
Epoch 21/50
  50/1940 [..............................] - ETA: 0s - loss: 26.1670 250/1940 [==>...........................] - ETA: 0s - loss: 25.5635 450/1940 [=====>........................] - ETA: 0s - loss: 26.2717 650/1940 [=========>....................] - ETA: 0s - loss: 25.8898 850/1940 [============>.................] - ETA: 0s - loss: 25.75001050/1940 [===============>..............] - ETA: 0s - loss: 25.68061250/1940 [==================>...........] - ETA: 0s - loss: 25.81251450/1940 [=====================>........] - ETA: 0s - loss: 25.74401650/1940 [========================>.....] - ETA: 0s - loss: 25.78221850/1940 [===========================>..] - ETA: 0s - loss: 25.6851
Epoch 00021: val_loss did not improve from 22.82728
1940/1940 [==============================] - 1s 289us/sample - loss: 25.6833 - val_loss: 24.1029
Epoch 22/50
  50/1940 [..............................] - ETA: 0s - loss: 25.7891 250/1940 [==>...........................] - ETA: 0s - loss: 25.7156 450/1940 [=====>........................] - ETA: 0s - loss: 26.5907 650/1940 [=========>....................] - ETA: 0s - loss: 26.6576 850/1940 [============>.................] - ETA: 0s - loss: 26.49421050/1940 [===============>..............] - ETA: 0s - loss: 26.53751250/1940 [==================>...........] - ETA: 0s - loss: 26.26641450/1940 [=====================>........] - ETA: 0s - loss: 26.07871650/1940 [========================>.....] - ETA: 0s - loss: 25.97751850/1940 [===========================>..] - ETA: 0s - loss: 26.0018
Epoch 00022: val_loss improved from 22.82728 to 22.71822, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 308us/sample - loss: 25.9852 - val_loss: 22.7182
Epoch 23/50
  50/1940 [..............................] - ETA: 0s - loss: 25.1640 250/1940 [==>...........................] - ETA: 0s - loss: 26.2899 450/1940 [=====>........................] - ETA: 0s - loss: 26.1712 650/1940 [=========>....................] - ETA: 0s - loss: 25.8490 850/1940 [============>.................] - ETA: 0s - loss: 25.79171050/1940 [===============>..............] - ETA: 0s - loss: 25.75331250/1940 [==================>...........] - ETA: 0s - loss: 25.57871450/1940 [=====================>........] - ETA: 0s - loss: 25.69501650/1940 [========================>.....] - ETA: 0s - loss: 25.67521800/1940 [==========================>...] - ETA: 0s - loss: 25.6293
Epoch 00023: val_loss did not improve from 22.71822
1940/1940 [==============================] - 1s 311us/sample - loss: 25.6351 - val_loss: 22.8995
Epoch 24/50
  50/1940 [..............................] - ETA: 0s - loss: 26.2229 250/1940 [==>...........................] - ETA: 0s - loss: 25.3130 450/1940 [=====>........................] - ETA: 0s - loss: 25.0779 650/1940 [=========>....................] - ETA: 0s - loss: 25.5082 850/1940 [============>.................] - ETA: 0s - loss: 25.28451050/1940 [===============>..............] - ETA: 0s - loss: 25.51321250/1940 [==================>...........] - ETA: 0s - loss: 25.35781450/1940 [=====================>........] - ETA: 0s - loss: 25.46151650/1940 [========================>.....] - ETA: 0s - loss: 25.55231850/1940 [===========================>..] - ETA: 0s - loss: 25.6027
Epoch 00024: val_loss did not improve from 22.71822

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1940/1940 [==============================] - 1s 296us/sample - loss: 25.6165 - val_loss: 22.8198
Epoch 25/50
  50/1940 [..............................] - ETA: 0s - loss: 24.4617 250/1940 [==>...........................] - ETA: 0s - loss: 25.0240 450/1940 [=====>........................] - ETA: 0s - loss: 25.7712 650/1940 [=========>....................] - ETA: 0s - loss: 25.9771 850/1940 [============>.................] - ETA: 0s - loss: 25.90671050/1940 [===============>..............] - ETA: 0s - loss: 25.80591250/1940 [==================>...........] - ETA: 0s - loss: 25.83111450/1940 [=====================>........] - ETA: 0s - loss: 25.80011650/1940 [========================>.....] - ETA: 0s - loss: 25.67691850/1940 [===========================>..] - ETA: 0s - loss: 25.5698
Epoch 00025: val_loss did not improve from 22.71822
1940/1940 [==============================] - 1s 298us/sample - loss: 25.6047 - val_loss: 22.9789
Epoch 26/50
  50/1940 [..............................] - ETA: 0s - loss: 24.9912 250/1940 [==>...........................] - ETA: 0s - loss: 25.8210 450/1940 [=====>........................] - ETA: 0s - loss: 25.8164 650/1940 [=========>....................] - ETA: 0s - loss: 25.5136 850/1940 [============>.................] - ETA: 0s - loss: 25.64391050/1940 [===============>..............] - ETA: 0s - loss: 25.38741250/1940 [==================>...........] - ETA: 0s - loss: 25.56101450/1940 [=====================>........] - ETA: 0s - loss: 25.42711650/1940 [========================>.....] - ETA: 0s - loss: 25.50901850/1940 [===========================>..] - ETA: 0s - loss: 25.5792
Epoch 00026: val_loss improved from 22.71822 to 22.64911, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 305us/sample - loss: 25.5582 - val_loss: 22.6491
Epoch 27/50
  50/1940 [..............................] - ETA: 0s - loss: 26.5756 250/1940 [==>...........................] - ETA: 0s - loss: 26.1730 450/1940 [=====>........................] - ETA: 0s - loss: 25.9942 650/1940 [=========>....................] - ETA: 0s - loss: 25.8646 850/1940 [============>.................] - ETA: 0s - loss: 25.61441050/1940 [===============>..............] - ETA: 0s - loss: 25.72721250/1940 [==================>...........] - ETA: 0s - loss: 25.38571450/1940 [=====================>........] - ETA: 0s - loss: 25.55851650/1940 [========================>.....] - ETA: 0s - loss: 25.61501850/1940 [===========================>..] - ETA: 0s - loss: 25.6582
Epoch 00027: val_loss improved from 22.64911 to 22.64456, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 350us/sample - loss: 25.6468 - val_loss: 22.6446
Epoch 28/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2696 250/1940 [==>...........................] - ETA: 0s - loss: 24.8499 450/1940 [=====>........................] - ETA: 0s - loss: 25.5328 650/1940 [=========>....................] - ETA: 0s - loss: 25.4794 850/1940 [============>.................] - ETA: 0s - loss: 25.69031050/1940 [===============>..............] - ETA: 0s - loss: 25.83501250/1940 [==================>...........] - ETA: 0s - loss: 25.97821450/1940 [=====================>........] - ETA: 0s - loss: 25.77921650/1940 [========================>.....] - ETA: 0s - loss: 25.73641850/1940 [===========================>..] - ETA: 0s - loss: 25.8544
Epoch 00028: val_loss did not improve from 22.64456
1940/1940 [==============================] - 1s 283us/sample - loss: 25.8523 - val_loss: 22.9079
Epoch 29/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2326 250/1940 [==>...........................] - ETA: 0s - loss: 24.7801 450/1940 [=====>........................] - ETA: 0s - loss: 24.9684 650/1940 [=========>....................] - ETA: 0s - loss: 25.1925 850/1940 [============>.................] - ETA: 0s - loss: 25.32991050/1940 [===============>..............] - ETA: 0s - loss: 25.43351250/1940 [==================>...........] - ETA: 0s - loss: 25.41511450/1940 [=====================>........] - ETA: 0s - loss: 25.46421650/1940 [========================>.....] - ETA: 0s - loss: 25.41261850/1940 [===========================>..] - ETA: 0s - loss: 25.4517
Epoch 00029: val_loss did not improve from 22.64456

Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1940/1940 [==============================] - 1s 288us/sample - loss: 25.5338 - val_loss: 22.6655
Epoch 30/50
  50/1940 [..............................] - ETA: 0s - loss: 27.9599 250/1940 [==>...........................] - ETA: 0s - loss: 27.1280 450/1940 [=====>........................] - ETA: 0s - loss: 26.7804 650/1940 [=========>....................] - ETA: 0s - loss: 26.3290 850/1940 [============>.................] - ETA: 0s - loss: 25.95551050/1940 [===============>..............] - ETA: 0s - loss: 25.87061250/1940 [==================>...........] - ETA: 0s - loss: 25.77111450/1940 [=====================>........] - ETA: 0s - loss: 25.73111650/1940 [========================>.....] - ETA: 0s - loss: 25.61121850/1940 [===========================>..] - ETA: 0s - loss: 25.4786
Epoch 00030: val_loss did not improve from 22.64456
1940/1940 [==============================] - 1s 292us/sample - loss: 25.4598 - val_loss: 22.8979
Epoch 31/50
  50/1940 [..............................] - ETA: 0s - loss: 23.4296 250/1940 [==>...........................] - ETA: 0s - loss: 24.6372 450/1940 [=====>........................] - ETA: 0s - loss: 25.0297 650/1940 [=========>....................] - ETA: 0s - loss: 24.9823 850/1940 [============>.................] - ETA: 0s - loss: 24.86801050/1940 [===============>..............] - ETA: 0s - loss: 25.17171250/1940 [==================>...........] - ETA: 0s - loss: 25.20741450/1940 [=====================>........] - ETA: 0s - loss: 25.28521650/1940 [========================>.....] - ETA: 0s - loss: 25.24371850/1940 [===========================>..] - ETA: 0s - loss: 25.5026
Epoch 00031: val_loss improved from 22.64456 to 22.61360, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1940/1940 [==============================] - 1s 313us/sample - loss: 25.4949 - val_loss: 22.6136
Epoch 32/50
  50/1940 [..............................] - ETA: 0s - loss: 25.7486 250/1940 [==>...........................] - ETA: 0s - loss: 25.9329 450/1940 [=====>........................] - ETA: 0s - loss: 26.1279 650/1940 [=========>....................] - ETA: 0s - loss: 25.6814 850/1940 [============>.................] - ETA: 0s - loss: 25.72081050/1940 [===============>..............] - ETA: 0s - loss: 25.59061250/1940 [==================>...........] - ETA: 0s - loss: 25.47491450/1940 [=====================>........] - ETA: 0s - loss: 25.40541650/1940 [========================>.....] - ETA: 0s - loss: 25.41531850/1940 [===========================>..] - ETA: 0s - loss: 25.5805
Epoch 00032: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 291us/sample - loss: 25.5803 - val_loss: 22.6802
Epoch 33/50
  50/1940 [..............................] - ETA: 0s - loss: 25.3074 250/1940 [==>...........................] - ETA: 0s - loss: 25.4439 450/1940 [=====>........................] - ETA: 0s - loss: 25.8373 650/1940 [=========>....................] - ETA: 0s - loss: 25.7752 850/1940 [============>.................] - ETA: 0s - loss: 25.52391050/1940 [===============>..............] - ETA: 0s - loss: 25.52811250/1940 [==================>...........] - ETA: 0s - loss: 25.68871450/1940 [=====================>........] - ETA: 0s - loss: 25.64641650/1940 [========================>.....] - ETA: 0s - loss: 25.49971850/1940 [===========================>..] - ETA: 0s - loss: 25.5878
Epoch 00033: val_loss did not improve from 22.61360

Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1940/1940 [==============================] - 1s 292us/sample - loss: 25.5450 - val_loss: 22.7833
Epoch 34/50
  50/1940 [..............................] - ETA: 0s - loss: 23.9605 250/1940 [==>...........................] - ETA: 0s - loss: 25.3808 450/1940 [=====>........................] - ETA: 0s - loss: 25.3765 650/1940 [=========>....................] - ETA: 0s - loss: 25.4132 850/1940 [============>.................] - ETA: 0s - loss: 25.60661050/1940 [===============>..............] - ETA: 0s - loss: 25.71021250/1940 [==================>...........] - ETA: 0s - loss: 25.72601450/1940 [=====================>........] - ETA: 0s - loss: 25.61961650/1940 [========================>.....] - ETA: 0s - loss: 25.58251850/1940 [===========================>..] - ETA: 0s - loss: 25.5407
Epoch 00034: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 289us/sample - loss: 25.4914 - val_loss: 22.6321
Epoch 35/50
  50/1940 [..............................] - ETA: 0s - loss: 24.1501 250/1940 [==>...........................] - ETA: 0s - loss: 24.7413 450/1940 [=====>........................] - ETA: 0s - loss: 25.1814 650/1940 [=========>....................] - ETA: 0s - loss: 24.9680 850/1940 [============>.................] - ETA: 0s - loss: 24.90451050/1940 [===============>..............] - ETA: 0s - loss: 25.27501250/1940 [==================>...........] - ETA: 0s - loss: 25.41001450/1940 [=====================>........] - ETA: 0s - loss: 25.26431650/1940 [========================>.....] - ETA: 0s - loss: 25.27361850/1940 [===========================>..] - ETA: 0s - loss: 25.4271
Epoch 00035: val_loss did not improve from 22.61360

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1940/1940 [==============================] - 1s 288us/sample - loss: 25.4857 - val_loss: 22.6420
Epoch 36/50
  50/1940 [..............................] - ETA: 0s - loss: 22.5520 250/1940 [==>...........................] - ETA: 0s - loss: 24.2120 450/1940 [=====>........................] - ETA: 0s - loss: 25.2681 650/1940 [=========>....................] - ETA: 0s - loss: 25.4292 850/1940 [============>.................] - ETA: 0s - loss: 25.53851050/1940 [===============>..............] - ETA: 0s - loss: 25.48451250/1940 [==================>...........] - ETA: 0s - loss: 25.51741450/1940 [=====================>........] - ETA: 0s - loss: 25.50681650/1940 [========================>.....] - ETA: 0s - loss: 25.54951850/1940 [===========================>..] - ETA: 0s - loss: 25.5555
Epoch 00036: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 291us/sample - loss: 25.5275 - val_loss: 22.6322
Epoch 37/50
  50/1940 [..............................] - ETA: 0s - loss: 23.7164 250/1940 [==>...........................] - ETA: 0s - loss: 25.1168 450/1940 [=====>........................] - ETA: 0s - loss: 26.0284 650/1940 [=========>....................] - ETA: 0s - loss: 25.8474 850/1940 [============>.................] - ETA: 0s - loss: 25.61401050/1940 [===============>..............] - ETA: 0s - loss: 25.43951250/1940 [==================>...........] - ETA: 0s - loss: 25.40691450/1940 [=====================>........] - ETA: 0s - loss: 25.56351650/1940 [========================>.....] - ETA: 0s - loss: 25.49191850/1940 [===========================>..] - ETA: 0s - loss: 25.5102
Epoch 00037: val_loss did not improve from 22.61360

Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-05.
1940/1940 [==============================] - 1s 286us/sample - loss: 25.4550 - val_loss: 22.6794
Epoch 38/50
  50/1940 [..............................] - ETA: 0s - loss: 25.0197 250/1940 [==>...........................] - ETA: 0s - loss: 25.1881 450/1940 [=====>........................] - ETA: 0s - loss: 25.0003 650/1940 [=========>....................] - ETA: 0s - loss: 24.7498 850/1940 [============>.................] - ETA: 0s - loss: 24.78611050/1940 [===============>..............] - ETA: 0s - loss: 24.83901250/1940 [==================>...........] - ETA: 0s - loss: 25.18081450/1940 [=====================>........] - ETA: 0s - loss: 25.41211650/1940 [========================>.....] - ETA: 0s - loss: 25.43251850/1940 [===========================>..] - ETA: 0s - loss: 25.3769
Epoch 00038: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 292us/sample - loss: 25.4005 - val_loss: 22.6417
Epoch 39/50
  50/1940 [..............................] - ETA: 0s - loss: 23.9221 250/1940 [==>...........................] - ETA: 0s - loss: 25.0160 450/1940 [=====>........................] - ETA: 0s - loss: 25.0877 650/1940 [=========>....................] - ETA: 0s - loss: 25.5184 850/1940 [============>.................] - ETA: 0s - loss: 25.63611050/1940 [===============>..............] - ETA: 0s - loss: 25.74121250/1940 [==================>...........] - ETA: 0s - loss: 25.70331450/1940 [=====================>........] - ETA: 0s - loss: 25.56111650/1940 [========================>.....] - ETA: 0s - loss: 25.47251850/1940 [===========================>..] - ETA: 0s - loss: 25.4482
Epoch 00039: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 288us/sample - loss: 25.4146 - val_loss: 22.6484
Epoch 40/50
  50/1940 [..............................] - ETA: 0s - loss: 25.3787 250/1940 [==>...........................] - ETA: 0s - loss: 25.5463 450/1940 [=====>........................] - ETA: 0s - loss: 25.7299 650/1940 [=========>....................] - ETA: 0s - loss: 25.4736 850/1940 [============>.................] - ETA: 0s - loss: 25.69341050/1940 [===============>..............] - ETA: 0s - loss: 25.42621250/1940 [==================>...........] - ETA: 0s - loss: 25.41521450/1940 [=====================>........] - ETA: 0s - loss: 25.46691650/1940 [========================>.....] - ETA: 0s - loss: 25.43921850/1940 [===========================>..] - ETA: 0s - loss: 25.4047
Epoch 00040: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 290us/sample - loss: 25.4184 - val_loss: 22.6545
Epoch 41/50
  50/1940 [..............................] - ETA: 0s - loss: 26.8773 250/1940 [==>...........................] - ETA: 0s - loss: 25.9632 450/1940 [=====>........................] - ETA: 0s - loss: 25.6813 650/1940 [=========>....................] - ETA: 0s - loss: 26.0329 850/1940 [============>.................] - ETA: 0s - loss: 25.71991050/1940 [===============>..............] - ETA: 0s - loss: 25.61011250/1940 [==================>...........] - ETA: 0s - loss: 25.57881450/1940 [=====================>........] - ETA: 0s - loss: 25.66701650/1940 [========================>.....] - ETA: 0s - loss: 25.56031850/1940 [===========================>..] - ETA: 0s - loss: 25.5465
Epoch 00041: val_loss did not improve from 22.61360
1940/1940 [==============================] - 1s 291us/sample - loss: 25.5728 - val_loss: 22.6712
Epoch 00041: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b172ea979e8> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b172ea905f8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172ea90b38> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172ea90da0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172e66e7f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172ea88630> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172eaa5ef0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eab8080> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b172eab1908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eade5c0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eae6278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eae6d30> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b172eafaa90> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b172eafa9e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b172eb0d7b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb0d2e8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eb1fef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb1fb70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eb29e48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb299e8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b172eb33da0> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b172ea979e8> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b172ea905f8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172ea90b38> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172ea90da0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172e66e7f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172ea88630> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172eaa5ef0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eab8080> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b172eab1908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eade5c0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eae6278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eae6d30> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b172eafaa90> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b172eafa9e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b172eb0d7b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb0d2e8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eb1fef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb1fb70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eb29e48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb299e8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b172eb33da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b1717b2ec88> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eb4d3c8> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 168.5412
Epoch 00001: val_loss improved from inf to 102.97196, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 10ms/sample - loss: 155.6624 - val_loss: 102.9720
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 90.0284
Epoch 00002: val_loss improved from 102.97196 to 46.87771, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 729us/sample - loss: 72.3828 - val_loss: 46.8777
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.6342
Epoch 00003: val_loss improved from 46.87771 to 38.33020, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 746us/sample - loss: 39.8647 - val_loss: 38.3302
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 35.0711
Epoch 00004: val_loss improved from 38.33020 to 36.46937, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 712us/sample - loss: 36.4291 - val_loss: 36.4694
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 32.4913
Epoch 00005: val_loss improved from 36.46937 to 27.44549, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 31.7287 - val_loss: 27.4455
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0046
Epoch 00006: val_loss improved from 27.44549 to 26.43301, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 746us/sample - loss: 25.7938 - val_loss: 26.4330
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.9726
Epoch 00007: val_loss improved from 26.43301 to 25.50175, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 738us/sample - loss: 26.4157 - val_loss: 25.5018
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1861
Epoch 00008: val_loss did not improve from 25.50175
121/121 [==============================] - 0s 549us/sample - loss: 25.2821 - val_loss: 25.7915
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.1375
Epoch 00009: val_loss improved from 25.50175 to 25.23192, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 748us/sample - loss: 25.0367 - val_loss: 25.2319
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.6415
Epoch 00010: val_loss improved from 25.23192 to 25.12293, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 24.9614 - val_loss: 25.1229
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3945
Epoch 00011: val_loss did not improve from 25.12293
121/121 [==============================] - 0s 533us/sample - loss: 24.7688 - val_loss: 25.2046
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2758
Epoch 00012: val_loss improved from 25.12293 to 24.81534, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 678us/sample - loss: 24.6713 - val_loss: 24.8153
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1547
Epoch 00013: val_loss improved from 24.81534 to 24.56285, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 708us/sample - loss: 24.3846 - val_loss: 24.5628
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9415
Epoch 00014: val_loss improved from 24.56285 to 24.46572, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 712us/sample - loss: 24.1711 - val_loss: 24.4657
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9349
Epoch 00015: val_loss improved from 24.46572 to 24.30548, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 729us/sample - loss: 24.0048 - val_loss: 24.3055
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3155
Epoch 00016: val_loss improved from 24.30548 to 24.15021, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 703us/sample - loss: 23.9251 - val_loss: 24.1502
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7538
Epoch 00017: val_loss improved from 24.15021 to 23.98363, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 730us/sample - loss: 23.7116 - val_loss: 23.9836
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2866
Epoch 00018: val_loss improved from 23.98363 to 23.93904, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 677us/sample - loss: 23.6466 - val_loss: 23.9390
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7145
Epoch 00019: val_loss improved from 23.93904 to 23.82736, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 23.5349 - val_loss: 23.8274
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1578
Epoch 00020: val_loss improved from 23.82736 to 23.75293, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 680us/sample - loss: 23.4009 - val_loss: 23.7529
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6586
Epoch 00021: val_loss did not improve from 23.75293
121/121 [==============================] - 0s 520us/sample - loss: 23.4474 - val_loss: 23.8209
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.9013
Epoch 00022: val_loss improved from 23.75293 to 23.63034, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 697us/sample - loss: 23.2485 - val_loss: 23.6303
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1208
Epoch 00023: val_loss improved from 23.63034 to 23.58483, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 23.1890 - val_loss: 23.5848
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0962
Epoch 00024: val_loss did not improve from 23.58483
121/121 [==============================] - 0s 534us/sample - loss: 23.0994 - val_loss: 23.5955
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4430
Epoch 00025: val_loss improved from 23.58483 to 23.40853, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 668us/sample - loss: 23.0870 - val_loss: 23.4085
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9166
Epoch 00026: val_loss did not improve from 23.40853
121/121 [==============================] - 0s 534us/sample - loss: 23.0913 - val_loss: 23.5335
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9373
Epoch 00027: val_loss did not improve from 23.40853

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 524us/sample - loss: 22.9760 - val_loss: 23.4756
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0231
Epoch 00028: val_loss did not improve from 23.40853
121/121 [==============================] - 0s 530us/sample - loss: 22.9312 - val_loss: 23.4210
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4266
Epoch 00029: val_loss improved from 23.40853 to 23.31955, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 720us/sample - loss: 22.8806 - val_loss: 23.3196
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2103
Epoch 00030: val_loss did not improve from 23.31955
121/121 [==============================] - 0s 523us/sample - loss: 22.8980 - val_loss: 23.3274
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0070
Epoch 00031: val_loss did not improve from 23.31955

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 530us/sample - loss: 22.8793 - val_loss: 23.3304
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0027
Epoch 00032: val_loss improved from 23.31955 to 23.28036, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 706us/sample - loss: 22.8599 - val_loss: 23.2804
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0683
Epoch 00033: val_loss did not improve from 23.28036
121/121 [==============================] - 0s 531us/sample - loss: 22.8505 - val_loss: 23.3001
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2358
Epoch 00034: val_loss did not improve from 23.28036

Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 504us/sample - loss: 22.8438 - val_loss: 23.4398
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1174
Epoch 00035: val_loss did not improve from 23.28036
121/121 [==============================] - 0s 523us/sample - loss: 22.8570 - val_loss: 23.3838
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7223
Epoch 00036: val_loss did not improve from 23.28036

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 554us/sample - loss: 22.8358 - val_loss: 23.3185
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5461
Epoch 00037: val_loss did not improve from 23.28036
121/121 [==============================] - 0s 547us/sample - loss: 22.8232 - val_loss: 23.2960
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1774
Epoch 00038: val_loss did not improve from 23.28036

Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 527us/sample - loss: 22.8240 - val_loss: 23.2888
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2261
Epoch 00039: val_loss did not improve from 23.28036
121/121 [==============================] - 0s 517us/sample - loss: 22.8234 - val_loss: 23.2866
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9525
Epoch 00040: val_loss did not improve from 23.28036

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 528us/sample - loss: 22.8232 - val_loss: 23.2862
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4028
Epoch 00041: val_loss did not improve from 23.28036
121/121 [==============================] - 0s 543us/sample - loss: 22.8224 - val_loss: 23.2839
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0235
Epoch 00042: val_loss did not improve from 23.28036
121/121 [==============================] - 0s 536us/sample - loss: 22.8228 - val_loss: 23.2854
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:209: RuntimeWarning: divide by zero encountered in log
  predictor_matrix[:,:,:,indicesPRED] = numpy.log(predictor_matrix[:,:,:,indicesPRED])
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00042: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b172ee9ce48> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b172ee12978> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172ee12c50> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172eea3a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eeac518> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eea3e80> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172eec2e48> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172f2cbe48> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b172f2cb710> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f2fb3c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f2fbcf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f304c18> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b172f319898> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b172f3197f0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b172f3240b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f32a160> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172f33acf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f33a978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172f347c50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f3477f0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b172f34fba8> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b172ee9ce48> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b172ee12978> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172ee12c50> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172eea3a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172eeac518> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172eea3e80> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172eec2e48> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172f2cbe48> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b172f2cb710> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f2fb3c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f2fbcf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f304c18> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b172f319898> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b172f3197f0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b172f3240b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f32a160> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172f33acf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f33a978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172f347c50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f3477f0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b172f34fba8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b1717f01588> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172f380b38> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 73.1707
Epoch 00001: val_loss improved from inf to 27.67858, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 7ms/sample - loss: 54.4557 - val_loss: 27.6786
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 28.2433
Epoch 00002: val_loss did not improve from 27.67858
121/121 [==============================] - 0s 542us/sample - loss: 29.9193 - val_loss: 31.6036
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 32.1618
Epoch 00003: val_loss improved from 27.67858 to 26.46304, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 706us/sample - loss: 31.5936 - val_loss: 26.4630
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.3623
Epoch 00004: val_loss did not improve from 26.46304
121/121 [==============================] - 0s 587us/sample - loss: 26.4974 - val_loss: 26.5603
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7102
Epoch 00005: val_loss improved from 26.46304 to 25.66107, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 735us/sample - loss: 26.6302 - val_loss: 25.6611
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.9367
Epoch 00006: val_loss improved from 25.66107 to 24.89504, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 704us/sample - loss: 25.6428 - val_loss: 24.8950
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4857
Epoch 00007: val_loss improved from 24.89504 to 24.54623, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 756us/sample - loss: 24.9520 - val_loss: 24.5462
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 28.1115
Epoch 00008: val_loss improved from 24.54623 to 24.23376, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 726us/sample - loss: 24.6567 - val_loss: 24.2338
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3376
Epoch 00009: val_loss improved from 24.23376 to 24.16966, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 723us/sample - loss: 24.3642 - val_loss: 24.1697
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2894
Epoch 00010: val_loss improved from 24.16966 to 24.12641, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 731us/sample - loss: 24.2724 - val_loss: 24.1264
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7396
Epoch 00011: val_loss improved from 24.12641 to 23.93556, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 819us/sample - loss: 24.2542 - val_loss: 23.9356
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8012
Epoch 00012: val_loss did not improve from 23.93556
121/121 [==============================] - 0s 587us/sample - loss: 24.2633 - val_loss: 23.9448
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1641
Epoch 00013: val_loss improved from 23.93556 to 23.85506, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 758us/sample - loss: 23.9883 - val_loss: 23.8551
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.4580
Epoch 00014: val_loss improved from 23.85506 to 23.72634, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 774us/sample - loss: 23.8829 - val_loss: 23.7263
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8020
Epoch 00015: val_loss did not improve from 23.72634
121/121 [==============================] - 0s 556us/sample - loss: 23.8522 - val_loss: 23.7556
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5763
Epoch 00016: val_loss improved from 23.72634 to 23.63627, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 733us/sample - loss: 23.7707 - val_loss: 23.6363
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9474
Epoch 00017: val_loss improved from 23.63627 to 23.60168, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 729us/sample - loss: 23.7457 - val_loss: 23.6017
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6610
Epoch 00018: val_loss improved from 23.60168 to 23.59132, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 749us/sample - loss: 23.7719 - val_loss: 23.5913
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1112
Epoch 00019: val_loss improved from 23.59132 to 23.57535, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 23.6970 - val_loss: 23.5754
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2956
Epoch 00020: val_loss improved from 23.57535 to 23.48358, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 23.6240 - val_loss: 23.4836
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2170
Epoch 00021: val_loss improved from 23.48358 to 23.42703, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 699us/sample - loss: 23.5788 - val_loss: 23.4270
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4825
Epoch 00022: val_loss did not improve from 23.42703
121/121 [==============================] - 0s 562us/sample - loss: 23.5324 - val_loss: 23.4477
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8194
Epoch 00023: val_loss improved from 23.42703 to 23.39094, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 23.5363 - val_loss: 23.3909
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3112
Epoch 00024: val_loss did not improve from 23.39094
121/121 [==============================] - 0s 555us/sample - loss: 23.4747 - val_loss: 23.3989
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4465
Epoch 00025: val_loss did not improve from 23.39094

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 613us/sample - loss: 23.4864 - val_loss: 23.6970
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7531
Epoch 00026: val_loss did not improve from 23.39094
121/121 [==============================] - 0s 520us/sample - loss: 23.7595 - val_loss: 23.4246
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3772
Epoch 00027: val_loss improved from 23.39094 to 23.28307, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 691us/sample - loss: 23.4293 - val_loss: 23.2831
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8373
Epoch 00028: val_loss did not improve from 23.28307
121/121 [==============================] - 0s 547us/sample - loss: 23.3635 - val_loss: 23.3895
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2924
Epoch 00029: val_loss did not improve from 23.28307

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 566us/sample - loss: 23.4456 - val_loss: 23.2932
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3063
Epoch 00030: val_loss improved from 23.28307 to 23.22418, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 727us/sample - loss: 23.3150 - val_loss: 23.2242
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2963
Epoch 00031: val_loss did not improve from 23.22418
121/121 [==============================] - 0s 570us/sample - loss: 23.3665 - val_loss: 23.2242
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8396
Epoch 00032: val_loss did not improve from 23.22418

Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 602us/sample - loss: 23.3114 - val_loss: 23.2856
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6333
Epoch 00033: val_loss did not improve from 23.22418
121/121 [==============================] - 0s 558us/sample - loss: 23.3055 - val_loss: 23.2801
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5935
Epoch 00034: val_loss did not improve from 23.22418

Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 588us/sample - loss: 23.2949 - val_loss: 23.2445
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9245
Epoch 00035: val_loss did not improve from 23.22418
121/121 [==============================] - 0s 553us/sample - loss: 23.2828 - val_loss: 23.2293
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4817
Epoch 00036: val_loss improved from 23.22418 to 23.21931, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 741us/sample - loss: 23.2815 - val_loss: 23.2193
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6410
Epoch 00037: val_loss did not improve from 23.21931
121/121 [==============================] - 0s 563us/sample - loss: 23.2814 - val_loss: 23.2197
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0225
Epoch 00038: val_loss improved from 23.21931 to 23.21848, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 745us/sample - loss: 23.2805 - val_loss: 23.2185
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7402
Epoch 00039: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 587us/sample - loss: 23.2796 - val_loss: 23.2193
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.6503
Epoch 00040: val_loss did not improve from 23.21848

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 606us/sample - loss: 23.2777 - val_loss: 23.2247
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0832
Epoch 00041: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 547us/sample - loss: 23.2775 - val_loss: 23.2296
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3214
Epoch 00042: val_loss did not improve from 23.21848

Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 559us/sample - loss: 23.2789 - val_loss: 23.2367
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2347100/121 [=======================>......] - ETA: 0s - loss: 23.2874
Epoch 00043: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 967us/sample - loss: 23.2812 - val_loss: 23.2437
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2356
Epoch 00044: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 571us/sample - loss: 23.2818 - val_loss: 23.2413
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0060
Epoch 00045: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 556us/sample - loss: 23.2793 - val_loss: 23.2348
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.0893
Epoch 00046: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 537us/sample - loss: 23.2785 - val_loss: 23.2303
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4333
Epoch 00047: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 578us/sample - loss: 23.2771 - val_loss: 23.2271
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6246
Epoch 00048: val_loss did not improve from 23.21848
121/121 [==============================] - 0s 571us/sample - loss: 23.2765 - val_loss: 23.2284
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00048: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b172f606b38> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b172fa8aeb8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fa8afd0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172faa5978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fa8add8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faa5da0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172fab8f28> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fac0f28> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b172fac07f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faee438> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faeed68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faf9c88> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b172fb0c908> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b172fb0c860> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b172fb14128> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb1d1d0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fb30d68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb309e8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fb3bcc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb3b860> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b172fb42c18> False
Model: "model_8"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_5[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 57, 2)    578         conv2d_4[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b172f606b38> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b172fa8aeb8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fa8afd0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172faa5978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fa8add8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faa5da0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b172fab8f28> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fac0f28> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b172fac07f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faee438> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faeed68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172faf9c88> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b172fb0c908> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b172fb0c860> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b172fb14128> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb1d1d0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fb30d68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb309e8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b172fb3bcc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb3b860> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b172fb42c18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb6deb8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b172fb5e2b0> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/200
 20/242 [=>............................] - ETA: 34s - loss: 182.6806220/242 [==========================>...] - ETA: 0s - loss: 133.3955 
Epoch 00001: val_loss improved from inf to 64.94298, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 4s 15ms/sample - loss: 128.5685 - val_loss: 64.9430
Epoch 2/200
 20/242 [=>............................] - ETA: 0s - loss: 62.8203220/242 [==========================>...] - ETA: 0s - loss: 51.3606
Epoch 00002: val_loss improved from 64.94298 to 38.15264, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 507us/sample - loss: 50.3869 - val_loss: 38.1526
Epoch 3/200
 20/242 [=>............................] - ETA: 0s - loss: 37.4517220/242 [==========================>...] - ETA: 0s - loss: 34.2231
Epoch 00003: val_loss improved from 38.15264 to 25.94422, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 526us/sample - loss: 33.3762 - val_loss: 25.9442
Epoch 4/200
 20/242 [=>............................] - ETA: 0s - loss: 23.1604200/242 [=======================>......] - ETA: 0s - loss: 26.6042
Epoch 00004: val_loss improved from 25.94422 to 25.73576, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 535us/sample - loss: 26.4984 - val_loss: 25.7358
Epoch 5/200
 20/242 [=>............................] - ETA: 0s - loss: 24.1430220/242 [==========================>...] - ETA: 0s - loss: 25.0598
Epoch 00005: val_loss improved from 25.73576 to 24.90894, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 496us/sample - loss: 25.1640 - val_loss: 24.9089
Epoch 6/200
 20/242 [=>............................] - ETA: 0s - loss: 24.9786200/242 [=======================>......] - ETA: 0s - loss: 24.4354
Epoch 00006: val_loss improved from 24.90894 to 24.21027, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 665us/sample - loss: 24.4904 - val_loss: 24.2103
Epoch 7/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3913220/242 [==========================>...] - ETA: 0s - loss: 23.8605
Epoch 00007: val_loss improved from 24.21027 to 23.66661, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 579us/sample - loss: 23.8767 - val_loss: 23.6666
Epoch 8/200
 20/242 [=>............................] - ETA: 0s - loss: 24.5397220/242 [==========================>...] - ETA: 0s - loss: 23.4507
Epoch 00008: val_loss improved from 23.66661 to 23.37534, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 538us/sample - loss: 23.5223 - val_loss: 23.3753
Epoch 9/200
 20/242 [=>............................] - ETA: 0s - loss: 22.4969220/242 [==========================>...] - ETA: 0s - loss: 23.4507
Epoch 00009: val_loss improved from 23.37534 to 23.22189, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 495us/sample - loss: 23.3394 - val_loss: 23.2219
Epoch 10/200
 20/242 [=>............................] - ETA: 0s - loss: 22.9211220/242 [==========================>...] - ETA: 0s - loss: 23.2376
Epoch 00010: val_loss improved from 23.22189 to 23.08015, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 478us/sample - loss: 23.2149 - val_loss: 23.0801
Epoch 11/200
 20/242 [=>............................] - ETA: 0s - loss: 24.8601220/242 [==========================>...] - ETA: 0s - loss: 23.2618
Epoch 00011: val_loss improved from 23.08015 to 23.01727, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 497us/sample - loss: 23.0570 - val_loss: 23.0173
Epoch 12/200
 20/242 [=>............................] - ETA: 0s - loss: 21.9566220/242 [==========================>...] - ETA: 0s - loss: 22.7679
Epoch 00012: val_loss improved from 23.01727 to 22.98623, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 505us/sample - loss: 23.0118 - val_loss: 22.9862
Epoch 13/200
 20/242 [=>............................] - ETA: 0s - loss: 22.6705220/242 [==========================>...] - ETA: 0s - loss: 22.9541
Epoch 00013: val_loss did not improve from 22.98623
242/242 [==============================] - 0s 421us/sample - loss: 23.0276 - val_loss: 23.1222
Epoch 14/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3404220/242 [==========================>...] - ETA: 0s - loss: 22.8649
Epoch 00014: val_loss improved from 22.98623 to 22.87474, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 491us/sample - loss: 22.9839 - val_loss: 22.8747
Epoch 15/200
 20/242 [=>............................] - ETA: 0s - loss: 23.2627220/242 [==========================>...] - ETA: 0s - loss: 23.2063
Epoch 00015: val_loss did not improve from 22.87474
242/242 [==============================] - 0s 406us/sample - loss: 22.9064 - val_loss: 22.9116
Epoch 16/200
 20/242 [=>............................] - ETA: 0s - loss: 20.5001220/242 [==========================>...] - ETA: 0s - loss: 22.8883
Epoch 00016: val_loss did not improve from 22.87474

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 425us/sample - loss: 22.8886 - val_loss: 22.8791
Epoch 17/200
 20/242 [=>............................] - ETA: 0s - loss: 21.9294220/242 [==========================>...] - ETA: 0s - loss: 22.8977
Epoch 00017: val_loss did not improve from 22.87474
242/242 [==============================] - 0s 426us/sample - loss: 22.8854 - val_loss: 22.9282
Epoch 18/200
 20/242 [=>............................] - ETA: 0s - loss: 23.3965220/242 [==========================>...] - ETA: 0s - loss: 22.7436
Epoch 00018: val_loss improved from 22.87474 to 22.81775, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 502us/sample - loss: 22.8055 - val_loss: 22.8177
Epoch 19/200
 20/242 [=>............................] - ETA: 0s - loss: 21.8760220/242 [==========================>...] - ETA: 0s - loss: 22.6500
Epoch 00019: val_loss did not improve from 22.81775
242/242 [==============================] - 0s 407us/sample - loss: 22.7875 - val_loss: 22.8594
Epoch 20/200
 20/242 [=>............................] - ETA: 0s - loss: 21.3393220/242 [==========================>...] - ETA: 0s - loss: 22.8361
Epoch 00020: val_loss improved from 22.81775 to 22.80899, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 501us/sample - loss: 22.7752 - val_loss: 22.8090
Epoch 21/200
 20/242 [=>............................] - ETA: 0s - loss: 21.5078200/242 [=======================>......] - ETA: 0s - loss: 22.7937
Epoch 00021: val_loss did not improve from 22.80899
242/242 [==============================] - 0s 424us/sample - loss: 22.7811 - val_loss: 22.8437
Epoch 22/200
 20/242 [=>............................] - ETA: 0s - loss: 22.6200220/242 [==========================>...] - ETA: 0s - loss: 22.7161
Epoch 00022: val_loss improved from 22.80899 to 22.80048, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 514us/sample - loss: 22.7734 - val_loss: 22.8005
Epoch 23/200
 20/242 [=>............................] - ETA: 0s - loss: 24.9510220/242 [==========================>...] - ETA: 0s - loss: 22.8127
Epoch 00023: val_loss did not improve from 22.80048
242/242 [==============================] - 0s 418us/sample - loss: 22.7505 - val_loss: 22.8028
Epoch 24/200
 20/242 [=>............................] - ETA: 0s - loss: 22.4634200/242 [=======================>......] - ETA: 0s - loss: 22.7244
Epoch 00024: val_loss did not improve from 22.80048

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 434us/sample - loss: 22.7459 - val_loss: 22.8176
Epoch 25/200
 20/242 [=>............................] - ETA: 0s - loss: 26.1222220/242 [==========================>...] - ETA: 0s - loss: 23.0762
Epoch 00025: val_loss improved from 22.80048 to 22.78362, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 534us/sample - loss: 22.7375 - val_loss: 22.7836
Epoch 26/200
 20/242 [=>............................] - ETA: 0s - loss: 19.8312220/242 [==========================>...] - ETA: 0s - loss: 22.3775
Epoch 00026: val_loss did not improve from 22.78362
242/242 [==============================] - 0s 417us/sample - loss: 22.7376 - val_loss: 22.7934
Epoch 27/200
 20/242 [=>............................] - ETA: 0s - loss: 21.2852220/242 [==========================>...] - ETA: 0s - loss: 22.7287
Epoch 00027: val_loss did not improve from 22.78362

Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 440us/sample - loss: 22.7334 - val_loss: 22.8211
Epoch 28/200
 20/242 [=>............................] - ETA: 0s - loss: 25.1201220/242 [==========================>...] - ETA: 0s - loss: 22.6947
Epoch 00028: val_loss did not improve from 22.78362
242/242 [==============================] - 0s 419us/sample - loss: 22.7324 - val_loss: 22.8028
Epoch 29/200
 20/242 [=>............................] - ETA: 0s - loss: 22.2215220/242 [==========================>...] - ETA: 0s - loss: 22.4681
Epoch 00029: val_loss improved from 22.78362 to 22.77607, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 498us/sample - loss: 22.7255 - val_loss: 22.7761
Epoch 30/200
 20/242 [=>............................] - ETA: 0s - loss: 25.5415220/242 [==========================>...] - ETA: 0s - loss: 22.9628
Epoch 00030: val_loss did not improve from 22.77607
242/242 [==============================] - 0s 393us/sample - loss: 22.7269 - val_loss: 22.7820
Epoch 31/200
 20/242 [=>............................] - ETA: 0s - loss: 21.7383220/242 [==========================>...] - ETA: 0s - loss: 22.6597
Epoch 00031: val_loss improved from 22.77607 to 22.77593, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2002/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 485us/sample - loss: 22.7234 - val_loss: 22.7759
Epoch 32/200
 20/242 [=>............................] - ETA: 0s - loss: 23.4722220/242 [==========================>...] - ETA: 0s - loss: 22.6224
Epoch 00032: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 409us/sample - loss: 22.7245 - val_loss: 22.7772
Epoch 33/200
 20/242 [=>............................] - ETA: 0s - loss: 23.3166220/242 [==========================>...] - ETA: 0s - loss: 22.7061
Epoch 00033: val_loss did not improve from 22.77593

Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 410us/sample - loss: 22.7224 - val_loss: 22.7896
Epoch 34/200
 20/242 [=>............................] - ETA: 0s - loss: 21.9932220/242 [==========================>...] - ETA: 0s - loss: 23.1084
Epoch 00034: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 412us/sample - loss: 22.7204 - val_loss: 22.7857
Epoch 35/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3599220/242 [==========================>...] - ETA: 0s - loss: 22.7525
Epoch 00035: val_loss did not improve from 22.77593

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 413us/sample - loss: 22.7198 - val_loss: 22.7839
Epoch 36/200
 20/242 [=>............................] - ETA: 0s - loss: 23.3987220/242 [==========================>...] - ETA: 0s - loss: 22.7673
Epoch 00036: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 394us/sample - loss: 22.7192 - val_loss: 22.7830
Epoch 37/200
 20/242 [=>............................] - ETA: 0s - loss: 24.5323220/242 [==========================>...] - ETA: 0s - loss: 22.7154
Epoch 00037: val_loss did not improve from 22.77593

Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-05.
242/242 [==============================] - 0s 414us/sample - loss: 22.7191 - val_loss: 22.7844
Epoch 38/200
 20/242 [=>............................] - ETA: 0s - loss: 26.7003200/242 [=======================>......] - ETA: 0s - loss: 23.0006
Epoch 00038: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 431us/sample - loss: 22.7188 - val_loss: 22.7845
Epoch 39/200
 20/242 [=>............................] - ETA: 0s - loss: 21.6153220/242 [==========================>...] - ETA: 0s - loss: 22.7734
Epoch 00039: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 390us/sample - loss: 22.7185 - val_loss: 22.7859
Epoch 40/200
 20/242 [=>............................] - ETA: 0s - loss: 20.6843220/242 [==========================>...] - ETA: 0s - loss: 22.9576
Epoch 00040: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 510us/sample - loss: 22.7184 - val_loss: 22.7852
Epoch 41/200
 20/242 [=>............................] - ETA: 0s - loss: 23.1339160/242 [==================>...........] - ETA: 0s - loss: 22.2707
Epoch 00041: val_loss did not improve from 22.77593
242/242 [==============================] - 0s 485us/sample - loss: 22.7182 - val_loss: 22.7836
Epoch 00041: early stopping
done
