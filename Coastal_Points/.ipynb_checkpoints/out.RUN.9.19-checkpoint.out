2020-11-09 21:17:36.151268: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 21:17:36.595283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 21:17:36.595625: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b0401a230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:17:36.595655: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 21:17:36.617393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 21:17:36.746526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:17:36.766624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:17:36.919574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:17:36.988748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:17:37.102369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:17:37.218071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:17:37.326104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:17:37.461681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:17:37.465532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:17:37.465641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:17:37.634046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:17:37.634110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:17:37.634138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:17:37.640245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:17:37.643739: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b04d95810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:17:37.643785: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 21:17:37.648000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:17:37.648119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:17:37.648139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:17:37.648157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:17:37.652236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:17:37.652257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:17:37.652272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:17:37.652288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:17:37.655385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:17:37.655437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:17:37.655449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:17:37.655459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:17:37.659773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:17:37.664930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:17:37.665009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:17:37.665028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:17:37.665044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:17:37.665059: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:17:37.665074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:17:37.665090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:17:37.665106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:17:37.673193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:17:37.673251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:17:37.673263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:17:37.673273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:17:37.677420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 158.9929, 152.3698
Mean and standard deviation for "p_sfc" = 984.2132, 62.0500
Mean and standard deviation for "u_tr_p" = 12.7071, 12.2973
Mean and standard deviation for "v_tr_p" = 1.3114, 13.2859
Mean and standard deviation for "Z_p" = 5573.6696, 203.1434
Mean and standard deviation for "IWV" = 13.3217, 7.6310
2020-11-09 21:17:58.610562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:17:58.634194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:17:58.634294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:17:58.634318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:17:58.634337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:17:58.634356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:17:58.634375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:17:58.634393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:17:58.637679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:17:58.728625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:17:58.729324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:17:58.729346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:17:58.729362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:17:58.729378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:17:58.729394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:17:58.729409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:17:58.729424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:17:58.732483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:17:58.732542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:17:58.732554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:17:58.732567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:17:58.735632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:18:06.211456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:18:09.901750: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 21:18:09.996122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 203.4813, 183.0193
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 2183 samples, validate on 121 samples
Epoch 1/50
  50/2183 [..............................] - ETA: 4:33 - loss: nan 150/2183 [=>............................] - ETA: 1:27 - loss: nan 350/2183 [===>..........................] - ETA: 34s - loss: nan  550/2183 [======>.......................] - ETA: 19s - loss: nan 750/2183 [=========>....................] - ETA: 12s - loss: nan 950/2183 [============>.................] - ETA: 8s - loss: nan 1150/2183 [==============>...............] - ETA: 6s - loss: nan1350/2183 [=================>............] - ETA: 4s - loss: nan1550/2183 [====================>.........] - ETA: 2s - loss: nan1750/2183 [=======================>......] - ETA: 1s - loss: nan1950/2183 [=========================>....] - ETA: 0s - loss: nan2150/2183 [============================>.] - ETA: 0s - loss: nan
Epoch 00001: val_loss did not improve from inf
2183/2183 [==============================] - 8s 3ms/sample - loss: nan - val_loss: nan
Epoch 2/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2183/2183 [==============================] - 1s 283us/sample - loss: nan - val_loss: nan
Epoch 3/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
2183/2183 [==============================] - 1s 300us/sample - loss: nan - val_loss: nan
Epoch 4/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2183/2183 [==============================] - 1s 288us/sample - loss: nan - val_loss: nan
Epoch 5/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
2183/2183 [==============================] - 1s 281us/sample - loss: nan - val_loss: nan
Epoch 6/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2183/2183 [==============================] - 1s 282us/sample - loss: nan - val_loss: nan
Epoch 7/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
2183/2183 [==============================] - 1s 284us/sample - loss: nan - val_loss: nan
Epoch 8/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2183/2183 [==============================] - 1s 290us/sample - loss: nan - val_loss: nan
Epoch 9/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
2183/2183 [==============================] - 1s 282us/sample - loss: nan - val_loss: nan
Epoch 10/50
  50/2183 [..............................] - ETA: 0s - loss: nan 250/2183 [==>...........................] - ETA: 0s - loss: nan 450/2183 [=====>........................] - ETA: 0s - loss: nan 650/2183 [=======>......................] - ETA: 0s - loss: nan 850/2183 [==========>...................] - ETA: 0s - loss: nan1050/2183 [=============>................] - ETA: 0s - loss: nan1250/2183 [================>.............] - ETA: 0s - loss: nan1450/2183 [==================>...........] - ETA: 0s - loss: nan1650/2183 [=====================>........] - ETA: 0s - loss: nan1850/2183 [========================>.....] - ETA: 0s - loss: nan2050/2183 [===========================>..] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2183/2183 [==============================] - 1s 283us/sample - loss: nan - val_loss: nan
Epoch 00010: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 2183 samples, validate on 121 samples
Epoch 1/50
  50/2183 [..............................] - ETA: 4:20 - loss: 154.5990 200/2183 [=>............................] - ETA: 1:01 - loss: 154.7112 400/2183 [====>.........................] - ETA: 27s - loss: 151.5228  600/2183 [=======>......................] - ETA: 16s - loss: 147.0364 800/2183 [=========>....................] - ETA: 10s - loss: 137.17121000/2183 [============>.................] - ETA: 7s - loss: 121.5383 1200/2183 [===============>..............] - ETA: 5s - loss: 111.50171400/2183 [==================>...........] - ETA: 3s - loss: 102.22311600/2183 [====================>.........] - ETA: 2s - loss: 95.1323 1800/2183 [=======================>......] - ETA: 1s - loss: 88.91772000/2183 [==========================>...] - ETA: 0s - loss: 83.8050
Epoch 00001: val_loss improved from inf to 80.15304, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 7s 3ms/sample - loss: 79.8949 - val_loss: 80.1530
Epoch 2/50
  50/2183 [..............................] - ETA: 0s - loss: 40.4292 250/2183 [==>...........................] - ETA: 0s - loss: 36.5087 450/2183 [=====>........................] - ETA: 0s - loss: 35.5330 650/2183 [=======>......................] - ETA: 0s - loss: 35.0403 850/2183 [==========>...................] - ETA: 0s - loss: 34.55961050/2183 [=============>................] - ETA: 0s - loss: 33.99571250/2183 [================>.............] - ETA: 0s - loss: 33.92101450/2183 [==================>...........] - ETA: 0s - loss: 33.71101650/2183 [=====================>........] - ETA: 0s - loss: 33.41591850/2183 [========================>.....] - ETA: 0s - loss: 33.06772050/2183 [===========================>..] - ETA: 0s - loss: 32.7982
Epoch 00002: val_loss improved from 80.15304 to 71.61774, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 307us/sample - loss: 32.6252 - val_loss: 71.6177
Epoch 3/50
  50/2183 [..............................] - ETA: 0s - loss: 29.5810 250/2183 [==>...........................] - ETA: 0s - loss: 29.4926 450/2183 [=====>........................] - ETA: 0s - loss: 28.9683 650/2183 [=======>......................] - ETA: 0s - loss: 29.3044 850/2183 [==========>...................] - ETA: 0s - loss: 29.20521050/2183 [=============>................] - ETA: 0s - loss: 29.22951250/2183 [================>.............] - ETA: 0s - loss: 29.08741450/2183 [==================>...........] - ETA: 0s - loss: 28.94881650/2183 [=====================>........] - ETA: 0s - loss: 28.95741850/2183 [========================>.....] - ETA: 0s - loss: 28.98122050/2183 [===========================>..] - ETA: 0s - loss: 28.9078
Epoch 00003: val_loss improved from 71.61774 to 43.16917, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 298us/sample - loss: 28.9669 - val_loss: 43.1692
Epoch 4/50
  50/2183 [..............................] - ETA: 0s - loss: 30.3473 250/2183 [==>...........................] - ETA: 0s - loss: 29.1863 450/2183 [=====>........................] - ETA: 0s - loss: 28.9626 650/2183 [=======>......................] - ETA: 0s - loss: 28.8482 850/2183 [==========>...................] - ETA: 0s - loss: 28.43541050/2183 [=============>................] - ETA: 0s - loss: 28.59481250/2183 [================>.............] - ETA: 0s - loss: 28.55461450/2183 [==================>...........] - ETA: 0s - loss: 28.59091650/2183 [=====================>........] - ETA: 0s - loss: 28.57301850/2183 [========================>.....] - ETA: 0s - loss: 28.45562050/2183 [===========================>..] - ETA: 0s - loss: 28.3740
Epoch 00004: val_loss improved from 43.16917 to 33.32325, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 299us/sample - loss: 28.4127 - val_loss: 33.3233
Epoch 5/50
  50/2183 [..............................] - ETA: 0s - loss: 28.2633 250/2183 [==>...........................] - ETA: 0s - loss: 28.5899 450/2183 [=====>........................] - ETA: 0s - loss: 28.1807 650/2183 [=======>......................] - ETA: 0s - loss: 28.0615 850/2183 [==========>...................] - ETA: 0s - loss: 27.80551050/2183 [=============>................] - ETA: 0s - loss: 27.93961250/2183 [================>.............] - ETA: 0s - loss: 28.09961450/2183 [==================>...........] - ETA: 0s - loss: 28.13741650/2183 [=====================>........] - ETA: 0s - loss: 28.06291850/2183 [========================>.....] - ETA: 0s - loss: 27.87402050/2183 [===========================>..] - ETA: 0s - loss: 27.8176
Epoch 00005: val_loss improved from 33.32325 to 31.17294, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 297us/sample - loss: 27.7831 - val_loss: 31.1729
Epoch 6/50
  50/2183 [..............................] - ETA: 0s - loss: 25.3345 250/2183 [==>...........................] - ETA: 0s - loss: 26.5548 450/2183 [=====>........................] - ETA: 0s - loss: 27.0892 650/2183 [=======>......................] - ETA: 0s - loss: 26.8610 850/2183 [==========>...................] - ETA: 0s - loss: 26.84771050/2183 [=============>................] - ETA: 0s - loss: 26.76631250/2183 [================>.............] - ETA: 0s - loss: 26.77731450/2183 [==================>...........] - ETA: 0s - loss: 26.98201650/2183 [=====================>........] - ETA: 0s - loss: 27.05281850/2183 [========================>.....] - ETA: 0s - loss: 27.10712050/2183 [===========================>..] - ETA: 0s - loss: 27.1280
Epoch 00006: val_loss improved from 31.17294 to 27.83413, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 302us/sample - loss: 27.0423 - val_loss: 27.8341
Epoch 7/50
  50/2183 [..............................] - ETA: 0s - loss: 24.4010 250/2183 [==>...........................] - ETA: 0s - loss: 26.8683 450/2183 [=====>........................] - ETA: 0s - loss: 26.7245 650/2183 [=======>......................] - ETA: 0s - loss: 26.5922 850/2183 [==========>...................] - ETA: 0s - loss: 26.84821050/2183 [=============>................] - ETA: 0s - loss: 27.16971250/2183 [================>.............] - ETA: 0s - loss: 27.22861450/2183 [==================>...........] - ETA: 0s - loss: 27.08221650/2183 [=====================>........] - ETA: 0s - loss: 27.14071850/2183 [========================>.....] - ETA: 0s - loss: 27.16892050/2183 [===========================>..] - ETA: 0s - loss: 27.1645
Epoch 00007: val_loss did not improve from 27.83413
2183/2183 [==============================] - 1s 292us/sample - loss: 27.1542 - val_loss: 29.1450
Epoch 8/50
  50/2183 [..............................] - ETA: 0s - loss: 27.3329 250/2183 [==>...........................] - ETA: 0s - loss: 26.8644 400/2183 [====>.........................] - ETA: 0s - loss: 27.1005 600/2183 [=======>......................] - ETA: 0s - loss: 27.0572 800/2183 [=========>....................] - ETA: 0s - loss: 26.66301000/2183 [============>.................] - ETA: 0s - loss: 26.58001150/2183 [==============>...............] - ETA: 0s - loss: 26.64881350/2183 [=================>............] - ETA: 0s - loss: 26.73761550/2183 [====================>.........] - ETA: 0s - loss: 26.76911750/2183 [=======================>......] - ETA: 0s - loss: 26.88131950/2183 [=========================>....] - ETA: 0s - loss: 27.00172150/2183 [============================>.] - ETA: 0s - loss: 26.9955
Epoch 00008: val_loss improved from 27.83413 to 25.05203, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 329us/sample - loss: 26.9957 - val_loss: 25.0520
Epoch 9/50
  50/2183 [..............................] - ETA: 0s - loss: 26.3038 250/2183 [==>...........................] - ETA: 0s - loss: 26.6039 450/2183 [=====>........................] - ETA: 0s - loss: 26.8497 650/2183 [=======>......................] - ETA: 0s - loss: 26.5900 850/2183 [==========>...................] - ETA: 0s - loss: 26.47581050/2183 [=============>................] - ETA: 0s - loss: 26.49041250/2183 [================>.............] - ETA: 0s - loss: 26.56841450/2183 [==================>...........] - ETA: 0s - loss: 26.67821650/2183 [=====================>........] - ETA: 0s - loss: 26.75231850/2183 [========================>.....] - ETA: 0s - loss: 26.73142050/2183 [===========================>..] - ETA: 0s - loss: 26.7498
Epoch 00009: val_loss did not improve from 25.05203
2183/2183 [==============================] - 1s 284us/sample - loss: 26.6745 - val_loss: 25.2769
Epoch 10/50
  50/2183 [..............................] - ETA: 0s - loss: 28.9732 250/2183 [==>...........................] - ETA: 0s - loss: 25.8801 450/2183 [=====>........................] - ETA: 0s - loss: 26.4961 650/2183 [=======>......................] - ETA: 0s - loss: 26.4907 850/2183 [==========>...................] - ETA: 0s - loss: 26.47671050/2183 [=============>................] - ETA: 0s - loss: 26.48491250/2183 [================>.............] - ETA: 0s - loss: 26.76741450/2183 [==================>...........] - ETA: 0s - loss: 26.62691650/2183 [=====================>........] - ETA: 0s - loss: 26.57481850/2183 [========================>.....] - ETA: 0s - loss: 26.61792050/2183 [===========================>..] - ETA: 0s - loss: 26.6651
Epoch 00010: val_loss improved from 25.05203 to 24.10252, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 297us/sample - loss: 26.6248 - val_loss: 24.1025
Epoch 11/50
  50/2183 [..............................] - ETA: 0s - loss: 26.3584 250/2183 [==>...........................] - ETA: 0s - loss: 26.1008 450/2183 [=====>........................] - ETA: 0s - loss: 26.1535 650/2183 [=======>......................] - ETA: 0s - loss: 26.4101 850/2183 [==========>...................] - ETA: 0s - loss: 26.65021050/2183 [=============>................] - ETA: 0s - loss: 26.58061250/2183 [================>.............] - ETA: 0s - loss: 26.42621450/2183 [==================>...........] - ETA: 0s - loss: 26.22131650/2183 [=====================>........] - ETA: 0s - loss: 26.23211850/2183 [========================>.....] - ETA: 0s - loss: 26.27432050/2183 [===========================>..] - ETA: 0s - loss: 26.3729
Epoch 00011: val_loss improved from 24.10252 to 23.17914, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 294us/sample - loss: 26.3898 - val_loss: 23.1791
Epoch 12/50
  50/2183 [..............................] - ETA: 0s - loss: 26.8506 250/2183 [==>...........................] - ETA: 0s - loss: 27.1368 450/2183 [=====>........................] - ETA: 0s - loss: 26.7226 650/2183 [=======>......................] - ETA: 0s - loss: 26.6708 850/2183 [==========>...................] - ETA: 0s - loss: 26.67041050/2183 [=============>................] - ETA: 0s - loss: 26.83061250/2183 [================>.............] - ETA: 0s - loss: 26.88931450/2183 [==================>...........] - ETA: 0s - loss: 26.78111650/2183 [=====================>........] - ETA: 0s - loss: 26.59521850/2183 [========================>.....] - ETA: 0s - loss: 26.54542050/2183 [===========================>..] - ETA: 0s - loss: 26.5091
Epoch 00012: val_loss improved from 23.17914 to 23.05164, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 285us/sample - loss: 26.4609 - val_loss: 23.0516
Epoch 13/50
  50/2183 [..............................] - ETA: 0s - loss: 24.7137 250/2183 [==>...........................] - ETA: 0s - loss: 27.3636 450/2183 [=====>........................] - ETA: 0s - loss: 27.1154 650/2183 [=======>......................] - ETA: 0s - loss: 27.2365 850/2183 [==========>...................] - ETA: 0s - loss: 27.19031050/2183 [=============>................] - ETA: 0s - loss: 27.23071250/2183 [================>.............] - ETA: 0s - loss: 27.15801450/2183 [==================>...........] - ETA: 0s - loss: 26.95711650/2183 [=====================>........] - ETA: 0s - loss: 26.85361850/2183 [========================>.....] - ETA: 0s - loss: 26.75452050/2183 [===========================>..] - ETA: 0s - loss: 26.6443
Epoch 00013: val_loss did not improve from 23.05164
2183/2183 [==============================] - 1s 282us/sample - loss: 26.6361 - val_loss: 24.2540
Epoch 14/50
  50/2183 [..............................] - ETA: 0s - loss: 26.0456 250/2183 [==>...........................] - ETA: 0s - loss: 25.9372 450/2183 [=====>........................] - ETA: 0s - loss: 25.4855 650/2183 [=======>......................] - ETA: 0s - loss: 25.5244 850/2183 [==========>...................] - ETA: 0s - loss: 25.98411050/2183 [=============>................] - ETA: 0s - loss: 26.23421250/2183 [================>.............] - ETA: 0s - loss: 26.35221450/2183 [==================>...........] - ETA: 0s - loss: 26.30421650/2183 [=====================>........] - ETA: 0s - loss: 26.27211850/2183 [========================>.....] - ETA: 0s - loss: 26.22812050/2183 [===========================>..] - ETA: 0s - loss: 26.2583
Epoch 00014: val_loss improved from 23.05164 to 22.96752, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 321us/sample - loss: 26.3458 - val_loss: 22.9675
Epoch 15/50
  50/2183 [..............................] - ETA: 0s - loss: 25.3531 250/2183 [==>...........................] - ETA: 0s - loss: 26.5468 450/2183 [=====>........................] - ETA: 0s - loss: 26.6065 650/2183 [=======>......................] - ETA: 0s - loss: 26.9219 850/2183 [==========>...................] - ETA: 0s - loss: 26.99651050/2183 [=============>................] - ETA: 0s - loss: 27.04751250/2183 [================>.............] - ETA: 0s - loss: 27.09751450/2183 [==================>...........] - ETA: 0s - loss: 27.20321650/2183 [=====================>........] - ETA: 0s - loss: 27.16311850/2183 [========================>.....] - ETA: 0s - loss: 27.08442050/2183 [===========================>..] - ETA: 0s - loss: 27.0270
Epoch 00015: val_loss improved from 22.96752 to 22.80729, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 306us/sample - loss: 26.9809 - val_loss: 22.8073
Epoch 16/50
  50/2183 [..............................] - ETA: 0s - loss: 24.4270 250/2183 [==>...........................] - ETA: 0s - loss: 25.6595 450/2183 [=====>........................] - ETA: 0s - loss: 25.7017 650/2183 [=======>......................] - ETA: 0s - loss: 25.7078 850/2183 [==========>...................] - ETA: 0s - loss: 25.85551050/2183 [=============>................] - ETA: 0s - loss: 25.99261250/2183 [================>.............] - ETA: 0s - loss: 25.91461450/2183 [==================>...........] - ETA: 0s - loss: 25.94481650/2183 [=====================>........] - ETA: 0s - loss: 25.92241850/2183 [========================>.....] - ETA: 0s - loss: 25.92572050/2183 [===========================>..] - ETA: 0s - loss: 25.9735
Epoch 00016: val_loss did not improve from 22.80729
2183/2183 [==============================] - 1s 286us/sample - loss: 26.1430 - val_loss: 23.4191
Epoch 17/50
  50/2183 [..............................] - ETA: 0s - loss: 24.2849 250/2183 [==>...........................] - ETA: 0s - loss: 25.5094 450/2183 [=====>........................] - ETA: 0s - loss: 26.1929 650/2183 [=======>......................] - ETA: 0s - loss: 26.4928 850/2183 [==========>...................] - ETA: 0s - loss: 26.63341050/2183 [=============>................] - ETA: 0s - loss: 26.58511250/2183 [================>.............] - ETA: 0s - loss: 26.56391450/2183 [==================>...........] - ETA: 0s - loss: 26.49551650/2183 [=====================>........] - ETA: 0s - loss: 26.50121850/2183 [========================>.....] - ETA: 0s - loss: 26.63142050/2183 [===========================>..] - ETA: 0s - loss: 26.6263
Epoch 00017: val_loss did not improve from 22.80729

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2183/2183 [==============================] - 1s 286us/sample - loss: 26.6783 - val_loss: 23.3988
Epoch 18/50
  50/2183 [..............................] - ETA: 0s - loss: 24.6776 250/2183 [==>...........................] - ETA: 0s - loss: 25.6196 450/2183 [=====>........................] - ETA: 0s - loss: 25.7172 650/2183 [=======>......................] - ETA: 0s - loss: 25.6484 850/2183 [==========>...................] - ETA: 0s - loss: 26.11941050/2183 [=============>................] - ETA: 0s - loss: 26.36811250/2183 [================>.............] - ETA: 0s - loss: 26.22011450/2183 [==================>...........] - ETA: 0s - loss: 26.24541650/2183 [=====================>........] - ETA: 0s - loss: 26.38531850/2183 [========================>.....] - ETA: 0s - loss: 26.41782050/2183 [===========================>..] - ETA: 0s - loss: 26.3490
Epoch 00018: val_loss did not improve from 22.80729
2183/2183 [==============================] - 1s 278us/sample - loss: 26.2795 - val_loss: 23.1036
Epoch 19/50
  50/2183 [..............................] - ETA: 0s - loss: 30.4259 250/2183 [==>...........................] - ETA: 0s - loss: 26.2742 450/2183 [=====>........................] - ETA: 0s - loss: 26.1053 650/2183 [=======>......................] - ETA: 0s - loss: 26.2699 850/2183 [==========>...................] - ETA: 0s - loss: 26.17291050/2183 [=============>................] - ETA: 0s - loss: 25.91311250/2183 [================>.............] - ETA: 0s - loss: 25.93501450/2183 [==================>...........] - ETA: 0s - loss: 25.91191650/2183 [=====================>........] - ETA: 0s - loss: 25.97201850/2183 [========================>.....] - ETA: 0s - loss: 25.99362050/2183 [===========================>..] - ETA: 0s - loss: 26.0809
Epoch 00019: val_loss did not improve from 22.80729

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2183/2183 [==============================] - 1s 278us/sample - loss: 26.0633 - val_loss: 22.8873
Epoch 20/50
  50/2183 [..............................] - ETA: 0s - loss: 26.7335 250/2183 [==>...........................] - ETA: 0s - loss: 26.5230 450/2183 [=====>........................] - ETA: 0s - loss: 26.2310 650/2183 [=======>......................] - ETA: 0s - loss: 26.4663 850/2183 [==========>...................] - ETA: 0s - loss: 26.15061050/2183 [=============>................] - ETA: 0s - loss: 26.16281250/2183 [================>.............] - ETA: 0s - loss: 26.01161450/2183 [==================>...........] - ETA: 0s - loss: 26.02651650/2183 [=====================>........] - ETA: 0s - loss: 26.10541850/2183 [========================>.....] - ETA: 0s - loss: 26.05732050/2183 [===========================>..] - ETA: 0s - loss: 25.9843
Epoch 00020: val_loss improved from 22.80729 to 22.68238, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 301us/sample - loss: 26.0532 - val_loss: 22.6824
Epoch 21/50
  50/2183 [..............................] - ETA: 0s - loss: 27.4393 250/2183 [==>...........................] - ETA: 0s - loss: 25.9599 450/2183 [=====>........................] - ETA: 0s - loss: 26.0259 650/2183 [=======>......................] - ETA: 0s - loss: 26.2856 850/2183 [==========>...................] - ETA: 0s - loss: 26.35631050/2183 [=============>................] - ETA: 0s - loss: 26.28301250/2183 [================>.............] - ETA: 0s - loss: 26.08001450/2183 [==================>...........] - ETA: 0s - loss: 26.09201650/2183 [=====================>........] - ETA: 0s - loss: 25.93681850/2183 [========================>.....] - ETA: 0s - loss: 25.93632050/2183 [===========================>..] - ETA: 0s - loss: 25.9302
Epoch 00021: val_loss did not improve from 22.68238
2183/2183 [==============================] - 1s 293us/sample - loss: 25.9460 - val_loss: 22.8328
Epoch 22/50
  50/2183 [..............................] - ETA: 0s - loss: 25.8414 250/2183 [==>...........................] - ETA: 0s - loss: 25.8528 450/2183 [=====>........................] - ETA: 0s - loss: 25.7640 650/2183 [=======>......................] - ETA: 0s - loss: 25.6512 850/2183 [==========>...................] - ETA: 0s - loss: 25.67031050/2183 [=============>................] - ETA: 0s - loss: 25.60991250/2183 [================>.............] - ETA: 0s - loss: 25.96501450/2183 [==================>...........] - ETA: 0s - loss: 25.97301650/2183 [=====================>........] - ETA: 0s - loss: 25.99721850/2183 [========================>.....] - ETA: 0s - loss: 25.97732050/2183 [===========================>..] - ETA: 0s - loss: 25.9370
Epoch 00022: val_loss improved from 22.68238 to 22.62258, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 300us/sample - loss: 26.0063 - val_loss: 22.6226
Epoch 23/50
  50/2183 [..............................] - ETA: 0s - loss: 27.4129 250/2183 [==>...........................] - ETA: 0s - loss: 26.4241 450/2183 [=====>........................] - ETA: 0s - loss: 26.2447 650/2183 [=======>......................] - ETA: 0s - loss: 26.3104 850/2183 [==========>...................] - ETA: 0s - loss: 25.99501050/2183 [=============>................] - ETA: 0s - loss: 26.00181250/2183 [================>.............] - ETA: 0s - loss: 26.16471450/2183 [==================>...........] - ETA: 0s - loss: 26.25361650/2183 [=====================>........] - ETA: 0s - loss: 26.18091850/2183 [========================>.....] - ETA: 0s - loss: 26.02862050/2183 [===========================>..] - ETA: 0s - loss: 25.9753
Epoch 00023: val_loss did not improve from 22.62258
2183/2183 [==============================] - 1s 285us/sample - loss: 25.9590 - val_loss: 22.9810
Epoch 24/50
  50/2183 [..............................] - ETA: 0s - loss: 24.7982 250/2183 [==>...........................] - ETA: 0s - loss: 25.1906 450/2183 [=====>........................] - ETA: 0s - loss: 25.8391 650/2183 [=======>......................] - ETA: 0s - loss: 25.8657 850/2183 [==========>...................] - ETA: 0s - loss: 25.90391050/2183 [=============>................] - ETA: 0s - loss: 25.98581250/2183 [================>.............] - ETA: 0s - loss: 25.92781450/2183 [==================>...........] - ETA: 0s - loss: 25.96261650/2183 [=====================>........] - ETA: 0s - loss: 25.92421850/2183 [========================>.....] - ETA: 0s - loss: 25.89582050/2183 [===========================>..] - ETA: 0s - loss: 25.9536
Epoch 00024: val_loss improved from 22.62258 to 22.60257, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 299us/sample - loss: 25.9532 - val_loss: 22.6026
Epoch 25/50
  50/2183 [..............................] - ETA: 0s - loss: 27.2676 250/2183 [==>...........................] - ETA: 0s - loss: 24.9760 450/2183 [=====>........................] - ETA: 0s - loss: 25.4158 650/2183 [=======>......................] - ETA: 0s - loss: 25.8174 850/2183 [==========>...................] - ETA: 0s - loss: 26.08561050/2183 [=============>................] - ETA: 0s - loss: 26.16961250/2183 [================>.............] - ETA: 0s - loss: 26.08891400/2183 [==================>...........] - ETA: 0s - loss: 26.08151550/2183 [====================>.........] - ETA: 0s - loss: 26.09311750/2183 [=======================>......] - ETA: 0s - loss: 25.98841950/2183 [=========================>....] - ETA: 0s - loss: 25.93592150/2183 [============================>.] - ETA: 0s - loss: 25.9523
Epoch 00025: val_loss did not improve from 22.60257
2183/2183 [==============================] - 1s 303us/sample - loss: 25.9619 - val_loss: 23.0590
Epoch 26/50
  50/2183 [..............................] - ETA: 0s - loss: 23.4455 250/2183 [==>...........................] - ETA: 0s - loss: 25.3758 450/2183 [=====>........................] - ETA: 0s - loss: 25.5766 650/2183 [=======>......................] - ETA: 0s - loss: 25.9132 850/2183 [==========>...................] - ETA: 0s - loss: 25.88371050/2183 [=============>................] - ETA: 0s - loss: 25.85901250/2183 [================>.............] - ETA: 0s - loss: 25.94011450/2183 [==================>...........] - ETA: 0s - loss: 26.02201650/2183 [=====================>........] - ETA: 0s - loss: 25.97881850/2183 [========================>.....] - ETA: 0s - loss: 25.93392050/2183 [===========================>..] - ETA: 0s - loss: 25.9396
Epoch 00026: val_loss did not improve from 22.60257

Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2183/2183 [==============================] - 1s 285us/sample - loss: 25.9024 - val_loss: 22.6637
Epoch 27/50
  50/2183 [..............................] - ETA: 0s - loss: 24.7538 250/2183 [==>...........................] - ETA: 0s - loss: 25.5338 450/2183 [=====>........................] - ETA: 0s - loss: 25.4282 650/2183 [=======>......................] - ETA: 0s - loss: 25.8458 850/2183 [==========>...................] - ETA: 0s - loss: 26.04171050/2183 [=============>................] - ETA: 0s - loss: 26.12091250/2183 [================>.............] - ETA: 0s - loss: 26.08221450/2183 [==================>...........] - ETA: 0s - loss: 26.08441650/2183 [=====================>........] - ETA: 0s - loss: 26.00091850/2183 [========================>.....] - ETA: 0s - loss: 26.06372050/2183 [===========================>..] - ETA: 0s - loss: 25.9926
Epoch 00027: val_loss did not improve from 22.60257
2183/2183 [==============================] - 1s 283us/sample - loss: 25.9769 - val_loss: 22.8679
Epoch 28/50
  50/2183 [..............................] - ETA: 0s - loss: 25.6534 250/2183 [==>...........................] - ETA: 0s - loss: 25.7086 450/2183 [=====>........................] - ETA: 0s - loss: 25.5779 650/2183 [=======>......................] - ETA: 0s - loss: 25.5433 850/2183 [==========>...................] - ETA: 0s - loss: 25.62741050/2183 [=============>................] - ETA: 0s - loss: 25.75421250/2183 [================>.............] - ETA: 0s - loss: 25.79021450/2183 [==================>...........] - ETA: 0s - loss: 25.78901650/2183 [=====================>........] - ETA: 0s - loss: 25.77621850/2183 [========================>.....] - ETA: 0s - loss: 25.88242050/2183 [===========================>..] - ETA: 0s - loss: 25.8879
Epoch 00028: val_loss improved from 22.60257 to 22.60226, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 298us/sample - loss: 25.8976 - val_loss: 22.6023
Epoch 29/50
  50/2183 [..............................] - ETA: 0s - loss: 25.7481 250/2183 [==>...........................] - ETA: 0s - loss: 25.7549 450/2183 [=====>........................] - ETA: 0s - loss: 25.6719 650/2183 [=======>......................] - ETA: 0s - loss: 25.7686 850/2183 [==========>...................] - ETA: 0s - loss: 25.94301050/2183 [=============>................] - ETA: 0s - loss: 25.85731250/2183 [================>.............] - ETA: 0s - loss: 25.76721450/2183 [==================>...........] - ETA: 0s - loss: 25.82531650/2183 [=====================>........] - ETA: 0s - loss: 25.78711850/2183 [========================>.....] - ETA: 0s - loss: 25.79832050/2183 [===========================>..] - ETA: 0s - loss: 25.8175
Epoch 00029: val_loss did not improve from 22.60226
2183/2183 [==============================] - 1s 283us/sample - loss: 25.8426 - val_loss: 22.7356
Epoch 30/50
  50/2183 [..............................] - ETA: 0s - loss: 24.2830 250/2183 [==>...........................] - ETA: 0s - loss: 25.7738 450/2183 [=====>........................] - ETA: 0s - loss: 26.0303 650/2183 [=======>......................] - ETA: 0s - loss: 26.1401 850/2183 [==========>...................] - ETA: 0s - loss: 25.87221050/2183 [=============>................] - ETA: 0s - loss: 25.78751250/2183 [================>.............] - ETA: 0s - loss: 25.81181450/2183 [==================>...........] - ETA: 0s - loss: 25.73611650/2183 [=====================>........] - ETA: 0s - loss: 25.72621850/2183 [========================>.....] - ETA: 0s - loss: 25.72752050/2183 [===========================>..] - ETA: 0s - loss: 25.7523
Epoch 00030: val_loss improved from 22.60226 to 22.60182, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
2183/2183 [==============================] - 1s 300us/sample - loss: 25.7783 - val_loss: 22.6018
Epoch 31/50
  50/2183 [..............................] - ETA: 0s - loss: 24.4250 250/2183 [==>...........................] - ETA: 0s - loss: 25.5540 450/2183 [=====>........................] - ETA: 0s - loss: 26.4234 650/2183 [=======>......................] - ETA: 0s - loss: 26.2740 850/2183 [==========>...................] - ETA: 0s - loss: 26.08051050/2183 [=============>................] - ETA: 0s - loss: 26.02281250/2183 [================>.............] - ETA: 0s - loss: 26.31091450/2183 [==================>...........] - ETA: 0s - loss: 26.24521650/2183 [=====================>........] - ETA: 0s - loss: 26.09991850/2183 [========================>.....] - ETA: 0s - loss: 26.01952050/2183 [===========================>..] - ETA: 0s - loss: 25.9441
Epoch 00031: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 284us/sample - loss: 25.9279 - val_loss: 22.8410
Epoch 32/50
  50/2183 [..............................] - ETA: 0s - loss: 28.8972 250/2183 [==>...........................] - ETA: 0s - loss: 25.6113 450/2183 [=====>........................] - ETA: 0s - loss: 25.8100 650/2183 [=======>......................] - ETA: 0s - loss: 25.5809 850/2183 [==========>...................] - ETA: 0s - loss: 25.77321050/2183 [=============>................] - ETA: 0s - loss: 25.84491250/2183 [================>.............] - ETA: 0s - loss: 25.85251450/2183 [==================>...........] - ETA: 0s - loss: 25.86911650/2183 [=====================>........] - ETA: 0s - loss: 26.06021850/2183 [========================>.....] - ETA: 0s - loss: 25.94822050/2183 [===========================>..] - ETA: 0s - loss: 25.8904
Epoch 00032: val_loss did not improve from 22.60182

Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2183/2183 [==============================] - 1s 284us/sample - loss: 25.8616 - val_loss: 22.6919
Epoch 33/50
  50/2183 [..............................] - ETA: 0s - loss: 24.8288 250/2183 [==>...........................] - ETA: 0s - loss: 26.2990 450/2183 [=====>........................] - ETA: 0s - loss: 26.1023 650/2183 [=======>......................] - ETA: 0s - loss: 26.1777 850/2183 [==========>...................] - ETA: 0s - loss: 25.92281050/2183 [=============>................] - ETA: 0s - loss: 26.09551250/2183 [================>.............] - ETA: 0s - loss: 26.15971450/2183 [==================>...........] - ETA: 0s - loss: 26.16921650/2183 [=====================>........] - ETA: 0s - loss: 26.05161850/2183 [========================>.....] - ETA: 0s - loss: 25.92872050/2183 [===========================>..] - ETA: 0s - loss: 25.90702100/2183 [===========================>..] - ETA: 0s - loss: 25.8939
Epoch 00033: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 293us/sample - loss: 25.8788 - val_loss: 22.6439
Epoch 34/50
  50/2183 [..............................] - ETA: 0s - loss: 23.6115 250/2183 [==>...........................] - ETA: 0s - loss: 24.9414 450/2183 [=====>........................] - ETA: 0s - loss: 24.8410 650/2183 [=======>......................] - ETA: 0s - loss: 25.2657 850/2183 [==========>...................] - ETA: 0s - loss: 25.72951050/2183 [=============>................] - ETA: 0s - loss: 25.57431250/2183 [================>.............] - ETA: 0s - loss: 25.81171450/2183 [==================>...........] - ETA: 0s - loss: 25.81541650/2183 [=====================>........] - ETA: 0s - loss: 25.91551850/2183 [========================>.....] - ETA: 0s - loss: 25.89582050/2183 [===========================>..] - ETA: 0s - loss: 25.8988
Epoch 00034: val_loss did not improve from 22.60182

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2183/2183 [==============================] - 1s 289us/sample - loss: 25.8138 - val_loss: 22.6081
Epoch 35/50
  50/2183 [..............................] - ETA: 0s - loss: 25.7348 250/2183 [==>...........................] - ETA: 0s - loss: 25.7218 450/2183 [=====>........................] - ETA: 0s - loss: 26.1739 650/2183 [=======>......................] - ETA: 0s - loss: 25.8586 850/2183 [==========>...................] - ETA: 0s - loss: 25.48371050/2183 [=============>................] - ETA: 0s - loss: 25.56941250/2183 [================>.............] - ETA: 0s - loss: 25.63101450/2183 [==================>...........] - ETA: 0s - loss: 25.76231650/2183 [=====================>........] - ETA: 0s - loss: 25.75501850/2183 [========================>.....] - ETA: 0s - loss: 25.62732050/2183 [===========================>..] - ETA: 0s - loss: 25.7165
Epoch 00035: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 289us/sample - loss: 25.7609 - val_loss: 22.6301
Epoch 36/50
  50/2183 [..............................] - ETA: 0s - loss: 25.1038 250/2183 [==>...........................] - ETA: 0s - loss: 25.6932 450/2183 [=====>........................] - ETA: 0s - loss: 25.8014 650/2183 [=======>......................] - ETA: 0s - loss: 25.9008 850/2183 [==========>...................] - ETA: 0s - loss: 25.64851050/2183 [=============>................] - ETA: 0s - loss: 25.61641250/2183 [================>.............] - ETA: 0s - loss: 25.59171450/2183 [==================>...........] - ETA: 0s - loss: 25.61041650/2183 [=====================>........] - ETA: 0s - loss: 25.70621850/2183 [========================>.....] - ETA: 0s - loss: 25.73632050/2183 [===========================>..] - ETA: 0s - loss: 25.7375
Epoch 00036: val_loss did not improve from 22.60182

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.
2183/2183 [==============================] - 1s 280us/sample - loss: 25.7807 - val_loss: 22.6201
Epoch 37/50
  50/2183 [..............................] - ETA: 0s - loss: 23.7170 250/2183 [==>...........................] - ETA: 0s - loss: 25.4937 450/2183 [=====>........................] - ETA: 0s - loss: 25.9500 650/2183 [=======>......................] - ETA: 0s - loss: 26.1062 850/2183 [==========>...................] - ETA: 0s - loss: 26.18031050/2183 [=============>................] - ETA: 0s - loss: 25.94891250/2183 [================>.............] - ETA: 0s - loss: 25.84851450/2183 [==================>...........] - ETA: 0s - loss: 25.79611650/2183 [=====================>........] - ETA: 0s - loss: 25.83711850/2183 [========================>.....] - ETA: 0s - loss: 25.78062050/2183 [===========================>..] - ETA: 0s - loss: 25.8205
Epoch 00037: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 286us/sample - loss: 25.7787 - val_loss: 22.6294
Epoch 38/50
  50/2183 [..............................] - ETA: 0s - loss: 23.7656 250/2183 [==>...........................] - ETA: 0s - loss: 25.7395 450/2183 [=====>........................] - ETA: 0s - loss: 25.4897 650/2183 [=======>......................] - ETA: 0s - loss: 25.6244 850/2183 [==========>...................] - ETA: 0s - loss: 25.57721050/2183 [=============>................] - ETA: 0s - loss: 25.61661250/2183 [================>.............] - ETA: 0s - loss: 25.70361450/2183 [==================>...........] - ETA: 0s - loss: 25.68511650/2183 [=====================>........] - ETA: 0s - loss: 25.68511850/2183 [========================>.....] - ETA: 0s - loss: 25.64492050/2183 [===========================>..] - ETA: 0s - loss: 25.5985
Epoch 00038: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 285us/sample - loss: 25.7433 - val_loss: 22.6213
Epoch 39/50
  50/2183 [..............................] - ETA: 0s - loss: 26.4478 250/2183 [==>...........................] - ETA: 0s - loss: 25.6653 450/2183 [=====>........................] - ETA: 0s - loss: 25.5556 650/2183 [=======>......................] - ETA: 0s - loss: 25.5549 850/2183 [==========>...................] - ETA: 0s - loss: 25.69771050/2183 [=============>................] - ETA: 0s - loss: 25.76851250/2183 [================>.............] - ETA: 0s - loss: 25.81211450/2183 [==================>...........] - ETA: 0s - loss: 25.84861650/2183 [=====================>........] - ETA: 0s - loss: 25.92561850/2183 [========================>.....] - ETA: 0s - loss: 25.95742050/2183 [===========================>..] - ETA: 0s - loss: 25.9459
Epoch 00039: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 282us/sample - loss: 25.8948 - val_loss: 22.6154
Epoch 40/50
  50/2183 [..............................] - ETA: 0s - loss: 26.5525 250/2183 [==>...........................] - ETA: 0s - loss: 26.6212 450/2183 [=====>........................] - ETA: 0s - loss: 26.4702 650/2183 [=======>......................] - ETA: 0s - loss: 26.4830 850/2183 [==========>...................] - ETA: 0s - loss: 26.10541050/2183 [=============>................] - ETA: 0s - loss: 26.16741250/2183 [================>.............] - ETA: 0s - loss: 25.95981450/2183 [==================>...........] - ETA: 0s - loss: 25.81051650/2183 [=====================>........] - ETA: 0s - loss: 25.73561850/2183 [========================>.....] - ETA: 0s - loss: 25.75412050/2183 [===========================>..] - ETA: 0s - loss: 25.8936
Epoch 00040: val_loss did not improve from 22.60182
2183/2183 [==============================] - 1s 284us/sample - loss: 25.8884 - val_loss: 22.6271
Epoch 00040: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 2304 samples, validate on 121 samples
Epoch 1/50
  50/2304 [..............................] - ETA: 54s - loss: 156.4754 150/2304 [>.............................] - ETA: 17s - loss: 157.1658 350/2304 [===>..........................] - ETA: 7s - loss: 152.5662  550/2304 [======>.......................] - ETA: 4s - loss: 149.3348 750/2304 [========>.....................] - ETA: 2s - loss: 143.3282 950/2304 [===========>..................] - ETA: 2s - loss: 130.44861150/2304 [=============>................] - ETA: 1s - loss: 119.09121350/2304 [================>.............] - ETA: 1s - loss: 108.30971550/2304 [===================>..........] - ETA: 0s - loss: 100.54951750/2304 [=====================>........] - ETA: 0s - loss: 94.3120 1950/2304 [========================>.....] - ETA: 0s - loss: 89.00932150/2304 [==========================>...] - ETA: 0s - loss: 84.1880
Epoch 00001: val_loss improved from inf to 58.88071, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 2s 1ms/sample - loss: 81.2493 - val_loss: 58.8807
Epoch 2/50
  50/2304 [..............................] - ETA: 0s - loss: 37.4580 250/2304 [==>...........................] - ETA: 0s - loss: 36.6625 450/2304 [====>.........................] - ETA: 0s - loss: 36.0571 650/2304 [=======>......................] - ETA: 0s - loss: 35.5875 850/2304 [==========>...................] - ETA: 0s - loss: 34.95861050/2304 [============>.................] - ETA: 0s - loss: 34.88491250/2304 [===============>..............] - ETA: 0s - loss: 34.65621450/2304 [=================>............] - ETA: 0s - loss: 34.28591650/2304 [====================>.........] - ETA: 0s - loss: 34.03521850/2304 [=======================>......] - ETA: 0s - loss: 33.79592050/2304 [=========================>....] - ETA: 0s - loss: 33.50952250/2304 [============================>.] - ETA: 0s - loss: 33.2942
Epoch 00002: val_loss improved from 58.88071 to 55.52298, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 294us/sample - loss: 33.2407 - val_loss: 55.5230
Epoch 3/50
  50/2304 [..............................] - ETA: 0s - loss: 30.4572 250/2304 [==>...........................] - ETA: 0s - loss: 30.7839 450/2304 [====>.........................] - ETA: 0s - loss: 30.7362 650/2304 [=======>......................] - ETA: 0s - loss: 30.6318 850/2304 [==========>...................] - ETA: 0s - loss: 30.33901050/2304 [============>.................] - ETA: 0s - loss: 30.14321250/2304 [===============>..............] - ETA: 0s - loss: 30.16131450/2304 [=================>............] - ETA: 0s - loss: 30.00941650/2304 [====================>.........] - ETA: 0s - loss: 29.91031850/2304 [=======================>......] - ETA: 0s - loss: 29.73862050/2304 [=========================>....] - ETA: 0s - loss: 29.70352250/2304 [============================>.] - ETA: 0s - loss: 29.5887
Epoch 00003: val_loss improved from 55.52298 to 54.46216, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 291us/sample - loss: 29.5532 - val_loss: 54.4622
Epoch 4/50
  50/2304 [..............................] - ETA: 0s - loss: 30.2972 250/2304 [==>...........................] - ETA: 0s - loss: 29.7166 450/2304 [====>.........................] - ETA: 0s - loss: 29.4682 650/2304 [=======>......................] - ETA: 0s - loss: 29.4080 850/2304 [==========>...................] - ETA: 0s - loss: 29.01451050/2304 [============>.................] - ETA: 0s - loss: 28.63361250/2304 [===============>..............] - ETA: 0s - loss: 28.49471450/2304 [=================>............] - ETA: 0s - loss: 28.42021650/2304 [====================>.........] - ETA: 0s - loss: 28.38581850/2304 [=======================>......] - ETA: 0s - loss: 28.41562050/2304 [=========================>....] - ETA: 0s - loss: 28.33332250/2304 [============================>.] - ETA: 0s - loss: 28.2757
Epoch 00004: val_loss improved from 54.46216 to 42.02555, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 292us/sample - loss: 28.2737 - val_loss: 42.0255
Epoch 5/50
  50/2304 [..............................] - ETA: 0s - loss: 26.5251 250/2304 [==>...........................] - ETA: 0s - loss: 28.8164 450/2304 [====>.........................] - ETA: 0s - loss: 28.4259 650/2304 [=======>......................] - ETA: 0s - loss: 28.3643 850/2304 [==========>...................] - ETA: 0s - loss: 28.22121050/2304 [============>.................] - ETA: 0s - loss: 28.24651250/2304 [===============>..............] - ETA: 0s - loss: 28.16181450/2304 [=================>............] - ETA: 0s - loss: 28.09971650/2304 [====================>.........] - ETA: 0s - loss: 27.92511850/2304 [=======================>......] - ETA: 0s - loss: 28.04472050/2304 [=========================>....] - ETA: 0s - loss: 28.05902250/2304 [============================>.] - ETA: 0s - loss: 28.0502
Epoch 00005: val_loss improved from 42.02555 to 31.59943, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 287us/sample - loss: 28.0581 - val_loss: 31.5994
Epoch 6/50
  50/2304 [..............................] - ETA: 0s - loss: 27.6004 250/2304 [==>...........................] - ETA: 0s - loss: 28.9017 450/2304 [====>.........................] - ETA: 0s - loss: 28.4376 650/2304 [=======>......................] - ETA: 0s - loss: 28.4089 850/2304 [==========>...................] - ETA: 0s - loss: 28.38621050/2304 [============>.................] - ETA: 0s - loss: 28.14231250/2304 [===============>..............] - ETA: 0s - loss: 28.17221450/2304 [=================>............] - ETA: 0s - loss: 28.04141650/2304 [====================>.........] - ETA: 0s - loss: 27.91221850/2304 [=======================>......] - ETA: 0s - loss: 27.65492050/2304 [=========================>....] - ETA: 0s - loss: 27.59722250/2304 [============================>.] - ETA: 0s - loss: 27.5292
Epoch 00006: val_loss did not improve from 31.59943
2304/2304 [==============================] - 1s 276us/sample - loss: 27.4796 - val_loss: 32.4033
Epoch 7/50
  50/2304 [..............................] - ETA: 0s - loss: 31.9735 250/2304 [==>...........................] - ETA: 0s - loss: 27.8146 450/2304 [====>.........................] - ETA: 0s - loss: 27.0066 650/2304 [=======>......................] - ETA: 0s - loss: 27.2816 850/2304 [==========>...................] - ETA: 0s - loss: 27.43551050/2304 [============>.................] - ETA: 0s - loss: 27.33731250/2304 [===============>..............] - ETA: 0s - loss: 27.18601450/2304 [=================>............] - ETA: 0s - loss: 27.23031650/2304 [====================>.........] - ETA: 0s - loss: 27.23781850/2304 [=======================>......] - ETA: 0s - loss: 27.29292050/2304 [=========================>....] - ETA: 0s - loss: 27.39102250/2304 [============================>.] - ETA: 0s - loss: 27.3691
Epoch 00007: val_loss improved from 31.59943 to 26.42222, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 292us/sample - loss: 27.3337 - val_loss: 26.4222
Epoch 8/50
  50/2304 [..............................] - ETA: 0s - loss: 29.2249 250/2304 [==>...........................] - ETA: 0s - loss: 26.9988 450/2304 [====>.........................] - ETA: 0s - loss: 27.0133 650/2304 [=======>......................] - ETA: 0s - loss: 27.4365 850/2304 [==========>...................] - ETA: 0s - loss: 27.53721050/2304 [============>.................] - ETA: 0s - loss: 27.40771250/2304 [===============>..............] - ETA: 0s - loss: 27.34071450/2304 [=================>............] - ETA: 0s - loss: 27.25251650/2304 [====================>.........] - ETA: 0s - loss: 27.30451850/2304 [=======================>......] - ETA: 0s - loss: 27.40522050/2304 [=========================>....] - ETA: 0s - loss: 27.40282250/2304 [============================>.] - ETA: 0s - loss: 27.4449
Epoch 00008: val_loss improved from 26.42222 to 24.69487, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 322us/sample - loss: 27.4311 - val_loss: 24.6949
Epoch 9/50
  50/2304 [..............................] - ETA: 0s - loss: 27.9032 250/2304 [==>...........................] - ETA: 0s - loss: 27.9161 450/2304 [====>.........................] - ETA: 0s - loss: 27.8750 650/2304 [=======>......................] - ETA: 0s - loss: 27.7740 850/2304 [==========>...................] - ETA: 0s - loss: 27.53581050/2304 [============>.................] - ETA: 0s - loss: 27.42581250/2304 [===============>..............] - ETA: 0s - loss: 27.20781450/2304 [=================>............] - ETA: 0s - loss: 27.27461650/2304 [====================>.........] - ETA: 0s - loss: 27.18831850/2304 [=======================>......] - ETA: 0s - loss: 27.20522000/2304 [=========================>....] - ETA: 0s - loss: 27.14292100/2304 [==========================>...] - ETA: 0s - loss: 27.10822300/2304 [============================>.] - ETA: 0s - loss: 27.0913
Epoch 00009: val_loss did not improve from 24.69487
2304/2304 [==============================] - 1s 320us/sample - loss: 27.0836 - val_loss: 25.1395
Epoch 10/50
  50/2304 [..............................] - ETA: 0s - loss: 26.1858 250/2304 [==>...........................] - ETA: 0s - loss: 27.1558 450/2304 [====>.........................] - ETA: 0s - loss: 26.9712 650/2304 [=======>......................] - ETA: 0s - loss: 26.9119 850/2304 [==========>...................] - ETA: 0s - loss: 26.67461050/2304 [============>.................] - ETA: 0s - loss: 26.78111250/2304 [===============>..............] - ETA: 0s - loss: 26.86491450/2304 [=================>............] - ETA: 0s - loss: 26.80601650/2304 [====================>.........] - ETA: 0s - loss: 26.79621850/2304 [=======================>......] - ETA: 0s - loss: 26.74262050/2304 [=========================>....] - ETA: 0s - loss: 26.74802250/2304 [============================>.] - ETA: 0s - loss: 26.7273
Epoch 00010: val_loss did not improve from 24.69487

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2304/2304 [==============================] - 1s 296us/sample - loss: 26.6699 - val_loss: 25.6122
Epoch 11/50
  50/2304 [..............................] - ETA: 0s - loss: 29.9444 250/2304 [==>...........................] - ETA: 0s - loss: 27.1693 450/2304 [====>.........................] - ETA: 0s - loss: 27.3886 650/2304 [=======>......................] - ETA: 0s - loss: 27.0390 850/2304 [==========>...................] - ETA: 0s - loss: 26.72601050/2304 [============>.................] - ETA: 0s - loss: 26.66751250/2304 [===============>..............] - ETA: 0s - loss: 26.72941450/2304 [=================>............] - ETA: 0s - loss: 26.66601650/2304 [====================>.........] - ETA: 0s - loss: 26.58181850/2304 [=======================>......] - ETA: 0s - loss: 26.52342050/2304 [=========================>....] - ETA: 0s - loss: 26.52822250/2304 [============================>.] - ETA: 0s - loss: 26.5315
Epoch 00011: val_loss improved from 24.69487 to 23.52362, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 312us/sample - loss: 26.5096 - val_loss: 23.5236
Epoch 12/50
  50/2304 [..............................] - ETA: 0s - loss: 26.7703 250/2304 [==>...........................] - ETA: 0s - loss: 27.1271 450/2304 [====>.........................] - ETA: 0s - loss: 26.5288 650/2304 [=======>......................] - ETA: 0s - loss: 26.5424 850/2304 [==========>...................] - ETA: 0s - loss: 26.75811050/2304 [============>.................] - ETA: 0s - loss: 26.92531250/2304 [===============>..............] - ETA: 0s - loss: 26.82901450/2304 [=================>............] - ETA: 0s - loss: 26.86891650/2304 [====================>.........] - ETA: 0s - loss: 26.79831850/2304 [=======================>......] - ETA: 0s - loss: 26.70142050/2304 [=========================>....] - ETA: 0s - loss: 26.64022250/2304 [============================>.] - ETA: 0s - loss: 26.5763
Epoch 00012: val_loss improved from 23.52362 to 23.48231, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 314us/sample - loss: 26.5452 - val_loss: 23.4823
Epoch 13/50
  50/2304 [..............................] - ETA: 0s - loss: 26.1205 250/2304 [==>...........................] - ETA: 0s - loss: 26.6639 450/2304 [====>.........................] - ETA: 0s - loss: 26.6559 650/2304 [=======>......................] - ETA: 0s - loss: 26.4922 850/2304 [==========>...................] - ETA: 0s - loss: 26.53141050/2304 [============>.................] - ETA: 0s - loss: 26.30351250/2304 [===============>..............] - ETA: 0s - loss: 26.53191450/2304 [=================>............] - ETA: 0s - loss: 26.44631650/2304 [====================>.........] - ETA: 0s - loss: 26.45561850/2304 [=======================>......] - ETA: 0s - loss: 26.47192050/2304 [=========================>....] - ETA: 0s - loss: 26.55422250/2304 [============================>.] - ETA: 0s - loss: 26.5418
Epoch 00013: val_loss did not improve from 23.48231
2304/2304 [==============================] - 1s 283us/sample - loss: 26.5397 - val_loss: 23.9149
Epoch 14/50
  50/2304 [..............................] - ETA: 0s - loss: 28.4659 300/2304 [==>...........................] - ETA: 0s - loss: 28.0963 500/2304 [=====>........................] - ETA: 0s - loss: 27.1147 700/2304 [========>.....................] - ETA: 0s - loss: 27.2618 900/2304 [==========>...................] - ETA: 0s - loss: 26.97601150/2304 [=============>................] - ETA: 0s - loss: 26.84731400/2304 [=================>............] - ETA: 0s - loss: 26.95141600/2304 [===================>..........] - ETA: 0s - loss: 26.87401800/2304 [======================>.......] - ETA: 0s - loss: 26.77482000/2304 [=========================>....] - ETA: 0s - loss: 26.84572200/2304 [===========================>..] - ETA: 0s - loss: 26.8032
Epoch 00014: val_loss improved from 23.48231 to 23.36075, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 282us/sample - loss: 26.7519 - val_loss: 23.3608
Epoch 15/50
  50/2304 [..............................] - ETA: 0s - loss: 26.1191 250/2304 [==>...........................] - ETA: 0s - loss: 25.5205 450/2304 [====>.........................] - ETA: 0s - loss: 25.8554 650/2304 [=======>......................] - ETA: 0s - loss: 25.8588 850/2304 [==========>...................] - ETA: 0s - loss: 25.98331050/2304 [============>.................] - ETA: 0s - loss: 26.03721250/2304 [===============>..............] - ETA: 0s - loss: 25.99351450/2304 [=================>............] - ETA: 0s - loss: 26.12361650/2304 [====================>.........] - ETA: 0s - loss: 26.07171850/2304 [=======================>......] - ETA: 0s - loss: 26.12362050/2304 [=========================>....] - ETA: 0s - loss: 26.14182250/2304 [============================>.] - ETA: 0s - loss: 26.1282
Epoch 00015: val_loss improved from 23.36075 to 23.28553, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 300us/sample - loss: 26.1515 - val_loss: 23.2855
Epoch 16/50
  50/2304 [..............................] - ETA: 0s - loss: 27.5804 250/2304 [==>...........................] - ETA: 0s - loss: 26.9854 450/2304 [====>.........................] - ETA: 0s - loss: 26.9205 650/2304 [=======>......................] - ETA: 0s - loss: 26.9012 850/2304 [==========>...................] - ETA: 0s - loss: 26.91521050/2304 [============>.................] - ETA: 0s - loss: 26.71991250/2304 [===============>..............] - ETA: 0s - loss: 26.58491450/2304 [=================>............] - ETA: 0s - loss: 26.54981650/2304 [====================>.........] - ETA: 0s - loss: 26.57811850/2304 [=======================>......] - ETA: 0s - loss: 26.52002050/2304 [=========================>....] - ETA: 0s - loss: 26.52182250/2304 [============================>.] - ETA: 0s - loss: 26.4918
Epoch 00016: val_loss did not improve from 23.28553
2304/2304 [==============================] - 1s 291us/sample - loss: 26.4429 - val_loss: 24.2648
Epoch 17/50
  50/2304 [..............................] - ETA: 0s - loss: 24.4768 250/2304 [==>...........................] - ETA: 0s - loss: 25.5375 450/2304 [====>.........................] - ETA: 0s - loss: 26.4120 650/2304 [=======>......................] - ETA: 0s - loss: 26.3454 850/2304 [==========>...................] - ETA: 0s - loss: 26.31851050/2304 [============>.................] - ETA: 0s - loss: 26.30531250/2304 [===============>..............] - ETA: 0s - loss: 26.20451450/2304 [=================>............] - ETA: 0s - loss: 26.32291650/2304 [====================>.........] - ETA: 0s - loss: 26.36411850/2304 [=======================>......] - ETA: 0s - loss: 26.26672050/2304 [=========================>....] - ETA: 0s - loss: 26.18552250/2304 [============================>.] - ETA: 0s - loss: 26.1774
Epoch 00017: val_loss did not improve from 23.28553

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2304/2304 [==============================] - 1s 286us/sample - loss: 26.2036 - val_loss: 23.8126
Epoch 18/50
  50/2304 [..............................] - ETA: 0s - loss: 26.8782 250/2304 [==>...........................] - ETA: 0s - loss: 27.0759 450/2304 [====>.........................] - ETA: 0s - loss: 26.4078 650/2304 [=======>......................] - ETA: 0s - loss: 26.2665 850/2304 [==========>...................] - ETA: 0s - loss: 26.31871050/2304 [============>.................] - ETA: 0s - loss: 26.54241250/2304 [===============>..............] - ETA: 0s - loss: 26.38381450/2304 [=================>............] - ETA: 0s - loss: 26.37691650/2304 [====================>.........] - ETA: 0s - loss: 26.21441850/2304 [=======================>......] - ETA: 0s - loss: 26.09162050/2304 [=========================>....] - ETA: 0s - loss: 26.09872250/2304 [============================>.] - ETA: 0s - loss: 26.1650
Epoch 00018: val_loss improved from 23.28553 to 23.10864, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 301us/sample - loss: 26.1647 - val_loss: 23.1086
Epoch 19/50
  50/2304 [..............................] - ETA: 0s - loss: 25.2055 250/2304 [==>...........................] - ETA: 0s - loss: 25.9715 450/2304 [====>.........................] - ETA: 0s - loss: 25.7161 650/2304 [=======>......................] - ETA: 0s - loss: 25.9425 850/2304 [==========>...................] - ETA: 0s - loss: 26.00101050/2304 [============>.................] - ETA: 0s - loss: 26.17791250/2304 [===============>..............] - ETA: 0s - loss: 26.07661450/2304 [=================>............] - ETA: 0s - loss: 26.13351650/2304 [====================>.........] - ETA: 0s - loss: 25.96171850/2304 [=======================>......] - ETA: 0s - loss: 25.94202050/2304 [=========================>....] - ETA: 0s - loss: 25.99862250/2304 [============================>.] - ETA: 0s - loss: 25.9599
Epoch 00019: val_loss improved from 23.10864 to 22.97165, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 299us/sample - loss: 25.9756 - val_loss: 22.9717
Epoch 20/50
  50/2304 [..............................] - ETA: 0s - loss: 26.7134 250/2304 [==>...........................] - ETA: 0s - loss: 26.1183 450/2304 [====>.........................] - ETA: 0s - loss: 26.2123 650/2304 [=======>......................] - ETA: 0s - loss: 25.7797 850/2304 [==========>...................] - ETA: 0s - loss: 25.97161050/2304 [============>.................] - ETA: 0s - loss: 25.91441250/2304 [===============>..............] - ETA: 0s - loss: 25.84681450/2304 [=================>............] - ETA: 0s - loss: 25.90521650/2304 [====================>.........] - ETA: 0s - loss: 25.98181850/2304 [=======================>......] - ETA: 0s - loss: 26.03322050/2304 [=========================>....] - ETA: 0s - loss: 26.00092250/2304 [============================>.] - ETA: 0s - loss: 26.0276
Epoch 00020: val_loss did not improve from 22.97165
2304/2304 [==============================] - 1s 292us/sample - loss: 26.0516 - val_loss: 22.9743
Epoch 21/50
  50/2304 [..............................] - ETA: 0s - loss: 26.0181 250/2304 [==>...........................] - ETA: 0s - loss: 26.1002 450/2304 [====>.........................] - ETA: 0s - loss: 25.9315 650/2304 [=======>......................] - ETA: 0s - loss: 26.0299 850/2304 [==========>...................] - ETA: 0s - loss: 25.90851050/2304 [============>.................] - ETA: 0s - loss: 26.22751250/2304 [===============>..............] - ETA: 0s - loss: 26.14921450/2304 [=================>............] - ETA: 0s - loss: 26.05721650/2304 [====================>.........] - ETA: 0s - loss: 26.00921850/2304 [=======================>......] - ETA: 0s - loss: 26.05802050/2304 [=========================>....] - ETA: 0s - loss: 26.03772250/2304 [============================>.] - ETA: 0s - loss: 25.9980
Epoch 00021: val_loss did not improve from 22.97165

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2304/2304 [==============================] - 1s 290us/sample - loss: 26.0012 - val_loss: 23.0729
Epoch 22/50
  50/2304 [..............................] - ETA: 0s - loss: 24.3629 250/2304 [==>...........................] - ETA: 0s - loss: 25.7912 450/2304 [====>.........................] - ETA: 0s - loss: 25.9356 650/2304 [=======>......................] - ETA: 0s - loss: 26.2129 850/2304 [==========>...................] - ETA: 0s - loss: 26.15491050/2304 [============>.................] - ETA: 0s - loss: 25.98991250/2304 [===============>..............] - ETA: 0s - loss: 25.81061450/2304 [=================>............] - ETA: 0s - loss: 25.87541650/2304 [====================>.........] - ETA: 0s - loss: 25.90091850/2304 [=======================>......] - ETA: 0s - loss: 25.88692050/2304 [=========================>....] - ETA: 0s - loss: 26.05022250/2304 [============================>.] - ETA: 0s - loss: 26.0292
Epoch 00022: val_loss improved from 22.97165 to 22.95394, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 299us/sample - loss: 26.0272 - val_loss: 22.9539
Epoch 23/50
  50/2304 [..............................] - ETA: 0s - loss: 25.1159 250/2304 [==>...........................] - ETA: 0s - loss: 26.0297 450/2304 [====>.........................] - ETA: 0s - loss: 26.0485 650/2304 [=======>......................] - ETA: 0s - loss: 25.8973 850/2304 [==========>...................] - ETA: 0s - loss: 25.88751050/2304 [============>.................] - ETA: 0s - loss: 25.97301250/2304 [===============>..............] - ETA: 0s - loss: 25.93531450/2304 [=================>............] - ETA: 0s - loss: 25.95021650/2304 [====================>.........] - ETA: 0s - loss: 25.93881850/2304 [=======================>......] - ETA: 0s - loss: 25.95172050/2304 [=========================>....] - ETA: 0s - loss: 25.94692250/2304 [============================>.] - ETA: 0s - loss: 25.9174
Epoch 00023: val_loss did not improve from 22.95394
2304/2304 [==============================] - 1s 283us/sample - loss: 25.9327 - val_loss: 23.1687
Epoch 24/50
  50/2304 [..............................] - ETA: 0s - loss: 24.4595 250/2304 [==>...........................] - ETA: 0s - loss: 25.5856 450/2304 [====>.........................] - ETA: 0s - loss: 25.8186 650/2304 [=======>......................] - ETA: 0s - loss: 25.9348 850/2304 [==========>...................] - ETA: 0s - loss: 25.79621050/2304 [============>.................] - ETA: 0s - loss: 25.84531200/2304 [==============>...............] - ETA: 0s - loss: 25.76431300/2304 [===============>..............] - ETA: 0s - loss: 25.69881500/2304 [==================>...........] - ETA: 0s - loss: 25.79761700/2304 [=====================>........] - ETA: 0s - loss: 25.78911900/2304 [=======================>......] - ETA: 0s - loss: 25.84042100/2304 [==========================>...] - ETA: 0s - loss: 25.81862300/2304 [============================>.] - ETA: 0s - loss: 25.9233
Epoch 00024: val_loss improved from 22.95394 to 22.91830, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 318us/sample - loss: 25.9284 - val_loss: 22.9183
Epoch 25/50
  50/2304 [..............................] - ETA: 0s - loss: 24.9354 250/2304 [==>...........................] - ETA: 0s - loss: 26.7524 450/2304 [====>.........................] - ETA: 0s - loss: 26.7189 650/2304 [=======>......................] - ETA: 0s - loss: 26.1565 850/2304 [==========>...................] - ETA: 0s - loss: 26.14971050/2304 [============>.................] - ETA: 0s - loss: 26.08481250/2304 [===============>..............] - ETA: 0s - loss: 25.91961450/2304 [=================>............] - ETA: 0s - loss: 25.82321650/2304 [====================>.........] - ETA: 0s - loss: 25.92931850/2304 [=======================>......] - ETA: 0s - loss: 25.92092050/2304 [=========================>....] - ETA: 0s - loss: 25.89612250/2304 [============================>.] - ETA: 0s - loss: 25.9591
Epoch 00025: val_loss improved from 22.91830 to 22.89514, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
2304/2304 [==============================] - 1s 298us/sample - loss: 25.9692 - val_loss: 22.8951
Epoch 26/50
  50/2304 [..............................] - ETA: 0s - loss: 24.6062 250/2304 [==>...........................] - ETA: 0s - loss: 25.9805 450/2304 [====>.........................] - ETA: 0s - loss: 25.8618 650/2304 [=======>......................] - ETA: 0s - loss: 26.2339 850/2304 [==========>...................] - ETA: 0s - loss: 26.29691050/2304 [============>.................] - ETA: 0s - loss: 26.22421250/2304 [===============>..............] - ETA: 0s - loss: 26.26621450/2304 [=================>............] - ETA: 0s - loss: 26.09541650/2304 [====================>.........] - ETA: 0s - loss: 25.97341850/2304 [=======================>......] - ETA: 0s - loss: 25.94922050/2304 [=========================>....] - ETA: 0s - loss: 25.95502250/2304 [============================>.] - ETA: 0s - loss: 25.9439
Epoch 00026: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 287us/sample - loss: 25.9819 - val_loss: 22.9602
Epoch 27/50
  50/2304 [..............................] - ETA: 0s - loss: 28.1507 250/2304 [==>...........................] - ETA: 0s - loss: 26.1414 450/2304 [====>.........................] - ETA: 0s - loss: 26.1636 650/2304 [=======>......................] - ETA: 0s - loss: 26.1283 850/2304 [==========>...................] - ETA: 0s - loss: 26.18361050/2304 [============>.................] - ETA: 0s - loss: 26.31731250/2304 [===============>..............] - ETA: 0s - loss: 26.11241450/2304 [=================>............] - ETA: 0s - loss: 26.19641650/2304 [====================>.........] - ETA: 0s - loss: 26.14701850/2304 [=======================>......] - ETA: 0s - loss: 26.12792050/2304 [=========================>....] - ETA: 0s - loss: 26.05282250/2304 [============================>.] - ETA: 0s - loss: 26.0403
Epoch 00027: val_loss did not improve from 22.89514

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2304/2304 [==============================] - 1s 288us/sample - loss: 25.9995 - val_loss: 23.0254
Epoch 28/50
  50/2304 [..............................] - ETA: 0s - loss: 23.6776 250/2304 [==>...........................] - ETA: 0s - loss: 25.0203 450/2304 [====>.........................] - ETA: 0s - loss: 25.2714 650/2304 [=======>......................] - ETA: 0s - loss: 25.4100 850/2304 [==========>...................] - ETA: 0s - loss: 25.61271050/2304 [============>.................] - ETA: 0s - loss: 25.63611250/2304 [===============>..............] - ETA: 0s - loss: 25.63381450/2304 [=================>............] - ETA: 0s - loss: 25.76331650/2304 [====================>.........] - ETA: 0s - loss: 25.92561850/2304 [=======================>......] - ETA: 0s - loss: 25.93832050/2304 [=========================>....] - ETA: 0s - loss: 25.94832250/2304 [============================>.] - ETA: 0s - loss: 25.9507
Epoch 00028: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 285us/sample - loss: 25.9358 - val_loss: 22.9202
Epoch 29/50
  50/2304 [..............................] - ETA: 0s - loss: 24.7078 250/2304 [==>...........................] - ETA: 0s - loss: 25.3426 450/2304 [====>.........................] - ETA: 0s - loss: 25.3879 650/2304 [=======>......................] - ETA: 0s - loss: 25.2929 850/2304 [==========>...................] - ETA: 0s - loss: 25.46441050/2304 [============>.................] - ETA: 0s - loss: 25.74441250/2304 [===============>..............] - ETA: 0s - loss: 25.80421450/2304 [=================>............] - ETA: 0s - loss: 25.70851650/2304 [====================>.........] - ETA: 0s - loss: 25.66451850/2304 [=======================>......] - ETA: 0s - loss: 25.75072050/2304 [=========================>....] - ETA: 0s - loss: 25.78282250/2304 [============================>.] - ETA: 0s - loss: 25.8466
Epoch 00029: val_loss did not improve from 22.89514

Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2304/2304 [==============================] - 1s 281us/sample - loss: 25.8880 - val_loss: 22.9244
Epoch 30/50
  50/2304 [..............................] - ETA: 0s - loss: 24.6779 250/2304 [==>...........................] - ETA: 0s - loss: 25.8335 450/2304 [====>.........................] - ETA: 0s - loss: 25.5210 650/2304 [=======>......................] - ETA: 0s - loss: 25.6080 850/2304 [==========>...................] - ETA: 0s - loss: 25.89251050/2304 [============>.................] - ETA: 0s - loss: 25.85121250/2304 [===============>..............] - ETA: 0s - loss: 25.79721450/2304 [=================>............] - ETA: 0s - loss: 25.82731650/2304 [====================>.........] - ETA: 0s - loss: 25.78121850/2304 [=======================>......] - ETA: 0s - loss: 25.84592050/2304 [=========================>....] - ETA: 0s - loss: 25.81072250/2304 [============================>.] - ETA: 0s - loss: 25.9251
Epoch 00030: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 293us/sample - loss: 25.9156 - val_loss: 22.9890
Epoch 31/50
  50/2304 [..............................] - ETA: 0s - loss: 27.6489 250/2304 [==>...........................] - ETA: 0s - loss: 25.8333 450/2304 [====>.........................] - ETA: 0s - loss: 25.7545 650/2304 [=======>......................] - ETA: 0s - loss: 25.8521 850/2304 [==========>...................] - ETA: 0s - loss: 26.13171050/2304 [============>.................] - ETA: 0s - loss: 25.94501250/2304 [===============>..............] - ETA: 0s - loss: 25.91581450/2304 [=================>............] - ETA: 0s - loss: 25.90601650/2304 [====================>.........] - ETA: 0s - loss: 25.90931850/2304 [=======================>......] - ETA: 0s - loss: 25.89992050/2304 [=========================>....] - ETA: 0s - loss: 25.87782250/2304 [============================>.] - ETA: 0s - loss: 25.8471
Epoch 00031: val_loss did not improve from 22.89514

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-05.
2304/2304 [==============================] - 1s 286us/sample - loss: 25.8315 - val_loss: 22.9794
Epoch 32/50
  50/2304 [..............................] - ETA: 0s - loss: 24.7269 250/2304 [==>...........................] - ETA: 0s - loss: 26.4608 450/2304 [====>.........................] - ETA: 0s - loss: 26.1229 650/2304 [=======>......................] - ETA: 0s - loss: 26.0969 850/2304 [==========>...................] - ETA: 0s - loss: 25.85431050/2304 [============>.................] - ETA: 0s - loss: 26.03561250/2304 [===============>..............] - ETA: 0s - loss: 26.11891450/2304 [=================>............] - ETA: 0s - loss: 25.97051650/2304 [====================>.........] - ETA: 0s - loss: 26.00911850/2304 [=======================>......] - ETA: 0s - loss: 25.88982050/2304 [=========================>....] - ETA: 0s - loss: 25.90412250/2304 [============================>.] - ETA: 0s - loss: 25.9109
Epoch 00032: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 278us/sample - loss: 25.8966 - val_loss: 22.9878
Epoch 33/50
  50/2304 [..............................] - ETA: 0s - loss: 25.5610 250/2304 [==>...........................] - ETA: 0s - loss: 26.0149 450/2304 [====>.........................] - ETA: 0s - loss: 25.5270 650/2304 [=======>......................] - ETA: 0s - loss: 25.8502 850/2304 [==========>...................] - ETA: 0s - loss: 25.82971050/2304 [============>.................] - ETA: 0s - loss: 25.96111250/2304 [===============>..............] - ETA: 0s - loss: 26.03651450/2304 [=================>............] - ETA: 0s - loss: 25.98291650/2304 [====================>.........] - ETA: 0s - loss: 25.94181850/2304 [=======================>......] - ETA: 0s - loss: 25.83432050/2304 [=========================>....] - ETA: 0s - loss: 25.86752250/2304 [============================>.] - ETA: 0s - loss: 25.9581
Epoch 00033: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 287us/sample - loss: 25.9487 - val_loss: 22.9849
Epoch 34/50
  50/2304 [..............................] - ETA: 0s - loss: 25.2378 250/2304 [==>...........................] - ETA: 0s - loss: 26.0879 450/2304 [====>.........................] - ETA: 0s - loss: 26.1577 650/2304 [=======>......................] - ETA: 0s - loss: 26.0828 850/2304 [==========>...................] - ETA: 0s - loss: 25.98511050/2304 [============>.................] - ETA: 0s - loss: 25.88171250/2304 [===============>..............] - ETA: 0s - loss: 25.62771450/2304 [=================>............] - ETA: 0s - loss: 25.71601650/2304 [====================>.........] - ETA: 0s - loss: 25.74861850/2304 [=======================>......] - ETA: 0s - loss: 25.79722050/2304 [=========================>....] - ETA: 0s - loss: 25.76792250/2304 [============================>.] - ETA: 0s - loss: 25.8177
Epoch 00034: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 287us/sample - loss: 25.8164 - val_loss: 22.9835
Epoch 35/50
  50/2304 [..............................] - ETA: 0s - loss: 25.6846 250/2304 [==>...........................] - ETA: 0s - loss: 26.1313 450/2304 [====>.........................] - ETA: 0s - loss: 26.1882 650/2304 [=======>......................] - ETA: 0s - loss: 26.0410 850/2304 [==========>...................] - ETA: 0s - loss: 25.84721050/2304 [============>.................] - ETA: 0s - loss: 25.78391250/2304 [===============>..............] - ETA: 0s - loss: 25.90181450/2304 [=================>............] - ETA: 0s - loss: 26.00191650/2304 [====================>.........] - ETA: 0s - loss: 25.91211850/2304 [=======================>......] - ETA: 0s - loss: 25.85632050/2304 [=========================>....] - ETA: 0s - loss: 25.78372250/2304 [============================>.] - ETA: 0s - loss: 25.7727
Epoch 00035: val_loss did not improve from 22.89514
2304/2304 [==============================] - 1s 286us/sample - loss: 25.8483 - val_loss: 22.9686
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:209: RuntimeWarning: divide by zero encountered in log
  predictor_matrix[:,:,:,indicesPRED] = numpy.log(predictor_matrix[:,:,:,indicesPRED])
Epoch 00035: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
Traceback (most recent call last):
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "Run_CNN_FineTune_ByYear.py", line 347, in <module>
    model.load_weights(Wsave_name)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 234, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1187, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 99, in NewCheckpointReader
    error_translator(e)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear1999/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
