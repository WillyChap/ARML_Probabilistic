2020-11-09 21:09:33.088695: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 21:09:33.427107: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 21:09:33.427385: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f0b1a2eef0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:09:33.427414: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 21:09:33.447042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 21:09:33.578668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:09:33.603012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:09:33.815736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:09:33.873031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:09:34.001736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:09:34.133643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:09:34.258024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:09:34.384751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:09:34.390430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:09:34.390538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:09:34.573536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:09:34.573604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:09:34.573634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:09:34.578833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:09:34.581766: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f0b27aa640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:09:34.581795: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 21:09:34.585384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:09:34.585471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:09:34.585490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:09:34.585506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:09:34.588460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:09:34.588478: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:09:34.588494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:09:34.588509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:09:34.593168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:09:34.593209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:09:34.593222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:09:34.593231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:09:34.596252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:09:34.601747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:09:34.601819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:09:34.601838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:09:34.601854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:09:34.601869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:09:34.601885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:09:34.601899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:09:34.601914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:09:34.606326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:09:34.606364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:09:34.606376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:09:34.606385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:09:34.611244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 159.4941, 152.8736
Mean and standard deviation for "p_sfc" = 984.0989, 62.0536
Mean and standard deviation for "u_tr_p" = 12.7292, 12.2947
Mean and standard deviation for "v_tr_p" = 1.3396, 13.2977
Mean and standard deviation for "Z_p" = 5572.9240, 203.8470
Mean and standard deviation for "IWV" = 13.3481, 7.6647
2020-11-09 21:09:55.111787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:09:55.118171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:09:55.118198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:09:55.118215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:09:55.118230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:09:55.118246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:09:55.118261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:09:55.118277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:09:55.121339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:09:55.216827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:09:55.216940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:09:55.216959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:09:55.216974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:09:55.217684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:09:55.217702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:09:55.217717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:09:55.217733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:09:55.224718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:09:55.224766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:09:55.224778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:09:55.224787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:09:55.227991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:10:04.379130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:10:08.274979: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 21:10:08.313671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 202.7021, 181.6135
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 1940 samples, validate on 121 samples
Epoch 1/50
  50/1940 [..............................] - ETA: 4:34 - loss: 160.1881 100/1940 [>.............................] - ETA: 2:14 - loss: 159.2104 300/1940 [===>..........................] - ETA: 40s - loss: 159.2041  500/1940 [======>.......................] - ETA: 21s - loss: 152.1240 700/1940 [=========>....................] - ETA: 13s - loss: 140.1409 900/1940 [============>.................] - ETA: 8s - loss: 123.2986 1100/1940 [================>.............] - ETA: 5s - loss: 111.67471300/1940 [===================>..........] - ETA: 3s - loss: 101.48651500/1940 [======================>.......] - ETA: 2s - loss: 93.9784 1700/1940 [=========================>....] - ETA: 1s - loss: 87.49521900/1940 [============================>.] - ETA: 0s - loss: 82.2519
Epoch 00001: val_loss improved from inf to 75.69898, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 9s 4ms/sample - loss: 81.3544 - val_loss: 75.6990
Epoch 2/50
  50/1940 [..............................] - ETA: 0s - loss: 34.3963 250/1940 [==>...........................] - ETA: 0s - loss: 35.6911 450/1940 [=====>........................] - ETA: 0s - loss: 34.9505 650/1940 [=========>....................] - ETA: 0s - loss: 34.5401 850/1940 [============>.................] - ETA: 0s - loss: 33.89271050/1940 [===============>..............] - ETA: 0s - loss: 33.77251250/1940 [==================>...........] - ETA: 0s - loss: 33.55021450/1940 [=====================>........] - ETA: 0s - loss: 33.18501650/1940 [========================>.....] - ETA: 0s - loss: 33.04641850/1940 [===========================>..] - ETA: 0s - loss: 32.74071900/1940 [============================>.] - ETA: 0s - loss: 32.6930
Epoch 00002: val_loss improved from 75.69898 to 74.95280, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 335us/sample - loss: 32.6451 - val_loss: 74.9528
Epoch 3/50
  50/1940 [..............................] - ETA: 0s - loss: 30.2769 250/1940 [==>...........................] - ETA: 0s - loss: 29.4925 450/1940 [=====>........................] - ETA: 0s - loss: 29.3426 650/1940 [=========>....................] - ETA: 0s - loss: 29.5001 850/1940 [============>.................] - ETA: 0s - loss: 29.45171050/1940 [===============>..............] - ETA: 0s - loss: 29.13131250/1940 [==================>...........] - ETA: 0s - loss: 29.46991450/1940 [=====================>........] - ETA: 0s - loss: 29.54251650/1940 [========================>.....] - ETA: 0s - loss: 29.44971850/1940 [===========================>..] - ETA: 0s - loss: 29.3581
Epoch 00003: val_loss improved from 74.95280 to 43.22428, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 301us/sample - loss: 29.3412 - val_loss: 43.2243
Epoch 4/50
  50/1940 [..............................] - ETA: 0s - loss: 28.2960 250/1940 [==>...........................] - ETA: 0s - loss: 28.6060 450/1940 [=====>........................] - ETA: 0s - loss: 28.7774 650/1940 [=========>....................] - ETA: 0s - loss: 28.3656 850/1940 [============>.................] - ETA: 0s - loss: 28.65091050/1940 [===============>..............] - ETA: 0s - loss: 28.53721250/1940 [==================>...........] - ETA: 0s - loss: 28.57301450/1940 [=====================>........] - ETA: 0s - loss: 28.43871650/1940 [========================>.....] - ETA: 0s - loss: 28.54171850/1940 [===========================>..] - ETA: 0s - loss: 28.4962
Epoch 00004: val_loss improved from 43.22428 to 38.72754, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 296us/sample - loss: 28.4044 - val_loss: 38.7275
Epoch 5/50
  50/1940 [..............................] - ETA: 0s - loss: 26.9040 250/1940 [==>...........................] - ETA: 0s - loss: 27.0401 450/1940 [=====>........................] - ETA: 0s - loss: 26.6508 650/1940 [=========>....................] - ETA: 0s - loss: 26.8773 850/1940 [============>.................] - ETA: 0s - loss: 27.07081050/1940 [===============>..............] - ETA: 0s - loss: 26.94211250/1940 [==================>...........] - ETA: 0s - loss: 26.97591450/1940 [=====================>........] - ETA: 0s - loss: 27.14921650/1940 [========================>.....] - ETA: 0s - loss: 27.12711750/1940 [==========================>...] - ETA: 0s - loss: 27.20511900/1940 [============================>.] - ETA: 0s - loss: 27.1783
Epoch 00005: val_loss improved from 38.72754 to 30.14126, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 391us/sample - loss: 27.2072 - val_loss: 30.1413
Epoch 6/50
  50/1940 [..............................] - ETA: 0s - loss: 25.8885 250/1940 [==>...........................] - ETA: 0s - loss: 26.1910 450/1940 [=====>........................] - ETA: 0s - loss: 26.2049 650/1940 [=========>....................] - ETA: 0s - loss: 25.8680 850/1940 [============>.................] - ETA: 0s - loss: 25.94601050/1940 [===============>..............] - ETA: 0s - loss: 26.56681250/1940 [==================>...........] - ETA: 0s - loss: 26.74971450/1940 [=====================>........] - ETA: 0s - loss: 26.64401650/1940 [========================>.....] - ETA: 0s - loss: 26.68681850/1940 [===========================>..] - ETA: 0s - loss: 26.7689
Epoch 00006: val_loss improved from 30.14126 to 29.60391, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 301us/sample - loss: 26.8407 - val_loss: 29.6039
Epoch 7/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2796 250/1940 [==>...........................] - ETA: 0s - loss: 26.4222 450/1940 [=====>........................] - ETA: 0s - loss: 26.6584 650/1940 [=========>....................] - ETA: 0s - loss: 26.7634 850/1940 [============>.................] - ETA: 0s - loss: 26.87721050/1940 [===============>..............] - ETA: 0s - loss: 26.78711250/1940 [==================>...........] - ETA: 0s - loss: 26.95941450/1940 [=====================>........] - ETA: 0s - loss: 26.85621650/1940 [========================>.....] - ETA: 0s - loss: 26.81681850/1940 [===========================>..] - ETA: 0s - loss: 26.7081
Epoch 00007: val_loss improved from 29.60391 to 26.81533, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 295us/sample - loss: 26.7630 - val_loss: 26.8153
Epoch 8/50
  50/1940 [..............................] - ETA: 0s - loss: 28.7245 250/1940 [==>...........................] - ETA: 0s - loss: 26.4821 450/1940 [=====>........................] - ETA: 0s - loss: 26.2715 650/1940 [=========>....................] - ETA: 0s - loss: 25.9590 850/1940 [============>.................] - ETA: 0s - loss: 26.34631050/1940 [===============>..............] - ETA: 0s - loss: 26.51351250/1940 [==================>...........] - ETA: 0s - loss: 26.56531450/1940 [=====================>........] - ETA: 0s - loss: 26.49941650/1940 [========================>.....] - ETA: 0s - loss: 26.77301850/1940 [===========================>..] - ETA: 0s - loss: 26.7662
Epoch 00008: val_loss did not improve from 26.81533
1940/1940 [==============================] - 1s 279us/sample - loss: 26.7083 - val_loss: 28.2051
Epoch 9/50
  50/1940 [..............................] - ETA: 0s - loss: 25.8902 250/1940 [==>...........................] - ETA: 0s - loss: 26.1347 450/1940 [=====>........................] - ETA: 0s - loss: 26.5116 650/1940 [=========>....................] - ETA: 0s - loss: 26.6155 850/1940 [============>.................] - ETA: 0s - loss: 26.56041050/1940 [===============>..............] - ETA: 0s - loss: 26.79341250/1940 [==================>...........] - ETA: 0s - loss: 26.87081450/1940 [=====================>........] - ETA: 0s - loss: 26.69511650/1940 [========================>.....] - ETA: 0s - loss: 26.55341850/1940 [===========================>..] - ETA: 0s - loss: 26.5125
Epoch 00009: val_loss improved from 26.81533 to 25.82286, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 294us/sample - loss: 26.5640 - val_loss: 25.8229
Epoch 10/50
  50/1940 [..............................] - ETA: 0s - loss: 28.5894 250/1940 [==>...........................] - ETA: 0s - loss: 27.0137 450/1940 [=====>........................] - ETA: 0s - loss: 26.4388 650/1940 [=========>....................] - ETA: 0s - loss: 26.6542 850/1940 [============>.................] - ETA: 0s - loss: 26.49111050/1940 [===============>..............] - ETA: 0s - loss: 26.47701250/1940 [==================>...........] - ETA: 0s - loss: 26.15331450/1940 [=====================>........] - ETA: 0s - loss: 26.23721650/1940 [========================>.....] - ETA: 0s - loss: 26.27701850/1940 [===========================>..] - ETA: 0s - loss: 26.3466
Epoch 00010: val_loss improved from 25.82286 to 25.68777, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 295us/sample - loss: 26.2907 - val_loss: 25.6878
Epoch 11/50
  50/1940 [..............................] - ETA: 0s - loss: 24.6396 250/1940 [==>...........................] - ETA: 0s - loss: 26.5771 450/1940 [=====>........................] - ETA: 0s - loss: 26.3532 650/1940 [=========>....................] - ETA: 0s - loss: 26.1440 850/1940 [============>.................] - ETA: 0s - loss: 26.11721050/1940 [===============>..............] - ETA: 0s - loss: 26.20511250/1940 [==================>...........] - ETA: 0s - loss: 26.19911450/1940 [=====================>........] - ETA: 0s - loss: 26.07811650/1940 [========================>.....] - ETA: 0s - loss: 26.14441850/1940 [===========================>..] - ETA: 0s - loss: 26.2156
Epoch 00011: val_loss improved from 25.68777 to 24.27307, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 298us/sample - loss: 26.2982 - val_loss: 24.2731
Epoch 12/50
  50/1940 [..............................] - ETA: 0s - loss: 27.1649 250/1940 [==>...........................] - ETA: 0s - loss: 26.1347 450/1940 [=====>........................] - ETA: 0s - loss: 26.2551 650/1940 [=========>....................] - ETA: 0s - loss: 26.0745 850/1940 [============>.................] - ETA: 0s - loss: 26.11711050/1940 [===============>..............] - ETA: 0s - loss: 26.00891250/1940 [==================>...........] - ETA: 0s - loss: 26.14571450/1940 [=====================>........] - ETA: 0s - loss: 26.11441650/1940 [========================>.....] - ETA: 0s - loss: 26.07221850/1940 [===========================>..] - ETA: 0s - loss: 26.1226
Epoch 00012: val_loss improved from 24.27307 to 23.85794, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 305us/sample - loss: 26.1640 - val_loss: 23.8579
Epoch 13/50
  50/1940 [..............................] - ETA: 0s - loss: 27.0841 250/1940 [==>...........................] - ETA: 0s - loss: 25.9331 450/1940 [=====>........................] - ETA: 0s - loss: 25.6845 650/1940 [=========>....................] - ETA: 0s - loss: 26.3307 850/1940 [============>.................] - ETA: 0s - loss: 26.34751050/1940 [===============>..............] - ETA: 0s - loss: 26.46681250/1940 [==================>...........] - ETA: 0s - loss: 26.42861450/1940 [=====================>........] - ETA: 0s - loss: 26.43911650/1940 [========================>.....] - ETA: 0s - loss: 26.38981850/1940 [===========================>..] - ETA: 0s - loss: 26.3767
Epoch 00013: val_loss did not improve from 23.85794
1940/1940 [==============================] - 1s 279us/sample - loss: 26.3131 - val_loss: 24.8810
Epoch 14/50
  50/1940 [..............................] - ETA: 0s - loss: 25.3193 250/1940 [==>...........................] - ETA: 0s - loss: 25.2204 450/1940 [=====>........................] - ETA: 0s - loss: 26.1666 650/1940 [=========>....................] - ETA: 0s - loss: 25.9956 850/1940 [============>.................] - ETA: 0s - loss: 26.40551050/1940 [===============>..............] - ETA: 0s - loss: 26.43211250/1940 [==================>...........] - ETA: 0s - loss: 26.45041450/1940 [=====================>........] - ETA: 0s - loss: 26.39671650/1940 [========================>.....] - ETA: 0s - loss: 26.41421850/1940 [===========================>..] - ETA: 0s - loss: 26.3627
Epoch 00014: val_loss improved from 23.85794 to 23.20127, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 311us/sample - loss: 26.3037 - val_loss: 23.2013
Epoch 15/50
  50/1940 [..............................] - ETA: 0s - loss: 27.0181 250/1940 [==>...........................] - ETA: 0s - loss: 26.0853 450/1940 [=====>........................] - ETA: 0s - loss: 26.0919 650/1940 [=========>....................] - ETA: 0s - loss: 25.8874 850/1940 [============>.................] - ETA: 0s - loss: 25.66061050/1940 [===============>..............] - ETA: 0s - loss: 25.80061250/1940 [==================>...........] - ETA: 0s - loss: 25.86341450/1940 [=====================>........] - ETA: 0s - loss: 25.75361650/1940 [========================>.....] - ETA: 0s - loss: 25.71711850/1940 [===========================>..] - ETA: 0s - loss: 25.6949
Epoch 00015: val_loss did not improve from 23.20127
1940/1940 [==============================] - 1s 281us/sample - loss: 25.7392 - val_loss: 23.9056
Epoch 16/50
  50/1940 [..............................] - ETA: 0s - loss: 25.6647 250/1940 [==>...........................] - ETA: 0s - loss: 25.9515 450/1940 [=====>........................] - ETA: 0s - loss: 25.5841 650/1940 [=========>....................] - ETA: 0s - loss: 25.7527 850/1940 [============>.................] - ETA: 0s - loss: 25.95371050/1940 [===============>..............] - ETA: 0s - loss: 26.11181250/1940 [==================>...........] - ETA: 0s - loss: 26.21241450/1940 [=====================>........] - ETA: 0s - loss: 26.22861650/1940 [========================>.....] - ETA: 0s - loss: 26.09641850/1940 [===========================>..] - ETA: 0s - loss: 26.1422
Epoch 00016: val_loss did not improve from 23.20127

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1940/1940 [==============================] - 1s 279us/sample - loss: 26.0567 - val_loss: 23.7291
Epoch 17/50
  50/1940 [..............................] - ETA: 0s - loss: 24.2515 250/1940 [==>...........................] - ETA: 0s - loss: 25.0561 450/1940 [=====>........................] - ETA: 0s - loss: 25.3662 650/1940 [=========>....................] - ETA: 0s - loss: 25.1693 850/1940 [============>.................] - ETA: 0s - loss: 25.46301050/1940 [===============>..............] - ETA: 0s - loss: 25.62151250/1940 [==================>...........] - ETA: 0s - loss: 25.55731450/1940 [=====================>........] - ETA: 0s - loss: 25.65391600/1940 [=======================>......] - ETA: 0s - loss: 25.74041800/1940 [==========================>...] - ETA: 0s - loss: 25.7598
Epoch 00017: val_loss improved from 23.20127 to 23.10922, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 307us/sample - loss: 25.7257 - val_loss: 23.1092
Epoch 18/50
  50/1940 [..............................] - ETA: 0s - loss: 24.6552 250/1940 [==>...........................] - ETA: 0s - loss: 25.7041 450/1940 [=====>........................] - ETA: 0s - loss: 25.8446 650/1940 [=========>....................] - ETA: 0s - loss: 25.6423 850/1940 [============>.................] - ETA: 0s - loss: 25.58651050/1940 [===============>..............] - ETA: 0s - loss: 25.54831250/1940 [==================>...........] - ETA: 0s - loss: 25.49001450/1940 [=====================>........] - ETA: 0s - loss: 25.54781650/1940 [========================>.....] - ETA: 0s - loss: 25.56531850/1940 [===========================>..] - ETA: 0s - loss: 25.6434
Epoch 00018: val_loss did not improve from 23.10922
1940/1940 [==============================] - 1s 281us/sample - loss: 25.6644 - val_loss: 23.4436
Epoch 19/50
  50/1940 [..............................] - ETA: 0s - loss: 28.3843 250/1940 [==>...........................] - ETA: 0s - loss: 26.5566 450/1940 [=====>........................] - ETA: 0s - loss: 25.8569 650/1940 [=========>....................] - ETA: 0s - loss: 25.8770 850/1940 [============>.................] - ETA: 0s - loss: 25.81891050/1940 [===============>..............] - ETA: 0s - loss: 25.69751250/1940 [==================>...........] - ETA: 0s - loss: 25.92001450/1940 [=====================>........] - ETA: 0s - loss: 25.88621650/1940 [========================>.....] - ETA: 0s - loss: 25.77931850/1940 [===========================>..] - ETA: 0s - loss: 25.8297
Epoch 00019: val_loss did not improve from 23.10922

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1940/1940 [==============================] - 1s 280us/sample - loss: 25.8222 - val_loss: 23.1474
Epoch 20/50
  50/1940 [..............................] - ETA: 0s - loss: 25.7730 250/1940 [==>...........................] - ETA: 0s - loss: 25.1423 450/1940 [=====>........................] - ETA: 0s - loss: 25.8116 650/1940 [=========>....................] - ETA: 0s - loss: 25.5153 850/1940 [============>.................] - ETA: 0s - loss: 25.52121050/1940 [===============>..............] - ETA: 0s - loss: 25.55691250/1940 [==================>...........] - ETA: 0s - loss: 25.69461450/1940 [=====================>........] - ETA: 0s - loss: 25.71041650/1940 [========================>.....] - ETA: 0s - loss: 25.67141850/1940 [===========================>..] - ETA: 0s - loss: 25.7326
Epoch 00020: val_loss improved from 23.10922 to 22.99571, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 296us/sample - loss: 25.7735 - val_loss: 22.9957
Epoch 21/50
  50/1940 [..............................] - ETA: 0s - loss: 25.0906 250/1940 [==>...........................] - ETA: 0s - loss: 24.9433 450/1940 [=====>........................] - ETA: 0s - loss: 25.4894 650/1940 [=========>....................] - ETA: 0s - loss: 26.0063 850/1940 [============>.................] - ETA: 0s - loss: 25.92921050/1940 [===============>..............] - ETA: 0s - loss: 26.10981250/1940 [==================>...........] - ETA: 0s - loss: 26.02891450/1940 [=====================>........] - ETA: 0s - loss: 26.21271650/1940 [========================>.....] - ETA: 0s - loss: 25.95421850/1940 [===========================>..] - ETA: 0s - loss: 26.0220
Epoch 00021: val_loss did not improve from 22.99571
1940/1940 [==============================] - 1s 280us/sample - loss: 26.0047 - val_loss: 23.0337
Epoch 22/50
  50/1940 [..............................] - ETA: 0s - loss: 26.3149 250/1940 [==>...........................] - ETA: 0s - loss: 25.6956 450/1940 [=====>........................] - ETA: 0s - loss: 25.9815 650/1940 [=========>....................] - ETA: 0s - loss: 25.5932 850/1940 [============>.................] - ETA: 0s - loss: 25.61161050/1940 [===============>..............] - ETA: 0s - loss: 25.60131250/1940 [==================>...........] - ETA: 0s - loss: 25.57131450/1940 [=====================>........] - ETA: 0s - loss: 25.50851650/1940 [========================>.....] - ETA: 0s - loss: 25.61391850/1940 [===========================>..] - ETA: 0s - loss: 25.6726
Epoch 00022: val_loss did not improve from 22.99571

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1940/1940 [==============================] - 1s 284us/sample - loss: 25.6601 - val_loss: 23.0102
Epoch 23/50
  50/1940 [..............................] - ETA: 0s - loss: 26.3553 250/1940 [==>...........................] - ETA: 0s - loss: 25.2062 450/1940 [=====>........................] - ETA: 0s - loss: 25.1926 650/1940 [=========>....................] - ETA: 0s - loss: 25.5195 850/1940 [============>.................] - ETA: 0s - loss: 25.51421050/1940 [===============>..............] - ETA: 0s - loss: 25.66471250/1940 [==================>...........] - ETA: 0s - loss: 25.68901450/1940 [=====================>........] - ETA: 0s - loss: 25.58941650/1940 [========================>.....] - ETA: 0s - loss: 25.54331850/1940 [===========================>..] - ETA: 0s - loss: 25.5823
Epoch 00023: val_loss improved from 22.99571 to 22.94486, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 370us/sample - loss: 25.6079 - val_loss: 22.9449
Epoch 24/50
  50/1940 [..............................] - ETA: 0s - loss: 26.1877 250/1940 [==>...........................] - ETA: 0s - loss: 25.2179 450/1940 [=====>........................] - ETA: 0s - loss: 25.0476 650/1940 [=========>....................] - ETA: 0s - loss: 25.1935 850/1940 [============>.................] - ETA: 0s - loss: 25.29981050/1940 [===============>..............] - ETA: 0s - loss: 25.33691250/1940 [==================>...........] - ETA: 0s - loss: 25.25891450/1940 [=====================>........] - ETA: 0s - loss: 25.32371650/1940 [========================>.....] - ETA: 0s - loss: 25.45581850/1940 [===========================>..] - ETA: 0s - loss: 25.5252
Epoch 00024: val_loss did not improve from 22.94486
1940/1940 [==============================] - 1s 282us/sample - loss: 25.6367 - val_loss: 23.0396
Epoch 25/50
  50/1940 [..............................] - ETA: 0s - loss: 25.4302 250/1940 [==>...........................] - ETA: 0s - loss: 25.9975 450/1940 [=====>........................] - ETA: 0s - loss: 25.5422 650/1940 [=========>....................] - ETA: 0s - loss: 25.3216 850/1940 [============>.................] - ETA: 0s - loss: 25.34691050/1940 [===============>..............] - ETA: 0s - loss: 25.37811250/1940 [==================>...........] - ETA: 0s - loss: 25.53281450/1940 [=====================>........] - ETA: 0s - loss: 25.63591650/1940 [========================>.....] - ETA: 0s - loss: 25.72111850/1940 [===========================>..] - ETA: 0s - loss: 25.7085
Epoch 00025: val_loss did not improve from 22.94486

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1940/1940 [==============================] - 1s 280us/sample - loss: 25.6643 - val_loss: 22.9799
Epoch 26/50
  50/1940 [..............................] - ETA: 0s - loss: 25.8266 250/1940 [==>...........................] - ETA: 0s - loss: 25.8803 450/1940 [=====>........................] - ETA: 0s - loss: 25.4885 650/1940 [=========>....................] - ETA: 0s - loss: 25.3835 850/1940 [============>.................] - ETA: 0s - loss: 25.35111050/1940 [===============>..............] - ETA: 0s - loss: 25.51781250/1940 [==================>...........] - ETA: 0s - loss: 25.64581450/1940 [=====================>........] - ETA: 0s - loss: 25.65841650/1940 [========================>.....] - ETA: 0s - loss: 25.69021850/1940 [===========================>..] - ETA: 0s - loss: 25.7232
Epoch 00026: val_loss did not improve from 22.94486
1940/1940 [==============================] - 1s 279us/sample - loss: 25.7175 - val_loss: 22.9603
Epoch 27/50
  50/1940 [..............................] - ETA: 0s - loss: 26.4746 250/1940 [==>...........................] - ETA: 0s - loss: 25.6776 450/1940 [=====>........................] - ETA: 0s - loss: 26.2111 650/1940 [=========>....................] - ETA: 0s - loss: 25.9331 850/1940 [============>.................] - ETA: 0s - loss: 25.76021050/1940 [===============>..............] - ETA: 0s - loss: 25.77651250/1940 [==================>...........] - ETA: 0s - loss: 25.82771450/1940 [=====================>........] - ETA: 0s - loss: 25.69881650/1940 [========================>.....] - ETA: 0s - loss: 25.73031850/1940 [===========================>..] - ETA: 0s - loss: 25.7083
Epoch 00027: val_loss did not improve from 22.94486

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1940/1940 [==============================] - 1s 279us/sample - loss: 25.7115 - val_loss: 22.9482
Epoch 28/50
  50/1940 [..............................] - ETA: 0s - loss: 28.3189 250/1940 [==>...........................] - ETA: 0s - loss: 26.2721 450/1940 [=====>........................] - ETA: 0s - loss: 26.0177 650/1940 [=========>....................] - ETA: 0s - loss: 25.8658 850/1940 [============>.................] - ETA: 0s - loss: 25.72001050/1940 [===============>..............] - ETA: 0s - loss: 25.69771250/1940 [==================>...........] - ETA: 0s - loss: 25.76401450/1940 [=====================>........] - ETA: 0s - loss: 25.71251650/1940 [========================>.....] - ETA: 0s - loss: 25.72701850/1940 [===========================>..] - ETA: 0s - loss: 25.6454
Epoch 00028: val_loss improved from 22.94486 to 22.93343, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 305us/sample - loss: 25.6314 - val_loss: 22.9334
Epoch 29/50
  50/1940 [..............................] - ETA: 0s - loss: 24.3701 250/1940 [==>...........................] - ETA: 0s - loss: 26.8914 450/1940 [=====>........................] - ETA: 0s - loss: 25.8173 650/1940 [=========>....................] - ETA: 0s - loss: 25.5960 850/1940 [============>.................] - ETA: 0s - loss: 25.80751050/1940 [===============>..............] - ETA: 0s - loss: 25.83871250/1940 [==================>...........] - ETA: 0s - loss: 25.73781450/1940 [=====================>........] - ETA: 0s - loss: 25.71921650/1940 [========================>.....] - ETA: 0s - loss: 25.58841850/1940 [===========================>..] - ETA: 0s - loss: 25.5810
Epoch 00029: val_loss improved from 22.93343 to 22.92482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 303us/sample - loss: 25.6069 - val_loss: 22.9248
Epoch 30/50
  50/1940 [..............................] - ETA: 0s - loss: 26.9900 250/1940 [==>...........................] - ETA: 0s - loss: 26.2453 450/1940 [=====>........................] - ETA: 0s - loss: 26.3146 650/1940 [=========>....................] - ETA: 0s - loss: 26.3085 850/1940 [============>.................] - ETA: 0s - loss: 26.16131050/1940 [===============>..............] - ETA: 0s - loss: 25.92301250/1940 [==================>...........] - ETA: 0s - loss: 25.75451450/1940 [=====================>........] - ETA: 0s - loss: 25.64031650/1940 [========================>.....] - ETA: 0s - loss: 25.60491850/1940 [===========================>..] - ETA: 0s - loss: 25.5342
Epoch 00030: val_loss did not improve from 22.92482
1940/1940 [==============================] - 1s 284us/sample - loss: 25.5885 - val_loss: 22.9259
Epoch 31/50
  50/1940 [..............................] - ETA: 0s - loss: 24.9680 250/1940 [==>...........................] - ETA: 0s - loss: 25.5714 450/1940 [=====>........................] - ETA: 0s - loss: 25.9865 650/1940 [=========>....................] - ETA: 0s - loss: 25.6889 850/1940 [============>.................] - ETA: 0s - loss: 25.44841050/1940 [===============>..............] - ETA: 0s - loss: 25.34311250/1940 [==================>...........] - ETA: 0s - loss: 25.34891450/1940 [=====================>........] - ETA: 0s - loss: 25.41221650/1940 [========================>.....] - ETA: 0s - loss: 25.45401850/1940 [===========================>..] - ETA: 0s - loss: 25.5085
Epoch 00031: val_loss improved from 22.92482 to 22.92327, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 299us/sample - loss: 25.5398 - val_loss: 22.9233
Epoch 32/50
  50/1940 [..............................] - ETA: 0s - loss: 23.6921 250/1940 [==>...........................] - ETA: 0s - loss: 25.2538 450/1940 [=====>........................] - ETA: 0s - loss: 25.6223 650/1940 [=========>....................] - ETA: 0s - loss: 25.5503 850/1940 [============>.................] - ETA: 0s - loss: 25.41951050/1940 [===============>..............] - ETA: 0s - loss: 25.57821250/1940 [==================>...........] - ETA: 0s - loss: 25.68551450/1940 [=====================>........] - ETA: 0s - loss: 25.66171650/1940 [========================>.....] - ETA: 0s - loss: 25.53221850/1940 [===========================>..] - ETA: 0s - loss: 25.5832
Epoch 00032: val_loss improved from 22.92327 to 22.91755, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 331us/sample - loss: 25.6198 - val_loss: 22.9175
Epoch 33/50
  50/1940 [..............................] - ETA: 0s - loss: 25.3060 250/1940 [==>...........................] - ETA: 0s - loss: 24.8740 450/1940 [=====>........................] - ETA: 0s - loss: 25.7389 650/1940 [=========>....................] - ETA: 0s - loss: 25.7719 850/1940 [============>.................] - ETA: 0s - loss: 26.04181050/1940 [===============>..............] - ETA: 0s - loss: 26.01991250/1940 [==================>...........] - ETA: 0s - loss: 25.88321450/1940 [=====================>........] - ETA: 0s - loss: 25.76821650/1940 [========================>.....] - ETA: 0s - loss: 25.64371850/1940 [===========================>..] - ETA: 0s - loss: 25.6828
Epoch 00033: val_loss improved from 22.91755 to 22.91753, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 294us/sample - loss: 25.6860 - val_loss: 22.9175
Epoch 34/50
  50/1940 [..............................] - ETA: 0s - loss: 25.5234 250/1940 [==>...........................] - ETA: 0s - loss: 25.6322 450/1940 [=====>........................] - ETA: 0s - loss: 25.8333 650/1940 [=========>....................] - ETA: 0s - loss: 25.8098 850/1940 [============>.................] - ETA: 0s - loss: 25.63151050/1940 [===============>..............] - ETA: 0s - loss: 25.65951250/1940 [==================>...........] - ETA: 0s - loss: 25.51901450/1940 [=====================>........] - ETA: 0s - loss: 25.54561650/1940 [========================>.....] - ETA: 0s - loss: 25.49831850/1940 [===========================>..] - ETA: 0s - loss: 25.5832
Epoch 00034: val_loss improved from 22.91753 to 22.91675, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 302us/sample - loss: 25.5499 - val_loss: 22.9167
Epoch 35/50
  50/1940 [..............................] - ETA: 0s - loss: 26.0415 250/1940 [==>...........................] - ETA: 0s - loss: 25.8623 450/1940 [=====>........................] - ETA: 0s - loss: 25.1873 650/1940 [=========>....................] - ETA: 0s - loss: 25.4906 850/1940 [============>.................] - ETA: 0s - loss: 25.49571050/1940 [===============>..............] - ETA: 0s - loss: 25.47381250/1940 [==================>...........] - ETA: 0s - loss: 25.56731450/1940 [=====================>........] - ETA: 0s - loss: 25.57941650/1940 [========================>.....] - ETA: 0s - loss: 25.56751850/1940 [===========================>..] - ETA: 0s - loss: 25.6371
Epoch 00035: val_loss improved from 22.91675 to 22.90844, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 299us/sample - loss: 25.5399 - val_loss: 22.9084
Epoch 36/50
  50/1940 [..............................] - ETA: 0s - loss: 24.8691 250/1940 [==>...........................] - ETA: 0s - loss: 25.4864 450/1940 [=====>........................] - ETA: 0s - loss: 25.7395 650/1940 [=========>....................] - ETA: 0s - loss: 25.4591 850/1940 [============>.................] - ETA: 0s - loss: 25.37131050/1940 [===============>..............] - ETA: 0s - loss: 25.30351250/1940 [==================>...........] - ETA: 0s - loss: 25.47791450/1940 [=====================>........] - ETA: 0s - loss: 25.62901650/1940 [========================>.....] - ETA: 0s - loss: 25.55421850/1940 [===========================>..] - ETA: 0s - loss: 25.6013
Epoch 00036: val_loss did not improve from 22.90844
1940/1940 [==============================] - 1s 281us/sample - loss: 25.6179 - val_loss: 22.9166
Epoch 37/50
  50/1940 [..............................] - ETA: 0s - loss: 26.6464 250/1940 [==>...........................] - ETA: 0s - loss: 25.7715 450/1940 [=====>........................] - ETA: 0s - loss: 25.6783 650/1940 [=========>....................] - ETA: 0s - loss: 25.6078 850/1940 [============>.................] - ETA: 0s - loss: 25.45231050/1940 [===============>..............] - ETA: 0s - loss: 25.50581250/1940 [==================>...........] - ETA: 0s - loss: 25.52171450/1940 [=====================>........] - ETA: 0s - loss: 25.52631650/1940 [========================>.....] - ETA: 0s - loss: 25.61331850/1940 [===========================>..] - ETA: 0s - loss: 25.5907
Epoch 00037: val_loss did not improve from 22.90844

Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-05.
1940/1940 [==============================] - 1s 281us/sample - loss: 25.5597 - val_loss: 22.9111
Epoch 38/50
  50/1940 [..............................] - ETA: 0s - loss: 27.0901 250/1940 [==>...........................] - ETA: 0s - loss: 25.8611 450/1940 [=====>........................] - ETA: 0s - loss: 25.7567 650/1940 [=========>....................] - ETA: 0s - loss: 25.6683 850/1940 [============>.................] - ETA: 0s - loss: 25.41071050/1940 [===============>..............] - ETA: 0s - loss: 25.57191250/1940 [==================>...........] - ETA: 0s - loss: 25.56441450/1940 [=====================>........] - ETA: 0s - loss: 25.62611650/1940 [========================>.....] - ETA: 0s - loss: 25.56731850/1940 [===========================>..] - ETA: 0s - loss: 25.5644
Epoch 00038: val_loss did not improve from 22.90844
1940/1940 [==============================] - 1s 281us/sample - loss: 25.5463 - val_loss: 22.9174
Epoch 39/50
  50/1940 [..............................] - ETA: 0s - loss: 27.8838 250/1940 [==>...........................] - ETA: 0s - loss: 25.6868 450/1940 [=====>........................] - ETA: 0s - loss: 25.7226 650/1940 [=========>....................] - ETA: 0s - loss: 25.7535 850/1940 [============>.................] - ETA: 0s - loss: 25.59951050/1940 [===============>..............] - ETA: 0s - loss: 25.59331250/1940 [==================>...........] - ETA: 0s - loss: 25.47771450/1940 [=====================>........] - ETA: 0s - loss: 25.48711650/1940 [========================>.....] - ETA: 0s - loss: 25.42771850/1940 [===========================>..] - ETA: 0s - loss: 25.5378
Epoch 00039: val_loss did not improve from 22.90844
1940/1940 [==============================] - 1s 281us/sample - loss: 25.5118 - val_loss: 22.9111
Epoch 40/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2014 250/1940 [==>...........................] - ETA: 0s - loss: 25.5168 450/1940 [=====>........................] - ETA: 0s - loss: 25.0512 650/1940 [=========>....................] - ETA: 0s - loss: 25.2164 850/1940 [============>.................] - ETA: 0s - loss: 25.34711050/1940 [===============>..............] - ETA: 0s - loss: 25.40891250/1940 [==================>...........] - ETA: 0s - loss: 25.48391450/1940 [=====================>........] - ETA: 0s - loss: 25.47861650/1940 [========================>.....] - ETA: 0s - loss: 25.51421850/1940 [===========================>..] - ETA: 0s - loss: 25.4984
Epoch 00040: val_loss improved from 22.90844 to 22.90609, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1940/1940 [==============================] - 1s 298us/sample - loss: 25.5186 - val_loss: 22.9061
Epoch 41/50
  50/1940 [..............................] - ETA: 0s - loss: 25.7677 250/1940 [==>...........................] - ETA: 0s - loss: 24.8498 450/1940 [=====>........................] - ETA: 0s - loss: 25.3302 650/1940 [=========>....................] - ETA: 0s - loss: 25.5070 850/1940 [============>.................] - ETA: 0s - loss: 25.58581050/1940 [===============>..............] - ETA: 0s - loss: 25.63961250/1940 [==================>...........] - ETA: 0s - loss: 25.60791450/1940 [=====================>........] - ETA: 0s - loss: 25.53461650/1940 [========================>.....] - ETA: 0s - loss: 25.65311850/1940 [===========================>..] - ETA: 0s - loss: 25.6035
Epoch 00041: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 282us/sample - loss: 25.5583 - val_loss: 22.9065
Epoch 42/50
  50/1940 [..............................] - ETA: 0s - loss: 27.0763 250/1940 [==>...........................] - ETA: 0s - loss: 26.8908 450/1940 [=====>........................] - ETA: 0s - loss: 25.9549 650/1940 [=========>....................] - ETA: 0s - loss: 25.9672 750/1940 [==========>...................] - ETA: 0s - loss: 25.8988 900/1940 [============>.................] - ETA: 0s - loss: 25.71171100/1940 [================>.............] - ETA: 0s - loss: 25.74481300/1940 [===================>..........] - ETA: 0s - loss: 25.68151500/1940 [======================>.......] - ETA: 0s - loss: 25.79611700/1940 [=========================>....] - ETA: 0s - loss: 25.72081900/1940 [============================>.] - ETA: 0s - loss: 25.6504
Epoch 00042: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 305us/sample - loss: 25.6621 - val_loss: 22.9137
Epoch 43/50
  50/1940 [..............................] - ETA: 0s - loss: 26.2551 250/1940 [==>...........................] - ETA: 0s - loss: 25.9978 450/1940 [=====>........................] - ETA: 0s - loss: 25.7723 650/1940 [=========>....................] - ETA: 0s - loss: 25.3428 850/1940 [============>.................] - ETA: 0s - loss: 25.24551050/1940 [===============>..............] - ETA: 0s - loss: 25.31361250/1940 [==================>...........] - ETA: 0s - loss: 25.24661500/1940 [======================>.......] - ETA: 0s - loss: 25.30871700/1940 [=========================>....] - ETA: 0s - loss: 25.4436
Epoch 00043: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 272us/sample - loss: 25.4713 - val_loss: 22.9127
Epoch 44/50
  50/1940 [..............................] - ETA: 0s - loss: 25.1596 250/1940 [==>...........................] - ETA: 0s - loss: 25.7236 450/1940 [=====>........................] - ETA: 0s - loss: 25.8584 650/1940 [=========>....................] - ETA: 0s - loss: 25.6362 850/1940 [============>.................] - ETA: 0s - loss: 25.47781050/1940 [===============>..............] - ETA: 0s - loss: 25.59201250/1940 [==================>...........] - ETA: 0s - loss: 25.55401450/1940 [=====================>........] - ETA: 0s - loss: 25.51441650/1940 [========================>.....] - ETA: 0s - loss: 25.60591850/1940 [===========================>..] - ETA: 0s - loss: 25.5710
Epoch 00044: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 278us/sample - loss: 25.5762 - val_loss: 22.9077
Epoch 45/50
  50/1940 [..............................] - ETA: 0s - loss: 24.9348 250/1940 [==>...........................] - ETA: 0s - loss: 25.6915 450/1940 [=====>........................] - ETA: 0s - loss: 25.5493 650/1940 [=========>....................] - ETA: 0s - loss: 25.7215 850/1940 [============>.................] - ETA: 0s - loss: 25.39981050/1940 [===============>..............] - ETA: 0s - loss: 25.44581250/1940 [==================>...........] - ETA: 0s - loss: 25.57721450/1940 [=====================>........] - ETA: 0s - loss: 25.53201650/1940 [========================>.....] - ETA: 0s - loss: 25.61691850/1940 [===========================>..] - ETA: 0s - loss: 25.63941900/1940 [============================>.] - ETA: 0s - loss: 25.6130
Epoch 00045: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 327us/sample - loss: 25.6130 - val_loss: 22.9090
Epoch 46/50
  50/1940 [..............................] - ETA: 0s - loss: 26.6871 250/1940 [==>...........................] - ETA: 0s - loss: 26.8936 450/1940 [=====>........................] - ETA: 0s - loss: 26.5025 650/1940 [=========>....................] - ETA: 0s - loss: 26.0611 850/1940 [============>.................] - ETA: 0s - loss: 25.78801050/1940 [===============>..............] - ETA: 0s - loss: 25.69821250/1940 [==================>...........] - ETA: 0s - loss: 25.53951450/1940 [=====================>........] - ETA: 0s - loss: 25.56601650/1940 [========================>.....] - ETA: 0s - loss: 25.62281850/1940 [===========================>..] - ETA: 0s - loss: 25.5107
Epoch 00046: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 291us/sample - loss: 25.5387 - val_loss: 22.9124
Epoch 47/50
  50/1940 [..............................] - ETA: 0s - loss: 24.8818 250/1940 [==>...........................] - ETA: 0s - loss: 25.8940 450/1940 [=====>........................] - ETA: 0s - loss: 25.0378 650/1940 [=========>....................] - ETA: 0s - loss: 25.4086 850/1940 [============>.................] - ETA: 0s - loss: 25.60131050/1940 [===============>..............] - ETA: 0s - loss: 25.76201250/1940 [==================>...........] - ETA: 0s - loss: 25.73061450/1940 [=====================>........] - ETA: 0s - loss: 25.70581650/1940 [========================>.....] - ETA: 0s - loss: 25.70181850/1940 [===========================>..] - ETA: 0s - loss: 25.6386
Epoch 00047: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 288us/sample - loss: 25.5876 - val_loss: 22.9079
Epoch 48/50
  50/1940 [..............................] - ETA: 0s - loss: 25.8748 250/1940 [==>...........................] - ETA: 0s - loss: 26.5045 450/1940 [=====>........................] - ETA: 0s - loss: 25.9223 650/1940 [=========>....................] - ETA: 0s - loss: 25.8965 850/1940 [============>.................] - ETA: 0s - loss: 25.77081050/1940 [===============>..............] - ETA: 0s - loss: 25.91431250/1940 [==================>...........] - ETA: 0s - loss: 25.87011450/1940 [=====================>........] - ETA: 0s - loss: 25.72991650/1940 [========================>.....] - ETA: 0s - loss: 25.63591850/1940 [===========================>..] - ETA: 0s - loss: 25.6604
Epoch 00048: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 283us/sample - loss: 25.6797 - val_loss: 22.9120
Epoch 49/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2756 250/1940 [==>...........................] - ETA: 0s - loss: 25.8518 450/1940 [=====>........................] - ETA: 0s - loss: 25.6185 650/1940 [=========>....................] - ETA: 0s - loss: 25.7558 850/1940 [============>.................] - ETA: 0s - loss: 25.74211050/1940 [===============>..............] - ETA: 0s - loss: 25.68571250/1940 [==================>...........] - ETA: 0s - loss: 25.78651450/1940 [=====================>........] - ETA: 0s - loss: 25.72361650/1940 [========================>.....] - ETA: 0s - loss: 25.69071850/1940 [===========================>..] - ETA: 0s - loss: 25.6873
Epoch 00049: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 277us/sample - loss: 25.6272 - val_loss: 22.9106
Epoch 50/50
  50/1940 [..............................] - ETA: 0s - loss: 23.8297 250/1940 [==>...........................] - ETA: 0s - loss: 25.9419 450/1940 [=====>........................] - ETA: 0s - loss: 25.6626 650/1940 [=========>....................] - ETA: 0s - loss: 25.9992 850/1940 [============>.................] - ETA: 0s - loss: 25.83361050/1940 [===============>..............] - ETA: 0s - loss: 25.66391250/1940 [==================>...........] - ETA: 0s - loss: 25.58201450/1940 [=====================>........] - ETA: 0s - loss: 25.58601650/1940 [========================>.....] - ETA: 0s - loss: 25.61231850/1940 [===========================>..] - ETA: 0s - loss: 25.6105
Epoch 00050: val_loss did not improve from 22.90609
1940/1940 [==============================] - 1s 281us/sample - loss: 25.6230 - val_loss: 22.9093
Epoch 00050: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 1940 samples, validate on 121 samples
Epoch 1/50
  50/1940 [..............................] - ETA: 4:02 - loss: 157.0019 200/1940 [==>...........................] - ETA: 56s - loss: 152.8291  400/1940 [=====>........................] - ETA: 25s - loss: 153.9339 600/1940 [========>.....................] - ETA: 14s - loss: 146.0906 800/1940 [===========>..................] - ETA: 9s - loss: 127.0658 1000/1940 [==============>...............] - ETA: 6s - loss: 113.96131200/1940 [=================>............] - ETA: 4s - loss: 102.53471400/1940 [====================>.........] - ETA: 2s - loss: 95.2059 1600/1940 [=======================>......] - ETA: 1s - loss: 88.81541800/1940 [==========================>...] - ETA: 0s - loss: 83.5056
Epoch 00001: val_loss improved from inf to 75.31132, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 7s 4ms/sample - loss: 80.4509 - val_loss: 75.3113
Epoch 2/50
  50/1940 [..............................] - ETA: 0s - loss: 39.1199 250/1940 [==>...........................] - ETA: 0s - loss: 38.5197 450/1940 [=====>........................] - ETA: 0s - loss: 38.0498 650/1940 [=========>....................] - ETA: 0s - loss: 37.0001 850/1940 [============>.................] - ETA: 0s - loss: 36.40081050/1940 [===============>..............] - ETA: 0s - loss: 35.58321250/1940 [==================>...........] - ETA: 0s - loss: 35.34121450/1940 [=====================>........] - ETA: 0s - loss: 34.90691650/1940 [========================>.....] - ETA: 0s - loss: 34.41351850/1940 [===========================>..] - ETA: 0s - loss: 34.1304
Epoch 00002: val_loss did not improve from 75.31132
1940/1940 [==============================] - 1s 279us/sample - loss: 33.9291 - val_loss: 77.9618
Epoch 3/50
  50/1940 [..............................] - ETA: 0s - loss: 30.4841 250/1940 [==>...........................] - ETA: 0s - loss: 30.4041 450/1940 [=====>........................] - ETA: 0s - loss: 29.8890 650/1940 [=========>....................] - ETA: 0s - loss: 29.8396 850/1940 [============>.................] - ETA: 0s - loss: 29.86961050/1940 [===============>..............] - ETA: 0s - loss: 29.85721250/1940 [==================>...........] - ETA: 0s - loss: 29.63721450/1940 [=====================>........] - ETA: 0s - loss: 29.56601650/1940 [========================>.....] - ETA: 0s - loss: 29.39791850/1940 [===========================>..] - ETA: 0s - loss: 29.3057
Epoch 00003: val_loss improved from 75.31132 to 57.08426, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 293us/sample - loss: 29.2602 - val_loss: 57.0843
Epoch 4/50
  50/1940 [..............................] - ETA: 0s - loss: 28.3359 250/1940 [==>...........................] - ETA: 0s - loss: 29.1448 450/1940 [=====>........................] - ETA: 0s - loss: 28.8720 650/1940 [=========>....................] - ETA: 0s - loss: 28.6597 850/1940 [============>.................] - ETA: 0s - loss: 28.67211050/1940 [===============>..............] - ETA: 0s - loss: 28.59801250/1940 [==================>...........] - ETA: 0s - loss: 28.42551450/1940 [=====================>........] - ETA: 0s - loss: 28.42481650/1940 [========================>.....] - ETA: 0s - loss: 28.37161850/1940 [===========================>..] - ETA: 0s - loss: 28.2008
Epoch 00004: val_loss improved from 57.08426 to 42.73192, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 295us/sample - loss: 28.2276 - val_loss: 42.7319
Epoch 5/50
  50/1940 [..............................] - ETA: 0s - loss: 28.1037 250/1940 [==>...........................] - ETA: 0s - loss: 27.6363 450/1940 [=====>........................] - ETA: 0s - loss: 27.9187 650/1940 [=========>....................] - ETA: 0s - loss: 27.9537 850/1940 [============>.................] - ETA: 0s - loss: 28.04221050/1940 [===============>..............] - ETA: 0s - loss: 28.01211250/1940 [==================>...........] - ETA: 0s - loss: 27.90921450/1940 [=====================>........] - ETA: 0s - loss: 27.74381650/1940 [========================>.....] - ETA: 0s - loss: 27.59461850/1940 [===========================>..] - ETA: 0s - loss: 27.5986
Epoch 00005: val_loss improved from 42.73192 to 31.06321, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 296us/sample - loss: 27.5820 - val_loss: 31.0632
Epoch 6/50
  50/1940 [..............................] - ETA: 0s - loss: 28.1358 250/1940 [==>...........................] - ETA: 0s - loss: 27.5450 450/1940 [=====>........................] - ETA: 0s - loss: 27.7718 650/1940 [=========>....................] - ETA: 0s - loss: 27.5253 850/1940 [============>.................] - ETA: 0s - loss: 27.67791050/1940 [===============>..............] - ETA: 0s - loss: 27.45231250/1940 [==================>...........] - ETA: 0s - loss: 27.29061450/1940 [=====================>........] - ETA: 0s - loss: 27.13701650/1940 [========================>.....] - ETA: 0s - loss: 27.21001850/1940 [===========================>..] - ETA: 0s - loss: 27.1087
Epoch 00006: val_loss improved from 31.06321 to 29.94172, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 296us/sample - loss: 27.0939 - val_loss: 29.9417
Epoch 7/50
  50/1940 [..............................] - ETA: 0s - loss: 25.9156 250/1940 [==>...........................] - ETA: 0s - loss: 26.8863 450/1940 [=====>........................] - ETA: 0s - loss: 27.0177 650/1940 [=========>....................] - ETA: 0s - loss: 27.1012 850/1940 [============>.................] - ETA: 0s - loss: 26.78311050/1940 [===============>..............] - ETA: 0s - loss: 26.79761250/1940 [==================>...........] - ETA: 0s - loss: 26.71011450/1940 [=====================>........] - ETA: 0s - loss: 26.65501650/1940 [========================>.....] - ETA: 0s - loss: 26.74221850/1940 [===========================>..] - ETA: 0s - loss: 26.7793
Epoch 00007: val_loss improved from 29.94172 to 27.70034, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 300us/sample - loss: 26.7640 - val_loss: 27.7003
Epoch 8/50
  50/1940 [..............................] - ETA: 0s - loss: 26.2623 250/1940 [==>...........................] - ETA: 0s - loss: 25.6490 450/1940 [=====>........................] - ETA: 0s - loss: 26.4556 650/1940 [=========>....................] - ETA: 0s - loss: 26.7868 850/1940 [============>.................] - ETA: 0s - loss: 27.08291050/1940 [===============>..............] - ETA: 0s - loss: 27.26311250/1940 [==================>...........] - ETA: 0s - loss: 27.31361450/1940 [=====================>........] - ETA: 0s - loss: 27.42821650/1940 [========================>.....] - ETA: 0s - loss: 27.38081850/1940 [===========================>..] - ETA: 0s - loss: 27.4742
Epoch 00008: val_loss improved from 27.70034 to 25.40410, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 300us/sample - loss: 27.4970 - val_loss: 25.4041
Epoch 9/50
  50/1940 [..............................] - ETA: 0s - loss: 26.1382 250/1940 [==>...........................] - ETA: 0s - loss: 27.2073 450/1940 [=====>........................] - ETA: 0s - loss: 27.3833 650/1940 [=========>....................] - ETA: 0s - loss: 27.3370 850/1940 [============>.................] - ETA: 0s - loss: 27.17911050/1940 [===============>..............] - ETA: 0s - loss: 27.10901250/1940 [==================>...........] - ETA: 0s - loss: 27.05571450/1940 [=====================>........] - ETA: 0s - loss: 27.10821650/1940 [========================>.....] - ETA: 0s - loss: 27.00881850/1940 [===========================>..] - ETA: 0s - loss: 27.1174
Epoch 00009: val_loss improved from 25.40410 to 24.37156, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 299us/sample - loss: 27.0617 - val_loss: 24.3716
Epoch 10/50
  50/1940 [..............................] - ETA: 0s - loss: 27.6063 250/1940 [==>...........................] - ETA: 0s - loss: 26.9559 450/1940 [=====>........................] - ETA: 0s - loss: 27.0410 650/1940 [=========>....................] - ETA: 0s - loss: 26.6968 850/1940 [============>.................] - ETA: 0s - loss: 26.76761050/1940 [===============>..............] - ETA: 0s - loss: 26.67881250/1940 [==================>...........] - ETA: 0s - loss: 26.64991400/1940 [====================>.........] - ETA: 0s - loss: 26.77601550/1940 [======================>.......] - ETA: 0s - loss: 26.83851750/1940 [==========================>...] - ETA: 0s - loss: 26.9651
Epoch 00010: val_loss did not improve from 24.37156
1940/1940 [==============================] - 1s 302us/sample - loss: 27.0254 - val_loss: 24.5548
Epoch 11/50
  50/1940 [..............................] - ETA: 0s - loss: 26.2072 250/1940 [==>...........................] - ETA: 0s - loss: 27.1998 450/1940 [=====>........................] - ETA: 0s - loss: 26.7242 650/1940 [=========>....................] - ETA: 0s - loss: 26.7383 850/1940 [============>.................] - ETA: 0s - loss: 26.52201050/1940 [===============>..............] - ETA: 0s - loss: 26.54321250/1940 [==================>...........] - ETA: 0s - loss: 26.34431450/1940 [=====================>........] - ETA: 0s - loss: 26.32311650/1940 [========================>.....] - ETA: 0s - loss: 26.51241850/1940 [===========================>..] - ETA: 0s - loss: 26.6826
Epoch 00011: val_loss did not improve from 24.37156

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1940/1940 [==============================] - 1s 277us/sample - loss: 26.6418 - val_loss: 24.4711
Epoch 12/50
  50/1940 [..............................] - ETA: 0s - loss: 25.6898 250/1940 [==>...........................] - ETA: 0s - loss: 25.5386 450/1940 [=====>........................] - ETA: 0s - loss: 25.6445 650/1940 [=========>....................] - ETA: 0s - loss: 25.7370 850/1940 [============>.................] - ETA: 0s - loss: 25.78051050/1940 [===============>..............] - ETA: 0s - loss: 25.75121250/1940 [==================>...........] - ETA: 0s - loss: 25.96681450/1940 [=====================>........] - ETA: 0s - loss: 26.07691600/1940 [=======================>......] - ETA: 0s - loss: 26.08431800/1940 [==========================>...] - ETA: 0s - loss: 26.0043
Epoch 00012: val_loss improved from 24.37156 to 23.69832, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 300us/sample - loss: 26.0812 - val_loss: 23.6983
Epoch 13/50
  50/1940 [..............................] - ETA: 0s - loss: 25.6531 250/1940 [==>...........................] - ETA: 0s - loss: 25.4595 450/1940 [=====>........................] - ETA: 0s - loss: 25.5653 650/1940 [=========>....................] - ETA: 0s - loss: 25.7742 850/1940 [============>.................] - ETA: 0s - loss: 25.85341050/1940 [===============>..............] - ETA: 0s - loss: 25.99401250/1940 [==================>...........] - ETA: 0s - loss: 26.05091450/1940 [=====================>........] - ETA: 0s - loss: 26.14901650/1940 [========================>.....] - ETA: 0s - loss: 26.08201850/1940 [===========================>..] - ETA: 0s - loss: 26.0998
Epoch 00013: val_loss improved from 23.69832 to 23.07797, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 298us/sample - loss: 26.0730 - val_loss: 23.0780
Epoch 14/50
  50/1940 [..............................] - ETA: 0s - loss: 25.6989 250/1940 [==>...........................] - ETA: 0s - loss: 25.3627 450/1940 [=====>........................] - ETA: 0s - loss: 25.8133 650/1940 [=========>....................] - ETA: 0s - loss: 25.9179 850/1940 [============>.................] - ETA: 0s - loss: 25.89071050/1940 [===============>..............] - ETA: 0s - loss: 25.80821250/1940 [==================>...........] - ETA: 0s - loss: 25.87731450/1940 [=====================>........] - ETA: 0s - loss: 25.97331650/1940 [========================>.....] - ETA: 0s - loss: 25.99371850/1940 [===========================>..] - ETA: 0s - loss: 25.9828
Epoch 00014: val_loss improved from 23.07797 to 23.05867, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 298us/sample - loss: 26.0384 - val_loss: 23.0587
Epoch 15/50
  50/1940 [..............................] - ETA: 0s - loss: 25.7996 250/1940 [==>...........................] - ETA: 0s - loss: 25.9479 400/1940 [=====>........................] - ETA: 0s - loss: 25.8309 600/1940 [========>.....................] - ETA: 0s - loss: 25.9896 800/1940 [===========>..................] - ETA: 0s - loss: 26.07411000/1940 [==============>...............] - ETA: 0s - loss: 25.83831200/1940 [=================>............] - ETA: 0s - loss: 26.01111400/1940 [====================>.........] - ETA: 0s - loss: 26.22561600/1940 [=======================>......] - ETA: 0s - loss: 26.15131800/1940 [==========================>...] - ETA: 0s - loss: 26.2259
Epoch 00015: val_loss did not improve from 23.05867
1940/1940 [==============================] - 1s 293us/sample - loss: 26.2351 - val_loss: 23.1047
Epoch 16/50
  50/1940 [..............................] - ETA: 0s - loss: 26.8197 250/1940 [==>...........................] - ETA: 0s - loss: 26.0704 450/1940 [=====>........................] - ETA: 0s - loss: 25.9865 650/1940 [=========>....................] - ETA: 0s - loss: 26.0560 850/1940 [============>.................] - ETA: 0s - loss: 25.98171050/1940 [===============>..............] - ETA: 0s - loss: 26.13821250/1940 [==================>...........] - ETA: 0s - loss: 26.23401450/1940 [=====================>........] - ETA: 0s - loss: 26.27951650/1940 [========================>.....] - ETA: 0s - loss: 26.29471850/1940 [===========================>..] - ETA: 0s - loss: 26.2111
Epoch 00016: val_loss improved from 23.05867 to 23.04585, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 301us/sample - loss: 26.2199 - val_loss: 23.0458
Epoch 17/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2564 250/1940 [==>...........................] - ETA: 0s - loss: 25.3069 450/1940 [=====>........................] - ETA: 0s - loss: 25.9934 650/1940 [=========>....................] - ETA: 0s - loss: 25.9973 850/1940 [============>.................] - ETA: 0s - loss: 25.99111050/1940 [===============>..............] - ETA: 0s - loss: 26.09401250/1940 [==================>...........] - ETA: 0s - loss: 26.15161450/1940 [=====================>........] - ETA: 0s - loss: 26.29201650/1940 [========================>.....] - ETA: 0s - loss: 26.31711850/1940 [===========================>..] - ETA: 0s - loss: 26.3111
Epoch 00017: val_loss did not improve from 23.04585
1940/1940 [==============================] - 1s 280us/sample - loss: 26.2218 - val_loss: 24.3405
Epoch 18/50
  50/1940 [..............................] - ETA: 0s - loss: 24.7425 250/1940 [==>...........................] - ETA: 0s - loss: 27.2641 450/1940 [=====>........................] - ETA: 0s - loss: 27.1139 650/1940 [=========>....................] - ETA: 0s - loss: 26.6985 850/1940 [============>.................] - ETA: 0s - loss: 26.51291050/1940 [===============>..............] - ETA: 0s - loss: 26.48971250/1940 [==================>...........] - ETA: 0s - loss: 26.38161450/1940 [=====================>........] - ETA: 0s - loss: 26.49261650/1940 [========================>.....] - ETA: 0s - loss: 26.50421850/1940 [===========================>..] - ETA: 0s - loss: 26.3668
Epoch 00018: val_loss did not improve from 23.04585

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1940/1940 [==============================] - 1s 281us/sample - loss: 26.3621 - val_loss: 23.6489
Epoch 19/50
  50/1940 [..............................] - ETA: 0s - loss: 25.2151 250/1940 [==>...........................] - ETA: 0s - loss: 25.5960 450/1940 [=====>........................] - ETA: 0s - loss: 25.6429 650/1940 [=========>....................] - ETA: 0s - loss: 25.8246 850/1940 [============>.................] - ETA: 0s - loss: 25.79411050/1940 [===============>..............] - ETA: 0s - loss: 25.69931250/1940 [==================>...........] - ETA: 0s - loss: 25.68451450/1940 [=====================>........] - ETA: 0s - loss: 25.84191650/1940 [========================>.....] - ETA: 0s - loss: 25.90721850/1940 [===========================>..] - ETA: 0s - loss: 25.9084
Epoch 00019: val_loss improved from 23.04585 to 22.86505, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 300us/sample - loss: 25.8880 - val_loss: 22.8650
Epoch 20/50
  50/1940 [..............................] - ETA: 0s - loss: 29.1364 250/1940 [==>...........................] - ETA: 0s - loss: 26.9695 450/1940 [=====>........................] - ETA: 0s - loss: 26.6302 650/1940 [=========>....................] - ETA: 0s - loss: 26.2620 850/1940 [============>.................] - ETA: 0s - loss: 26.21341050/1940 [===============>..............] - ETA: 0s - loss: 25.95551250/1940 [==================>...........] - ETA: 0s - loss: 26.05041450/1940 [=====================>........] - ETA: 0s - loss: 26.02531650/1940 [========================>.....] - ETA: 0s - loss: 26.04011850/1940 [===========================>..] - ETA: 0s - loss: 26.0124
Epoch 00020: val_loss improved from 22.86505 to 22.86481, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 295us/sample - loss: 26.0202 - val_loss: 22.8648
Epoch 21/50
  50/1940 [..............................] - ETA: 0s - loss: 24.8811 250/1940 [==>...........................] - ETA: 0s - loss: 25.2315 450/1940 [=====>........................] - ETA: 0s - loss: 25.6198 650/1940 [=========>....................] - ETA: 0s - loss: 25.8085 850/1940 [============>.................] - ETA: 0s - loss: 25.93051050/1940 [===============>..............] - ETA: 0s - loss: 25.88051250/1940 [==================>...........] - ETA: 0s - loss: 25.98931450/1940 [=====================>........] - ETA: 0s - loss: 25.91151650/1940 [========================>.....] - ETA: 0s - loss: 25.98241850/1940 [===========================>..] - ETA: 0s - loss: 25.9414
Epoch 00021: val_loss did not improve from 22.86481
1940/1940 [==============================] - 1s 283us/sample - loss: 25.8953 - val_loss: 23.2815
Epoch 22/50
  50/1940 [..............................] - ETA: 0s - loss: 24.6439 250/1940 [==>...........................] - ETA: 0s - loss: 25.4371 450/1940 [=====>........................] - ETA: 0s - loss: 25.5766 650/1940 [=========>....................] - ETA: 0s - loss: 25.3847 850/1940 [============>.................] - ETA: 0s - loss: 25.65631050/1940 [===============>..............] - ETA: 0s - loss: 25.80181250/1940 [==================>...........] - ETA: 0s - loss: 25.87721450/1940 [=====================>........] - ETA: 0s - loss: 25.78471650/1940 [========================>.....] - ETA: 0s - loss: 25.72821850/1940 [===========================>..] - ETA: 0s - loss: 25.7999
Epoch 00022: val_loss did not improve from 22.86481

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1940/1940 [==============================] - 1s 278us/sample - loss: 25.8555 - val_loss: 23.0512
Epoch 23/50
  50/1940 [..............................] - ETA: 0s - loss: 23.8994 250/1940 [==>...........................] - ETA: 0s - loss: 25.7911 450/1940 [=====>........................] - ETA: 0s - loss: 25.4346 650/1940 [=========>....................] - ETA: 0s - loss: 25.6017 850/1940 [============>.................] - ETA: 0s - loss: 25.75641050/1940 [===============>..............] - ETA: 0s - loss: 25.72061250/1940 [==================>...........] - ETA: 0s - loss: 25.88481450/1940 [=====================>........] - ETA: 0s - loss: 25.76141650/1940 [========================>.....] - ETA: 0s - loss: 25.79651850/1940 [===========================>..] - ETA: 0s - loss: 25.7954
Epoch 00023: val_loss did not improve from 22.86481
1940/1940 [==============================] - 1s 284us/sample - loss: 25.9280 - val_loss: 22.9180
Epoch 24/50
  50/1940 [..............................] - ETA: 0s - loss: 24.1916 250/1940 [==>...........................] - ETA: 0s - loss: 25.2082 450/1940 [=====>........................] - ETA: 0s - loss: 25.8063 650/1940 [=========>....................] - ETA: 0s - loss: 25.9350 850/1940 [============>.................] - ETA: 0s - loss: 25.82961050/1940 [===============>..............] - ETA: 0s - loss: 25.87081250/1940 [==================>...........] - ETA: 0s - loss: 25.87311450/1940 [=====================>........] - ETA: 0s - loss: 25.81731650/1940 [========================>.....] - ETA: 0s - loss: 25.77391850/1940 [===========================>..] - ETA: 0s - loss: 25.8039
Epoch 00024: val_loss did not improve from 22.86481

Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1940/1940 [==============================] - 1s 283us/sample - loss: 25.8427 - val_loss: 22.9943
Epoch 25/50
  50/1940 [..............................] - ETA: 0s - loss: 26.5086 250/1940 [==>...........................] - ETA: 0s - loss: 25.3497 450/1940 [=====>........................] - ETA: 0s - loss: 25.3517 650/1940 [=========>....................] - ETA: 0s - loss: 25.8055 850/1940 [============>.................] - ETA: 0s - loss: 25.90571000/1940 [==============>...............] - ETA: 0s - loss: 25.84371200/1940 [=================>............] - ETA: 0s - loss: 25.85311400/1940 [====================>.........] - ETA: 0s - loss: 25.72901600/1940 [=======================>......] - ETA: 0s - loss: 25.84891800/1940 [==========================>...] - ETA: 0s - loss: 25.8939
Epoch 00025: val_loss improved from 22.86481 to 22.82655, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1940/1940 [==============================] - 1s 308us/sample - loss: 25.9307 - val_loss: 22.8265
Epoch 26/50
  50/1940 [..............................] - ETA: 0s - loss: 26.4726 250/1940 [==>...........................] - ETA: 0s - loss: 25.5875 450/1940 [=====>........................] - ETA: 0s - loss: 25.4621 650/1940 [=========>....................] - ETA: 0s - loss: 25.7124 850/1940 [============>.................] - ETA: 0s - loss: 25.71791050/1940 [===============>..............] - ETA: 0s - loss: 25.72511250/1940 [==================>...........] - ETA: 0s - loss: 25.67631450/1940 [=====================>........] - ETA: 0s - loss: 25.74231650/1940 [========================>.....] - ETA: 0s - loss: 25.70651850/1940 [===========================>..] - ETA: 0s - loss: 25.7228
Epoch 00026: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 281us/sample - loss: 25.7635 - val_loss: 22.8687
Epoch 27/50
  50/1940 [..............................] - ETA: 0s - loss: 25.4486 250/1940 [==>...........................] - ETA: 0s - loss: 26.6023 450/1940 [=====>........................] - ETA: 0s - loss: 26.0135 650/1940 [=========>....................] - ETA: 0s - loss: 26.0095 850/1940 [============>.................] - ETA: 0s - loss: 26.11961050/1940 [===============>..............] - ETA: 0s - loss: 26.06541250/1940 [==================>...........] - ETA: 0s - loss: 25.95811450/1940 [=====================>........] - ETA: 0s - loss: 25.90201650/1940 [========================>.....] - ETA: 0s - loss: 25.89411850/1940 [===========================>..] - ETA: 0s - loss: 25.8689
Epoch 00027: val_loss did not improve from 22.82655

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1940/1940 [==============================] - 1s 283us/sample - loss: 25.8155 - val_loss: 22.8576
Epoch 28/50
  50/1940 [..............................] - ETA: 0s - loss: 25.1865 250/1940 [==>...........................] - ETA: 0s - loss: 26.2637 450/1940 [=====>........................] - ETA: 0s - loss: 25.9743 650/1940 [=========>....................] - ETA: 0s - loss: 25.8243 800/1940 [===========>..................] - ETA: 0s - loss: 25.5500 900/1940 [============>.................] - ETA: 0s - loss: 25.47581100/1940 [================>.............] - ETA: 0s - loss: 25.51821300/1940 [===================>..........] - ETA: 0s - loss: 25.57061500/1940 [======================>.......] - ETA: 0s - loss: 25.72491700/1940 [=========================>....] - ETA: 0s - loss: 25.86771900/1940 [============================>.] - ETA: 0s - loss: 25.8846
Epoch 00028: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 298us/sample - loss: 25.8604 - val_loss: 22.8574
Epoch 29/50
  50/1940 [..............................] - ETA: 0s - loss: 27.9969 250/1940 [==>...........................] - ETA: 0s - loss: 26.1806 450/1940 [=====>........................] - ETA: 0s - loss: 25.8398 650/1940 [=========>....................] - ETA: 0s - loss: 25.7894 850/1940 [============>.................] - ETA: 0s - loss: 25.77441050/1940 [===============>..............] - ETA: 0s - loss: 25.93531250/1940 [==================>...........] - ETA: 0s - loss: 25.94881450/1940 [=====================>........] - ETA: 0s - loss: 26.04701650/1940 [========================>.....] - ETA: 0s - loss: 25.93791850/1940 [===========================>..] - ETA: 0s - loss: 25.9526
Epoch 00029: val_loss did not improve from 22.82655

Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-05.
1940/1940 [==============================] - 1s 286us/sample - loss: 25.9033 - val_loss: 22.8558
Epoch 30/50
  50/1940 [..............................] - ETA: 0s - loss: 25.9495 250/1940 [==>...........................] - ETA: 0s - loss: 26.1471 450/1940 [=====>........................] - ETA: 0s - loss: 26.1867 650/1940 [=========>....................] - ETA: 0s - loss: 25.9013 850/1940 [============>.................] - ETA: 0s - loss: 25.92441050/1940 [===============>..............] - ETA: 0s - loss: 25.75811250/1940 [==================>...........] - ETA: 0s - loss: 25.81071450/1940 [=====================>........] - ETA: 0s - loss: 25.74531650/1940 [========================>.....] - ETA: 0s - loss: 25.75721850/1940 [===========================>..] - ETA: 0s - loss: 25.8093
Epoch 00030: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 288us/sample - loss: 25.7695 - val_loss: 22.8447
Epoch 31/50
  50/1940 [..............................] - ETA: 0s - loss: 25.5340 250/1940 [==>...........................] - ETA: 0s - loss: 25.8705 450/1940 [=====>........................] - ETA: 0s - loss: 25.7081 650/1940 [=========>....................] - ETA: 0s - loss: 25.9542 850/1940 [============>.................] - ETA: 0s - loss: 26.00511050/1940 [===============>..............] - ETA: 0s - loss: 26.04241250/1940 [==================>...........] - ETA: 0s - loss: 25.88311450/1940 [=====================>........] - ETA: 0s - loss: 25.76701650/1940 [========================>.....] - ETA: 0s - loss: 25.82141850/1940 [===========================>..] - ETA: 0s - loss: 25.8235
Epoch 00031: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 293us/sample - loss: 25.7911 - val_loss: 22.8518
Epoch 32/50
  50/1940 [..............................] - ETA: 0s - loss: 25.9313 250/1940 [==>...........................] - ETA: 0s - loss: 26.0937 450/1940 [=====>........................] - ETA: 0s - loss: 25.9165 650/1940 [=========>....................] - ETA: 0s - loss: 25.9652 850/1940 [============>.................] - ETA: 0s - loss: 26.02361050/1940 [===============>..............] - ETA: 0s - loss: 25.97291250/1940 [==================>...........] - ETA: 0s - loss: 25.92631450/1940 [=====================>........] - ETA: 0s - loss: 25.88181650/1940 [========================>.....] - ETA: 0s - loss: 25.81241850/1940 [===========================>..] - ETA: 0s - loss: 25.8084
Epoch 00032: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 289us/sample - loss: 25.7818 - val_loss: 22.8551
Epoch 33/50
  50/1940 [..............................] - ETA: 0s - loss: 26.8735 250/1940 [==>...........................] - ETA: 0s - loss: 25.6790 450/1940 [=====>........................] - ETA: 0s - loss: 25.7585 650/1940 [=========>....................] - ETA: 0s - loss: 25.7016 850/1940 [============>.................] - ETA: 0s - loss: 25.53281050/1940 [===============>..............] - ETA: 0s - loss: 25.57591250/1940 [==================>...........] - ETA: 0s - loss: 25.74761450/1940 [=====================>........] - ETA: 0s - loss: 25.69941650/1940 [========================>.....] - ETA: 0s - loss: 25.89651850/1940 [===========================>..] - ETA: 0s - loss: 25.9121
Epoch 00033: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 289us/sample - loss: 25.8740 - val_loss: 22.8362
Epoch 34/50
  50/1940 [..............................] - ETA: 0s - loss: 25.4691 250/1940 [==>...........................] - ETA: 0s - loss: 26.3662 450/1940 [=====>........................] - ETA: 0s - loss: 26.4188 650/1940 [=========>....................] - ETA: 0s - loss: 26.3636 850/1940 [============>.................] - ETA: 0s - loss: 26.17021050/1940 [===============>..............] - ETA: 0s - loss: 25.88281250/1940 [==================>...........] - ETA: 0s - loss: 25.84171450/1940 [=====================>........] - ETA: 0s - loss: 25.83721650/1940 [========================>.....] - ETA: 0s - loss: 25.74171850/1940 [===========================>..] - ETA: 0s - loss: 25.7480
Epoch 00034: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 283us/sample - loss: 25.7618 - val_loss: 22.8563
Epoch 35/50
  50/1940 [..............................] - ETA: 0s - loss: 23.9450 250/1940 [==>...........................] - ETA: 0s - loss: 25.9620 450/1940 [=====>........................] - ETA: 0s - loss: 26.1234 650/1940 [=========>....................] - ETA: 0s - loss: 26.3259 850/1940 [============>.................] - ETA: 0s - loss: 26.08611050/1940 [===============>..............] - ETA: 0s - loss: 25.90841250/1940 [==================>...........] - ETA: 0s - loss: 25.92261450/1940 [=====================>........] - ETA: 0s - loss: 25.78361650/1940 [========================>.....] - ETA: 0s - loss: 25.80441750/1940 [==========================>...] - ETA: 0s - loss: 25.7738
Epoch 00035: val_loss did not improve from 22.82655
1940/1940 [==============================] - 1s 293us/sample - loss: 25.8197 - val_loss: 22.8540
Epoch 00035: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 2061 samples, validate on 121 samples
Epoch 1/50
  50/2061 [..............................] - ETA: 50s - loss: nan 100/2061 [>.............................] - ETA: 25s - loss: nan 300/2061 [===>..........................] - ETA: 8s - loss: nan  500/2061 [======>.......................] - ETA: 4s - loss: nan 700/2061 [=========>....................] - ETA: 2s - loss: nan 900/2061 [============>.................] - ETA: 2s - loss: nan1100/2061 [===============>..............] - ETA: 1s - loss: nan1300/2061 [=================>............] - ETA: 0s - loss: nan1500/2061 [====================>.........] - ETA: 0s - loss: nan1700/2061 [=======================>......] - ETA: 0s - loss: nan1900/2061 [==========================>...] - ETA: 0s - loss: nan
Epoch 00001: val_loss did not improve from inf
2061/2061 [==============================] - 2s 1ms/sample - loss: nan - val_loss: nan
Epoch 2/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2061/2061 [==============================] - 1s 313us/sample - loss: nan - val_loss: nan
Epoch 3/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2000/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
2061/2061 [==============================] - 1s 329us/sample - loss: nan - val_loss: nan
Epoch 4/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1200/2061 [================>.............] - ETA: 0s - loss: nan1400/2061 [===================>..........] - ETA: 0s - loss: nan1600/2061 [======================>.......] - ETA: 0s - loss: nan1800/2061 [=========================>....] - ETA: 0s - loss: nan1950/2061 [===========================>..] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2061/2061 [==============================] - 1s 325us/sample - loss: nan - val_loss: nan
Epoch 5/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1000/2061 [=============>................] - ETA: 0s - loss: nan1150/2061 [===============>..............] - ETA: 0s - loss: nan1350/2061 [==================>...........] - ETA: 0s - loss: nan1500/2061 [====================>.........] - ETA: 0s - loss: nan1700/2061 [=======================>......] - ETA: 0s - loss: nan1900/2061 [==========================>...] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
2061/2061 [==============================] - 1s 339us/sample - loss: nan - val_loss: nan
Epoch 6/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 400/2061 [====>.........................] - ETA: 0s - loss: nan 600/2061 [=======>......................] - ETA: 0s - loss: nan 750/2061 [=========>....................] - ETA: 0s - loss: nan 950/2061 [============>.................] - ETA: 0s - loss: nan1100/2061 [===============>..............] - ETA: 0s - loss: nan1300/2061 [=================>............] - ETA: 0s - loss: nan1500/2061 [====================>.........] - ETA: 0s - loss: nan1700/2061 [=======================>......] - ETA: 0s - loss: nan1900/2061 [==========================>...] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2061/2061 [==============================] - 1s 330us/sample - loss: nan - val_loss: nan
Epoch 7/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 400/2061 [====>.........................] - ETA: 0s - loss: nan 600/2061 [=======>......................] - ETA: 0s - loss: nan 750/2061 [=========>....................] - ETA: 0s - loss: nan 950/2061 [============>.................] - ETA: 0s - loss: nan1100/2061 [===============>..............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1400/2061 [===================>..........] - ETA: 0s - loss: nan1600/2061 [======================>.......] - ETA: 0s - loss: nan1750/2061 [========================>.....] - ETA: 0s - loss: nan1950/2061 [===========================>..] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
2061/2061 [==============================] - 1s 366us/sample - loss: nan - val_loss: nan
Epoch 8/50
  50/2061 [..............................] - ETA: 0s - loss: nan 200/2061 [=>............................] - ETA: 0s - loss: nan 350/2061 [====>.........................] - ETA: 0s - loss: nan 550/2061 [=======>......................] - ETA: 0s - loss: nan 750/2061 [=========>....................] - ETA: 0s - loss: nan 950/2061 [============>.................] - ETA: 0s - loss: nan1150/2061 [===============>..............] - ETA: 0s - loss: nan1300/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2061/2061 [==============================] - 1s 346us/sample - loss: nan - val_loss: nan
Epoch 9/50
  50/2061 [..............................] - ETA: 0s - loss: nan 200/2061 [=>............................] - ETA: 0s - loss: nan 400/2061 [====>.........................] - ETA: 0s - loss: nan 550/2061 [=======>......................] - ETA: 0s - loss: nan 700/2061 [=========>....................] - ETA: 0s - loss: nan 900/2061 [============>.................] - ETA: 0s - loss: nan1100/2061 [===============>..............] - ETA: 0s - loss: nan1300/2061 [=================>............] - ETA: 0s - loss: nan1500/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2000/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
2061/2061 [==============================] - 1s 365us/sample - loss: nan - val_loss: nan
Epoch 10/50
  50/2061 [..............................] - ETA: 1s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 400/2061 [====>.........................] - ETA: 0s - loss: nan 500/2061 [======>.......................] - ETA: 0s - loss: nan 700/2061 [=========>....................] - ETA: 0s - loss: nan 900/2061 [============>.................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1200/2061 [================>.............] - ETA: 0s - loss: nan1400/2061 [===================>..........] - ETA: 0s - loss: nan1600/2061 [======================>.......] - ETA: 0s - loss: nan1800/2061 [=========================>....] - ETA: 0s - loss: nan2000/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2061/2061 [==============================] - 1s 361us/sample - loss: nan - val_loss: nan
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:209: RuntimeWarning: divide by zero encountered in log
  predictor_matrix[:,:,:,indicesPRED] = numpy.log(predictor_matrix[:,:,:,indicesPRED])
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00010: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b279dcbdda0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b279dccec18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279dccef98> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279dcd26d8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279dcda198> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279dcd2a20> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279e0f0eb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e0f0b00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b279e0f9400> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e129048> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e129b38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e130898> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b279e144518> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b279e144470> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b279e14ee80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e14ed68> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e16a978> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e16abe0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e16ad68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e172b38> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b279e172cc0> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b279dcbdda0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b279dccec18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279dccef98> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279dcd26d8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279dcda198> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279dcd2a20> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279e0f0eb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e0f0b00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b279e0f9400> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e129048> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e129b38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e130898> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b279e144518> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b279e144470> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b279e14ee80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e14ed68> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e16a978> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e16abe0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e16ad68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e172b38> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b279e172cc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279d1bc6d8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e1ae438> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 119.7464
Epoch 00001: val_loss improved from inf to 50.93616, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 2s 14ms/sample - loss: 91.3606 - val_loss: 50.9362
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 52.5184
Epoch 00002: val_loss improved from 50.93616 to 31.81044, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 993us/sample - loss: 38.4618 - val_loss: 31.8104
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 29.6490
Epoch 00003: val_loss did not improve from 31.81044
121/121 [==============================] - 0s 556us/sample - loss: 31.9932 - val_loss: 33.9388
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.2026
Epoch 00004: val_loss improved from 31.81044 to 26.10250, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 689us/sample - loss: 29.4599 - val_loss: 26.1025
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.3905
Epoch 00005: val_loss improved from 26.10250 to 25.14465, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 694us/sample - loss: 24.8873 - val_loss: 25.1447
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.6281
Epoch 00006: val_loss improved from 25.14465 to 24.76177, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 728us/sample - loss: 25.3130 - val_loss: 24.7618
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8885
Epoch 00007: val_loss did not improve from 24.76177
121/121 [==============================] - 0s 606us/sample - loss: 24.4291 - val_loss: 25.0924
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0220
Epoch 00008: val_loss did not improve from 24.76177

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 522us/sample - loss: 24.6493 - val_loss: 24.7798
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7400
Epoch 00009: val_loss improved from 24.76177 to 24.45141, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 689us/sample - loss: 24.2290 - val_loss: 24.4514
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8513
Epoch 00010: val_loss improved from 24.45141 to 24.27806, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 726us/sample - loss: 24.0804 - val_loss: 24.2781
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1028
Epoch 00011: val_loss improved from 24.27806 to 24.21209, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 721us/sample - loss: 24.0090 - val_loss: 24.2121
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5007
Epoch 00012: val_loss improved from 24.21209 to 24.16959, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 836us/sample - loss: 23.9764 - val_loss: 24.1696
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9010
Epoch 00013: val_loss did not improve from 24.16959
121/121 [==============================] - 0s 611us/sample - loss: 23.9082 - val_loss: 24.1698
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3861
Epoch 00014: val_loss did not improve from 24.16959

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 544us/sample - loss: 23.8491 - val_loss: 24.1702
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7271
Epoch 00015: val_loss improved from 24.16959 to 24.14715, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 768us/sample - loss: 23.8182 - val_loss: 24.1472
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7336
Epoch 00016: val_loss improved from 24.14715 to 24.10825, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 2ms/sample - loss: 23.7964 - val_loss: 24.1083
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0726
Epoch 00017: val_loss improved from 24.10825 to 24.06242, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 783us/sample - loss: 23.7770 - val_loss: 24.0624
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3876
Epoch 00018: val_loss improved from 24.06242 to 24.04402, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 783us/sample - loss: 23.7698 - val_loss: 24.0440
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.8743
Epoch 00019: val_loss improved from 24.04402 to 24.03074, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 758us/sample - loss: 23.7563 - val_loss: 24.0307
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.6772
Epoch 00020: val_loss did not improve from 24.03074
121/121 [==============================] - 0s 545us/sample - loss: 23.7219 - val_loss: 24.0357
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7897
Epoch 00021: val_loss did not improve from 24.03074

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 566us/sample - loss: 23.7062 - val_loss: 24.0758
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3977
Epoch 00022: val_loss did not improve from 24.03074
121/121 [==============================] - 0s 574us/sample - loss: 23.7001 - val_loss: 24.0600
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6249
Epoch 00023: val_loss did not improve from 24.03074

Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 538us/sample - loss: 23.6890 - val_loss: 24.0350
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4115
Epoch 00024: val_loss improved from 24.03074 to 24.01925, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 724us/sample - loss: 23.6769 - val_loss: 24.0192
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5969
Epoch 00025: val_loss improved from 24.01925 to 24.00970, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 760us/sample - loss: 23.6711 - val_loss: 24.0097
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6462
Epoch 00026: val_loss improved from 24.00970 to 24.00234, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 776us/sample - loss: 23.6656 - val_loss: 24.0023
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3020
Epoch 00027: val_loss improved from 24.00234 to 23.99417, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 778us/sample - loss: 23.6610 - val_loss: 23.9942
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6584
Epoch 00028: val_loss improved from 23.99417 to 23.99069, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 731us/sample - loss: 23.6570 - val_loss: 23.9907
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8466
Epoch 00029: val_loss improved from 23.99069 to 23.98594, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 814us/sample - loss: 23.6532 - val_loss: 23.9859
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9145
Epoch 00030: val_loss improved from 23.98594 to 23.98168, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 766us/sample - loss: 23.6489 - val_loss: 23.9817
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1747
Epoch 00031: val_loss improved from 23.98168 to 23.97656, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 837us/sample - loss: 23.6443 - val_loss: 23.9766
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.9504
Epoch 00032: val_loss improved from 23.97656 to 23.97257, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 699us/sample - loss: 23.6391 - val_loss: 23.9726
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1298
Epoch 00033: val_loss improved from 23.97257 to 23.96998, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 732us/sample - loss: 23.6345 - val_loss: 23.9700
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4584
Epoch 00034: val_loss improved from 23.96998 to 23.96948, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 23.6300 - val_loss: 23.9695
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8560
Epoch 00035: val_loss improved from 23.96948 to 23.96158, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 735us/sample - loss: 23.6246 - val_loss: 23.9616
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.5715
Epoch 00036: val_loss improved from 23.96158 to 23.95487, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 762us/sample - loss: 23.6186 - val_loss: 23.9549
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6007
Epoch 00037: val_loss improved from 23.95487 to 23.94507, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 719us/sample - loss: 23.6126 - val_loss: 23.9451
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2208
Epoch 00038: val_loss improved from 23.94507 to 23.93963, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 718us/sample - loss: 23.6055 - val_loss: 23.9396
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7515
Epoch 00039: val_loss improved from 23.93963 to 23.93225, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 694us/sample - loss: 23.5983 - val_loss: 23.9322
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.5859
Epoch 00040: val_loss improved from 23.93225 to 23.92847, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 664us/sample - loss: 23.5919 - val_loss: 23.9285
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9223
Epoch 00041: val_loss did not improve from 23.92847
121/121 [==============================] - 0s 493us/sample - loss: 23.5852 - val_loss: 23.9290
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.6808
Epoch 00042: val_loss improved from 23.92847 to 23.92363, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 683us/sample - loss: 23.5780 - val_loss: 23.9236
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2721
Epoch 00043: val_loss improved from 23.92363 to 23.91666, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 23.5704 - val_loss: 23.9167
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0393
Epoch 00044: val_loss improved from 23.91666 to 23.90819, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 742us/sample - loss: 23.5617 - val_loss: 23.9082
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7033
Epoch 00045: val_loss improved from 23.90819 to 23.90098, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 687us/sample - loss: 23.5541 - val_loss: 23.9010
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4830
Epoch 00046: val_loss improved from 23.90098 to 23.88807, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 653us/sample - loss: 23.5457 - val_loss: 23.8881
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8977
Epoch 00047: val_loss improved from 23.88807 to 23.88063, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 628us/sample - loss: 23.5377 - val_loss: 23.8806
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9863
Epoch 00048: val_loss improved from 23.88063 to 23.87057, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 624us/sample - loss: 23.5295 - val_loss: 23.8706
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6801
Epoch 00049: val_loss improved from 23.87057 to 23.86313, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 612us/sample - loss: 23.5222 - val_loss: 23.8631
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4128
Epoch 00050: val_loss improved from 23.86313 to 23.85316, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 638us/sample - loss: 23.5148 - val_loss: 23.8532
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0944
Epoch 00051: val_loss improved from 23.85316 to 23.84116, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 615us/sample - loss: 23.5075 - val_loss: 23.8412
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.0356
Epoch 00052: val_loss improved from 23.84116 to 23.82958, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 23.5002 - val_loss: 23.8296
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3338
Epoch 00053: val_loss improved from 23.82958 to 23.82559, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 601us/sample - loss: 23.4929 - val_loss: 23.8256
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7761
Epoch 00054: val_loss improved from 23.82559 to 23.81498, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 603us/sample - loss: 23.4848 - val_loss: 23.8150
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1493
Epoch 00055: val_loss improved from 23.81498 to 23.80693, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 592us/sample - loss: 23.4788 - val_loss: 23.8069
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2347
Epoch 00056: val_loss improved from 23.80693 to 23.79968, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 604us/sample - loss: 23.4735 - val_loss: 23.7997
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6273
Epoch 00057: val_loss improved from 23.79968 to 23.79476, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 597us/sample - loss: 23.4700 - val_loss: 23.7948
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8195
Epoch 00058: val_loss improved from 23.79476 to 23.79190, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 607us/sample - loss: 23.4629 - val_loss: 23.7919
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4964
Epoch 00059: val_loss improved from 23.79190 to 23.78759, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 604us/sample - loss: 23.4558 - val_loss: 23.7876
Epoch 60/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.4472
Epoch 00060: val_loss improved from 23.78759 to 23.78563, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 607us/sample - loss: 23.4496 - val_loss: 23.7856
Epoch 61/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0021
Epoch 00061: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 440us/sample - loss: 23.4445 - val_loss: 23.7882
Epoch 62/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6570
Epoch 00062: val_loss did not improve from 23.78563

Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 432us/sample - loss: 23.4386 - val_loss: 23.7883
Epoch 63/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3062
Epoch 00063: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 426us/sample - loss: 23.4347 - val_loss: 23.7883
Epoch 64/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0339
Epoch 00064: val_loss did not improve from 23.78563

Epoch 00064: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 431us/sample - loss: 23.4329 - val_loss: 23.7888
Epoch 65/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0462
Epoch 00065: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 426us/sample - loss: 23.4312 - val_loss: 23.7881
Epoch 66/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2723
Epoch 00066: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 430us/sample - loss: 23.4293 - val_loss: 23.7863
Epoch 67/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3392
Epoch 00067: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 440us/sample - loss: 23.4274 - val_loss: 23.7876
Epoch 68/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3384
Epoch 00068: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 478us/sample - loss: 23.4256 - val_loss: 23.7874
Epoch 69/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9441
Epoch 00069: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 488us/sample - loss: 23.4237 - val_loss: 23.7882
Epoch 70/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0251
Epoch 00070: val_loss did not improve from 23.78563
121/121 [==============================] - 0s 496us/sample - loss: 23.4221 - val_loss: 23.7860
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00070: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b279e4bdc88> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b279e4c4390> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e4c4898> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279e4c9780> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e4d8240> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e4c9ac8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279e8edf60> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e8edba8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b279e8f64a8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e923160> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e923c50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e92e9b0> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b279e93e630> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b279e93e588> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b279e94af98> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e94ae80> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e964a90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e964cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e964e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e96dc50> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b279e96ddd8> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b279e4bdc88> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b279e4c4390> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e4c4898> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279e4c9780> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e4d8240> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e4c9ac8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b279e8edf60> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e8edba8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b279e8f64a8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e923160> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e923c50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e92e9b0> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b279e93e630> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b279e93e588> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b279e94af98> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e94ae80> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e964a90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e964cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b279e964e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e96dc50> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b279e96ddd8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279d98b780> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b279e983e80> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 2s - loss: 183.0412
Epoch 00001: val_loss improved from inf to 97.69839, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 7ms/sample - loss: 154.7824 - val_loss: 97.6984
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 88.4097
Epoch 00002: val_loss improved from 97.69839 to 40.73281, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 662us/sample - loss: 75.2168 - val_loss: 40.7328
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.8686
Epoch 00003: val_loss improved from 40.73281 to 33.62964, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 671us/sample - loss: 36.7186 - val_loss: 33.6296
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.5880
Epoch 00004: val_loss improved from 33.62964 to 33.39251, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 704us/sample - loss: 36.4142 - val_loss: 33.3925
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 35.8580
Epoch 00005: val_loss improved from 33.39251 to 25.51114, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 706us/sample - loss: 31.3709 - val_loss: 25.5111
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 30.2252
Epoch 00006: val_loss did not improve from 25.51114
121/121 [==============================] - 0s 551us/sample - loss: 25.5149 - val_loss: 26.5718
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.4317
Epoch 00007: val_loss improved from 25.51114 to 25.02103, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 656us/sample - loss: 26.3309 - val_loss: 25.0210
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.5693
Epoch 00008: val_loss improved from 25.02103 to 24.85481, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 659us/sample - loss: 25.0596 - val_loss: 24.8548
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.2644
Epoch 00009: val_loss improved from 24.85481 to 24.59631, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 702us/sample - loss: 24.9909 - val_loss: 24.5963
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4365
Epoch 00010: val_loss improved from 24.59631 to 24.41868, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 697us/sample - loss: 24.7289 - val_loss: 24.4187
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7701
Epoch 00011: val_loss improved from 24.41868 to 24.38072, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 691us/sample - loss: 24.6129 - val_loss: 24.3807
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1019
Epoch 00012: val_loss did not improve from 24.38072
121/121 [==============================] - 0s 521us/sample - loss: 24.5365 - val_loss: 24.3868
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0816
Epoch 00013: val_loss improved from 24.38072 to 24.11970, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 649us/sample - loss: 24.3723 - val_loss: 24.1197
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7337
Epoch 00014: val_loss did not improve from 24.11970
121/121 [==============================] - 0s 517us/sample - loss: 24.2662 - val_loss: 24.1474
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1049
Epoch 00015: val_loss improved from 24.11970 to 23.95760, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 699us/sample - loss: 24.2193 - val_loss: 23.9576
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.4927
Epoch 00016: val_loss improved from 23.95760 to 23.87295, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 24.0992 - val_loss: 23.8730
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.3653
Epoch 00017: val_loss improved from 23.87295 to 23.86374, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 735us/sample - loss: 24.0002 - val_loss: 23.8637
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.2554
Epoch 00018: val_loss improved from 23.86374 to 23.75249, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 674us/sample - loss: 23.9156 - val_loss: 23.7525
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8417
Epoch 00019: val_loss improved from 23.75249 to 23.69424, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 686us/sample - loss: 23.8519 - val_loss: 23.6942
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8198
Epoch 00020: val_loss improved from 23.69424 to 23.65081, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 649us/sample - loss: 23.7813 - val_loss: 23.6508
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2854
Epoch 00021: val_loss improved from 23.65081 to 23.60464, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 661us/sample - loss: 23.7197 - val_loss: 23.6046
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0477
Epoch 00022: val_loss improved from 23.60464 to 23.58798, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 663us/sample - loss: 23.6901 - val_loss: 23.5880
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3505
Epoch 00023: val_loss improved from 23.58798 to 23.47707, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 655us/sample - loss: 23.6554 - val_loss: 23.4771
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5514
Epoch 00024: val_loss did not improve from 23.47707
121/121 [==============================] - 0s 505us/sample - loss: 23.6247 - val_loss: 23.4824
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3522
Epoch 00025: val_loss improved from 23.47707 to 23.40128, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 671us/sample - loss: 23.6012 - val_loss: 23.4013
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4769
Epoch 00026: val_loss improved from 23.40128 to 23.34035, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 674us/sample - loss: 23.6034 - val_loss: 23.3404
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0550
Epoch 00027: val_loss did not improve from 23.34035
121/121 [==============================] - 0s 529us/sample - loss: 23.4561 - val_loss: 23.3554
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1668
Epoch 00028: val_loss improved from 23.34035 to 23.12955, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 658us/sample - loss: 23.3976 - val_loss: 23.1295
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3104
Epoch 00029: val_loss improved from 23.12955 to 23.11503, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 692us/sample - loss: 23.2805 - val_loss: 23.1150
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2963
Epoch 00030: val_loss improved from 23.11503 to 23.08524, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 688us/sample - loss: 23.2540 - val_loss: 23.0852
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8732
Epoch 00031: val_loss improved from 23.08524 to 22.99994, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 695us/sample - loss: 23.5129 - val_loss: 22.9999
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9577
Epoch 00032: val_loss did not improve from 22.99994
121/121 [==============================] - 0s 513us/sample - loss: 23.2865 - val_loss: 23.0187
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8377
Epoch 00033: val_loss did not improve from 22.99994

Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 524us/sample - loss: 23.1369 - val_loss: 23.1300
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2054
Epoch 00034: val_loss improved from 22.99994 to 22.97570, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 670us/sample - loss: 23.2041 - val_loss: 22.9757
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9672
Epoch 00035: val_loss improved from 22.97570 to 22.91507, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 697us/sample - loss: 23.0758 - val_loss: 22.9151
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7497
Epoch 00036: val_loss improved from 22.91507 to 22.90533, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 701us/sample - loss: 23.1066 - val_loss: 22.9053
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1674
Epoch 00037: val_loss did not improve from 22.90533
121/121 [==============================] - 0s 528us/sample - loss: 23.0478 - val_loss: 22.9342
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6110
Epoch 00038: val_loss did not improve from 22.90533

Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 681us/sample - loss: 23.0397 - val_loss: 22.9536
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1626
Epoch 00039: val_loss did not improve from 22.90533
121/121 [==============================] - 0s 741us/sample - loss: 23.0381 - val_loss: 22.9360
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6603
Epoch 00040: val_loss improved from 22.90533 to 22.90429, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 686us/sample - loss: 23.0286 - val_loss: 22.9043
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1260
Epoch 00041: val_loss improved from 22.90429 to 22.90009, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 23.0337 - val_loss: 22.9001
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2023
Epoch 00042: val_loss did not improve from 22.90009
121/121 [==============================] - 0s 509us/sample - loss: 23.0114 - val_loss: 22.9187
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2857
Epoch 00043: val_loss did not improve from 22.90009

Epoch 00043: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 528us/sample - loss: 23.0286 - val_loss: 22.9295
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5743
Epoch 00044: val_loss improved from 22.90009 to 22.89176, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 23.0175 - val_loss: 22.8918
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7363
Epoch 00045: val_loss improved from 22.89176 to 22.87567, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 23.0084 - val_loss: 22.8757
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7603
Epoch 00046: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 571us/sample - loss: 23.0080 - val_loss: 22.8822
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8213
Epoch 00047: val_loss did not improve from 22.87567

Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 530us/sample - loss: 23.0045 - val_loss: 22.8931
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4725
Epoch 00048: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 485us/sample - loss: 23.0050 - val_loss: 22.9012
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9119
Epoch 00049: val_loss did not improve from 22.87567

Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 518us/sample - loss: 23.0058 - val_loss: 22.9125
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8281
Epoch 00050: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 503us/sample - loss: 23.0077 - val_loss: 22.9157
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1939
Epoch 00051: val_loss did not improve from 22.87567

Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 511us/sample - loss: 23.0090 - val_loss: 22.9160
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8295
Epoch 00052: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 502us/sample - loss: 23.0082 - val_loss: 22.9117
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2001
Epoch 00053: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 509us/sample - loss: 23.0065 - val_loss: 22.9067
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7165
Epoch 00054: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 531us/sample - loss: 23.0055 - val_loss: 22.9051
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0376
Epoch 00055: val_loss did not improve from 22.87567
121/121 [==============================] - 0s 516us/sample - loss: 23.0044 - val_loss: 22.9001
Epoch 00055: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
Traceback (most recent call last):
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "Run_CNN_FineTune_ByYear.py", line 347, in <module>
    model.load_weights(Wsave_name)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 234, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1187, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 99, in NewCheckpointReader
    error_translator(e)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2001/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
