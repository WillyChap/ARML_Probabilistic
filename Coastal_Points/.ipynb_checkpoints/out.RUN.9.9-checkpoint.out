2020-11-09 20:21:59.626077: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 20:21:59.633846: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 20:21:59.634014: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556e183ee480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 20:21:59.634034: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 20:21:59.635548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 20:21:59.737674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:21:59.755820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:21:59.842805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:21:59.876970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:21:59.925130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:21:59.991497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:22:00.026073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:22:00.062819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:22:00.066414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:22:00.066533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:22:00.225620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:22:00.225732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:22:00.225749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:22:00.230819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:22:00.233328: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556e19169af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 20:22:00.233384: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 20:22:00.239185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:22:00.239296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:22:00.239315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:22:00.239331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:22:00.242021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:22:00.242042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:22:00.242058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:22:00.242074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:22:00.245187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:22:00.245237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:22:00.245249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:22:00.245259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:22:00.248343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:22:00.250279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:22:00.250356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:22:00.250375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:22:00.250391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:22:00.250406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:22:00.250421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:22:00.250436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:22:00.250451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:22:00.253461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:22:00.253511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:22:00.253523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:22:00.253532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:22:00.261394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 160.7834, 153.5173
Mean and standard deviation for "p_sfc" = 983.9792, 61.8424
Mean and standard deviation for "u_tr_p" = 12.5789, 12.3583
Mean and standard deviation for "v_tr_p" = 1.5146, 13.2555
Mean and standard deviation for "Z_p" = 5574.7589, 201.8906
Mean and standard deviation for "IWV" = 13.5527, 7.8044
2020-11-09 20:22:08.112933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:22:08.113730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:22:08.113758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:22:08.113774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:22:08.113789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:22:08.113806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:22:08.113821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:22:08.113837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:22:08.130364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:22:08.133326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:22:08.133452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:22:08.133475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:22:08.133492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:22:08.133510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:22:08.133528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:22:08.133545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:22:08.133570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:22:08.140090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:22:08.140171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:22:08.140184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:22:08.140195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:22:08.144687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:22:10.380944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:22:11.706908: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 20:22:11.722288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 205.2482, 181.0314
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 970 samples, validate on 121 samples
Epoch 1/50
 50/970 [>.............................] - ETA: 53s - loss: 163.8701250/970 [======>.......................] - ETA: 8s - loss: 158.4942 450/970 [============>.................] - ETA: 3s - loss: 157.9723650/970 [===================>..........] - ETA: 1s - loss: 155.4815850/970 [=========================>....] - ETA: 0s - loss: 149.5620
Epoch 00001: val_loss improved from inf to 66.16134, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 4s 4ms/sample - loss: 142.4374 - val_loss: 66.1613
Epoch 2/50
 50/970 [>.............................] - ETA: 0s - loss: 48.0916250/970 [======>.......................] - ETA: 0s - loss: 54.2580450/970 [============>.................] - ETA: 0s - loss: 52.8945650/970 [===================>..........] - ETA: 0s - loss: 50.5940850/970 [=========================>....] - ETA: 0s - loss: 49.7802
Epoch 00002: val_loss improved from 66.16134 to 46.48758, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 346us/sample - loss: 49.2349 - val_loss: 46.4876
Epoch 3/50
 50/970 [>.............................] - ETA: 0s - loss: 44.0551250/970 [======>.......................] - ETA: 0s - loss: 42.1947450/970 [============>.................] - ETA: 0s - loss: 42.4384650/970 [===================>..........] - ETA: 0s - loss: 41.1432850/970 [=========================>....] - ETA: 0s - loss: 40.2507
Epoch 00003: val_loss did not improve from 46.48758
970/970 [==============================] - 0s 309us/sample - loss: 39.7516 - val_loss: 51.6610
Epoch 4/50
 50/970 [>.............................] - ETA: 0s - loss: 36.2780250/970 [======>.......................] - ETA: 0s - loss: 34.9593450/970 [============>.................] - ETA: 0s - loss: 35.3328650/970 [===================>..........] - ETA: 0s - loss: 35.1185850/970 [=========================>....] - ETA: 0s - loss: 34.7267
Epoch 00004: val_loss did not improve from 46.48758

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
970/970 [==============================] - 0s 318us/sample - loss: 34.4035 - val_loss: 54.9824
Epoch 5/50
 50/970 [>.............................] - ETA: 0s - loss: 34.9228250/970 [======>.......................] - ETA: 0s - loss: 33.1826450/970 [============>.................] - ETA: 0s - loss: 32.4412650/970 [===================>..........] - ETA: 0s - loss: 32.1272850/970 [=========================>....] - ETA: 0s - loss: 32.0240
Epoch 00005: val_loss did not improve from 46.48758
970/970 [==============================] - 0s 305us/sample - loss: 31.9032 - val_loss: 52.0903
Epoch 6/50
 50/970 [>.............................] - ETA: 0s - loss: 31.2888250/970 [======>.......................] - ETA: 0s - loss: 31.1436450/970 [============>.................] - ETA: 0s - loss: 30.7408650/970 [===================>..........] - ETA: 0s - loss: 30.5820850/970 [=========================>....] - ETA: 0s - loss: 30.6052
Epoch 00006: val_loss did not improve from 46.48758

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
970/970 [==============================] - 0s 305us/sample - loss: 30.5594 - val_loss: 49.5947
Epoch 7/50
 50/970 [>.............................] - ETA: 0s - loss: 28.6554250/970 [======>.......................] - ETA: 0s - loss: 30.3125450/970 [============>.................] - ETA: 0s - loss: 29.6633650/970 [===================>..........] - ETA: 0s - loss: 29.8390850/970 [=========================>....] - ETA: 0s - loss: 29.7782
Epoch 00007: val_loss improved from 46.48758 to 45.89043, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 343us/sample - loss: 29.6689 - val_loss: 45.8904
Epoch 8/50
 50/970 [>.............................] - ETA: 0s - loss: 28.8062250/970 [======>.......................] - ETA: 0s - loss: 29.5604450/970 [============>.................] - ETA: 0s - loss: 29.2674650/970 [===================>..........] - ETA: 0s - loss: 29.2469850/970 [=========================>....] - ETA: 0s - loss: 29.2353
Epoch 00008: val_loss improved from 45.89043 to 41.11856, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 345us/sample - loss: 29.3447 - val_loss: 41.1186
Epoch 9/50
 50/970 [>.............................] - ETA: 0s - loss: 29.3255250/970 [======>.......................] - ETA: 0s - loss: 29.0437450/970 [============>.................] - ETA: 0s - loss: 28.9690650/970 [===================>..........] - ETA: 0s - loss: 28.9585850/970 [=========================>....] - ETA: 0s - loss: 28.9892
Epoch 00009: val_loss improved from 41.11856 to 37.88354, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 349us/sample - loss: 28.9686 - val_loss: 37.8835
Epoch 10/50
 50/970 [>.............................] - ETA: 0s - loss: 28.3992250/970 [======>.......................] - ETA: 0s - loss: 28.1729450/970 [============>.................] - ETA: 0s - loss: 28.9834650/970 [===================>..........] - ETA: 0s - loss: 28.8027850/970 [=========================>....] - ETA: 0s - loss: 28.6629
Epoch 00010: val_loss improved from 37.88354 to 35.89397, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 346us/sample - loss: 28.7151 - val_loss: 35.8940
Epoch 11/50
 50/970 [>.............................] - ETA: 0s - loss: 29.1648250/970 [======>.......................] - ETA: 0s - loss: 28.5207450/970 [============>.................] - ETA: 0s - loss: 28.4925650/970 [===================>..........] - ETA: 0s - loss: 28.4532850/970 [=========================>....] - ETA: 0s - loss: 28.3681
Epoch 00011: val_loss improved from 35.89397 to 33.15711, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 353us/sample - loss: 28.4280 - val_loss: 33.1571
Epoch 12/50
 50/970 [>.............................] - ETA: 0s - loss: 28.9744250/970 [======>.......................] - ETA: 0s - loss: 28.6052450/970 [============>.................] - ETA: 0s - loss: 28.3333650/970 [===================>..........] - ETA: 0s - loss: 28.3761850/970 [=========================>....] - ETA: 0s - loss: 28.1086
Epoch 00012: val_loss improved from 33.15711 to 31.63939, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 342us/sample - loss: 28.0936 - val_loss: 31.6394
Epoch 13/50
 50/970 [>.............................] - ETA: 0s - loss: 27.2727250/970 [======>.......................] - ETA: 0s - loss: 28.6441450/970 [============>.................] - ETA: 0s - loss: 28.6679600/970 [=================>............] - ETA: 0s - loss: 28.5967800/970 [=======================>......] - ETA: 0s - loss: 28.3379
Epoch 00013: val_loss improved from 31.63939 to 30.06280, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 375us/sample - loss: 28.1019 - val_loss: 30.0628
Epoch 14/50
 50/970 [>.............................] - ETA: 0s - loss: 28.5439250/970 [======>.......................] - ETA: 0s - loss: 28.6219450/970 [============>.................] - ETA: 0s - loss: 28.0217650/970 [===================>..........] - ETA: 0s - loss: 27.8592850/970 [=========================>....] - ETA: 0s - loss: 27.8430
Epoch 00014: val_loss improved from 30.06280 to 28.34738, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 339us/sample - loss: 27.8506 - val_loss: 28.3474
Epoch 15/50
 50/970 [>.............................] - ETA: 0s - loss: 31.5021250/970 [======>.......................] - ETA: 0s - loss: 28.0490450/970 [============>.................] - ETA: 0s - loss: 28.0603650/970 [===================>..........] - ETA: 0s - loss: 27.9747850/970 [=========================>....] - ETA: 0s - loss: 27.8315
Epoch 00015: val_loss improved from 28.34738 to 27.66148, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 347us/sample - loss: 27.6356 - val_loss: 27.6615
Epoch 16/50
 50/970 [>.............................] - ETA: 0s - loss: 25.7178250/970 [======>.......................] - ETA: 0s - loss: 26.6215450/970 [============>.................] - ETA: 0s - loss: 27.1821650/970 [===================>..........] - ETA: 0s - loss: 27.4268850/970 [=========================>....] - ETA: 0s - loss: 27.4257
Epoch 00016: val_loss improved from 27.66148 to 26.90929, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 351us/sample - loss: 27.4874 - val_loss: 26.9093
Epoch 17/50
 50/970 [>.............................] - ETA: 0s - loss: 28.9967250/970 [======>.......................] - ETA: 0s - loss: 28.3313450/970 [============>.................] - ETA: 0s - loss: 27.7848650/970 [===================>..........] - ETA: 0s - loss: 27.5501850/970 [=========================>....] - ETA: 0s - loss: 27.3968
Epoch 00017: val_loss did not improve from 26.90929
970/970 [==============================] - 0s 321us/sample - loss: 27.4944 - val_loss: 27.1803
Epoch 18/50
 50/970 [>.............................] - ETA: 0s - loss: 28.9920250/970 [======>.......................] - ETA: 0s - loss: 27.8087450/970 [============>.................] - ETA: 0s - loss: 27.4936650/970 [===================>..........] - ETA: 0s - loss: 27.2016850/970 [=========================>....] - ETA: 0s - loss: 27.2315
Epoch 00018: val_loss improved from 26.90929 to 26.27590, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 346us/sample - loss: 27.3300 - val_loss: 26.2759
Epoch 19/50
 50/970 [>.............................] - ETA: 0s - loss: 27.0403250/970 [======>.......................] - ETA: 0s - loss: 26.7594450/970 [============>.................] - ETA: 0s - loss: 27.2113650/970 [===================>..........] - ETA: 0s - loss: 27.2837850/970 [=========================>....] - ETA: 0s - loss: 27.2733
Epoch 00019: val_loss improved from 26.27590 to 25.84657, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 337us/sample - loss: 27.2332 - val_loss: 25.8466
Epoch 20/50
 50/970 [>.............................] - ETA: 0s - loss: 26.4562250/970 [======>.......................] - ETA: 0s - loss: 27.7136450/970 [============>.................] - ETA: 0s - loss: 27.3366650/970 [===================>..........] - ETA: 0s - loss: 27.3941850/970 [=========================>....] - ETA: 0s - loss: 27.2668
Epoch 00020: val_loss improved from 25.84657 to 25.67634, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 342us/sample - loss: 27.1436 - val_loss: 25.6763
Epoch 21/50
 50/970 [>.............................] - ETA: 0s - loss: 26.0218250/970 [======>.......................] - ETA: 0s - loss: 27.3806450/970 [============>.................] - ETA: 0s - loss: 27.3318650/970 [===================>..........] - ETA: 0s - loss: 27.2262850/970 [=========================>....] - ETA: 0s - loss: 27.0527
Epoch 00021: val_loss improved from 25.67634 to 25.57506, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 355us/sample - loss: 27.0244 - val_loss: 25.5751
Epoch 22/50
 50/970 [>.............................] - ETA: 0s - loss: 31.0609250/970 [======>.......................] - ETA: 0s - loss: 27.2662450/970 [============>.................] - ETA: 0s - loss: 26.9764650/970 [===================>..........] - ETA: 0s - loss: 26.8110850/970 [=========================>....] - ETA: 0s - loss: 27.0016
Epoch 00022: val_loss did not improve from 25.57506
970/970 [==============================] - 0s 310us/sample - loss: 26.8943 - val_loss: 25.6459
Epoch 23/50
 50/970 [>.............................] - ETA: 0s - loss: 26.3082250/970 [======>.......................] - ETA: 0s - loss: 26.7716450/970 [============>.................] - ETA: 0s - loss: 26.9440650/970 [===================>..........] - ETA: 0s - loss: 26.8784850/970 [=========================>....] - ETA: 0s - loss: 26.7152
Epoch 00023: val_loss improved from 25.57506 to 25.42204, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 341us/sample - loss: 26.8279 - val_loss: 25.4220
Epoch 24/50
 50/970 [>.............................] - ETA: 0s - loss: 25.2655250/970 [======>.......................] - ETA: 0s - loss: 26.6250450/970 [============>.................] - ETA: 0s - loss: 26.8028650/970 [===================>..........] - ETA: 0s - loss: 26.7193850/970 [=========================>....] - ETA: 0s - loss: 26.6798
Epoch 00024: val_loss improved from 25.42204 to 25.29741, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 351us/sample - loss: 26.7847 - val_loss: 25.2974
Epoch 25/50
 50/970 [>.............................] - ETA: 0s - loss: 25.5593250/970 [======>.......................] - ETA: 0s - loss: 26.7715450/970 [============>.................] - ETA: 0s - loss: 26.7396650/970 [===================>..........] - ETA: 0s - loss: 26.7740850/970 [=========================>....] - ETA: 0s - loss: 26.7129
Epoch 00025: val_loss improved from 25.29741 to 25.20383, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 345us/sample - loss: 26.6985 - val_loss: 25.2038
Epoch 26/50
 50/970 [>.............................] - ETA: 0s - loss: 28.8645250/970 [======>.......................] - ETA: 0s - loss: 26.7917450/970 [============>.................] - ETA: 0s - loss: 26.6220650/970 [===================>..........] - ETA: 0s - loss: 26.4457850/970 [=========================>....] - ETA: 0s - loss: 26.4237
Epoch 00026: val_loss improved from 25.20383 to 25.11147, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 351us/sample - loss: 26.4504 - val_loss: 25.1115
Epoch 27/50
 50/970 [>.............................] - ETA: 0s - loss: 28.1341250/970 [======>.......................] - ETA: 0s - loss: 26.7753450/970 [============>.................] - ETA: 0s - loss: 26.8141650/970 [===================>..........] - ETA: 0s - loss: 26.7442850/970 [=========================>....] - ETA: 0s - loss: 26.6994
Epoch 00027: val_loss did not improve from 25.11147
970/970 [==============================] - 0s 311us/sample - loss: 26.6584 - val_loss: 25.3643
Epoch 28/50
 50/970 [>.............................] - ETA: 0s - loss: 25.8913250/970 [======>.......................] - ETA: 0s - loss: 26.1824450/970 [============>.................] - ETA: 0s - loss: 26.0887650/970 [===================>..........] - ETA: 0s - loss: 26.4461850/970 [=========================>....] - ETA: 0s - loss: 26.4511
Epoch 00028: val_loss improved from 25.11147 to 24.94010, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 335us/sample - loss: 26.3826 - val_loss: 24.9401
Epoch 29/50
 50/970 [>.............................] - ETA: 0s - loss: 25.8718250/970 [======>.......................] - ETA: 0s - loss: 26.4766450/970 [============>.................] - ETA: 0s - loss: 26.6614650/970 [===================>..........] - ETA: 0s - loss: 26.6961850/970 [=========================>....] - ETA: 0s - loss: 26.6173
Epoch 00029: val_loss did not improve from 24.94010
970/970 [==============================] - 0s 318us/sample - loss: 26.4535 - val_loss: 25.2479
Epoch 30/50
 50/970 [>.............................] - ETA: 0s - loss: 28.7149250/970 [======>.......................] - ETA: 0s - loss: 26.1357450/970 [============>.................] - ETA: 0s - loss: 26.5803650/970 [===================>..........] - ETA: 0s - loss: 26.5815850/970 [=========================>....] - ETA: 0s - loss: 26.5033
Epoch 00030: val_loss improved from 24.94010 to 24.67731, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 338us/sample - loss: 26.4419 - val_loss: 24.6773
Epoch 31/50
 50/970 [>.............................] - ETA: 0s - loss: 25.6742250/970 [======>.......................] - ETA: 0s - loss: 26.1379450/970 [============>.................] - ETA: 0s - loss: 25.9161650/970 [===================>..........] - ETA: 0s - loss: 26.0314850/970 [=========================>....] - ETA: 0s - loss: 26.1984
Epoch 00031: val_loss did not improve from 24.67731
970/970 [==============================] - 0s 321us/sample - loss: 26.3243 - val_loss: 24.7654
Epoch 32/50
 50/970 [>.............................] - ETA: 0s - loss: 25.9942250/970 [======>.......................] - ETA: 0s - loss: 26.7259450/970 [============>.................] - ETA: 0s - loss: 26.6710650/970 [===================>..........] - ETA: 0s - loss: 26.2663850/970 [=========================>....] - ETA: 0s - loss: 26.4379
Epoch 00032: val_loss improved from 24.67731 to 24.59651, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 348us/sample - loss: 26.2944 - val_loss: 24.5965
Epoch 33/50
 50/970 [>.............................] - ETA: 0s - loss: 26.5667250/970 [======>.......................] - ETA: 0s - loss: 26.1268450/970 [============>.................] - ETA: 0s - loss: 26.0295650/970 [===================>..........] - ETA: 0s - loss: 25.9540850/970 [=========================>....] - ETA: 0s - loss: 26.2164
Epoch 00033: val_loss improved from 24.59651 to 24.57723, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 342us/sample - loss: 26.2234 - val_loss: 24.5772
Epoch 34/50
 50/970 [>.............................] - ETA: 0s - loss: 24.9327250/970 [======>.......................] - ETA: 0s - loss: 25.7884450/970 [============>.................] - ETA: 0s - loss: 25.9518650/970 [===================>..........] - ETA: 0s - loss: 26.0641850/970 [=========================>....] - ETA: 0s - loss: 26.0314
Epoch 00034: val_loss improved from 24.57723 to 24.54945, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 338us/sample - loss: 26.0976 - val_loss: 24.5495
Epoch 35/50
 50/970 [>.............................] - ETA: 0s - loss: 27.2118250/970 [======>.......................] - ETA: 0s - loss: 26.1358450/970 [============>.................] - ETA: 0s - loss: 25.9762650/970 [===================>..........] - ETA: 0s - loss: 26.0002850/970 [=========================>....] - ETA: 0s - loss: 26.0547
Epoch 00035: val_loss improved from 24.54945 to 24.43744, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 334us/sample - loss: 26.0949 - val_loss: 24.4374
Epoch 36/50
 50/970 [>.............................] - ETA: 0s - loss: 24.8245250/970 [======>.......................] - ETA: 0s - loss: 25.2387450/970 [============>.................] - ETA: 0s - loss: 25.6147650/970 [===================>..........] - ETA: 0s - loss: 25.7622850/970 [=========================>....] - ETA: 0s - loss: 26.0138
Epoch 00036: val_loss did not improve from 24.43744
970/970 [==============================] - 0s 311us/sample - loss: 26.0117 - val_loss: 24.5286
Epoch 37/50
 50/970 [>.............................] - ETA: 0s - loss: 24.6315250/970 [======>.......................] - ETA: 0s - loss: 25.4765450/970 [============>.................] - ETA: 0s - loss: 25.8819650/970 [===================>..........] - ETA: 0s - loss: 26.0087850/970 [=========================>....] - ETA: 0s - loss: 25.9766
Epoch 00037: val_loss improved from 24.43744 to 24.28727, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 346us/sample - loss: 26.0243 - val_loss: 24.2873
Epoch 38/50
 50/970 [>.............................] - ETA: 0s - loss: 24.7225250/970 [======>.......................] - ETA: 0s - loss: 25.7872450/970 [============>.................] - ETA: 0s - loss: 25.7824650/970 [===================>..........] - ETA: 0s - loss: 25.5514850/970 [=========================>....] - ETA: 0s - loss: 25.7367
Epoch 00038: val_loss did not improve from 24.28727
970/970 [==============================] - 0s 318us/sample - loss: 25.8989 - val_loss: 25.0255
Epoch 39/50
 50/970 [>.............................] - ETA: 0s - loss: 26.1374250/970 [======>.......................] - ETA: 0s - loss: 26.3148450/970 [============>.................] - ETA: 0s - loss: 26.2084650/970 [===================>..........] - ETA: 0s - loss: 25.9730850/970 [=========================>....] - ETA: 0s - loss: 26.1042
Epoch 00039: val_loss did not improve from 24.28727

Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
970/970 [==============================] - 0s 308us/sample - loss: 26.2503 - val_loss: 24.4182
Epoch 40/50
 50/970 [>.............................] - ETA: 0s - loss: 26.4464250/970 [======>.......................] - ETA: 0s - loss: 25.8649450/970 [============>.................] - ETA: 0s - loss: 25.8640650/970 [===================>..........] - ETA: 0s - loss: 26.0105850/970 [=========================>....] - ETA: 0s - loss: 25.9463
Epoch 00040: val_loss did not improve from 24.28727
970/970 [==============================] - 0s 308us/sample - loss: 25.9110 - val_loss: 24.3014
Epoch 41/50
 50/970 [>.............................] - ETA: 0s - loss: 26.9848250/970 [======>.......................] - ETA: 0s - loss: 25.2805450/970 [============>.................] - ETA: 0s - loss: 25.7763650/970 [===================>..........] - ETA: 0s - loss: 25.8198850/970 [=========================>....] - ETA: 0s - loss: 25.9106
Epoch 00041: val_loss improved from 24.28727 to 24.25320, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 336us/sample - loss: 25.8857 - val_loss: 24.2532
Epoch 42/50
 50/970 [>.............................] - ETA: 0s - loss: 27.3996250/970 [======>.......................] - ETA: 0s - loss: 25.9834450/970 [============>.................] - ETA: 0s - loss: 25.8700650/970 [===================>..........] - ETA: 0s - loss: 25.9638850/970 [=========================>....] - ETA: 0s - loss: 25.9077
Epoch 00042: val_loss did not improve from 24.25320
970/970 [==============================] - 0s 304us/sample - loss: 25.8618 - val_loss: 24.3051
Epoch 43/50
 50/970 [>.............................] - ETA: 0s - loss: 27.9196250/970 [======>.......................] - ETA: 0s - loss: 25.9879450/970 [============>.................] - ETA: 0s - loss: 25.7487650/970 [===================>..........] - ETA: 0s - loss: 25.7645850/970 [=========================>....] - ETA: 0s - loss: 26.0748
Epoch 00043: val_loss did not improve from 24.25320

Epoch 00043: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
970/970 [==============================] - 0s 318us/sample - loss: 25.9792 - val_loss: 24.2712
Epoch 44/50
 50/970 [>.............................] - ETA: 0s - loss: 25.6565200/970 [=====>........................] - ETA: 0s - loss: 25.7270350/970 [=========>....................] - ETA: 0s - loss: 26.0613550/970 [================>.............] - ETA: 0s - loss: 25.8917750/970 [======================>.......] - ETA: 0s - loss: 25.8592950/970 [============================>.] - ETA: 0s - loss: 25.8357
Epoch 00044: val_loss improved from 24.25320 to 24.12420, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 399us/sample - loss: 25.8555 - val_loss: 24.1242
Epoch 45/50
 50/970 [>.............................] - ETA: 0s - loss: 27.5327250/970 [======>.......................] - ETA: 0s - loss: 25.8448450/970 [============>.................] - ETA: 0s - loss: 25.6647650/970 [===================>..........] - ETA: 0s - loss: 25.6237850/970 [=========================>....] - ETA: 0s - loss: 25.9108
Epoch 00045: val_loss improved from 24.12420 to 24.10284, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 356us/sample - loss: 25.8471 - val_loss: 24.1028
Epoch 46/50
 50/970 [>.............................] - ETA: 0s - loss: 25.4280250/970 [======>.......................] - ETA: 0s - loss: 25.4515450/970 [============>.................] - ETA: 0s - loss: 25.8659650/970 [===================>..........] - ETA: 0s - loss: 25.7956850/970 [=========================>....] - ETA: 0s - loss: 25.6261
Epoch 00046: val_loss did not improve from 24.10284
970/970 [==============================] - 0s 318us/sample - loss: 25.7262 - val_loss: 24.1078
Epoch 47/50
 50/970 [>.............................] - ETA: 0s - loss: 26.5278250/970 [======>.......................] - ETA: 0s - loss: 25.8345450/970 [============>.................] - ETA: 0s - loss: 25.7368650/970 [===================>..........] - ETA: 0s - loss: 25.7007850/970 [=========================>....] - ETA: 0s - loss: 25.7244
Epoch 00047: val_loss improved from 24.10284 to 24.09423, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 358us/sample - loss: 25.7315 - val_loss: 24.0942
Epoch 48/50
 50/970 [>.............................] - ETA: 0s - loss: 25.9428250/970 [======>.......................] - ETA: 0s - loss: 25.7765450/970 [============>.................] - ETA: 0s - loss: 26.1486650/970 [===================>..........] - ETA: 0s - loss: 25.9075850/970 [=========================>....] - ETA: 0s - loss: 25.8805
Epoch 00048: val_loss improved from 24.09423 to 24.08789, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 350us/sample - loss: 25.7287 - val_loss: 24.0879
Epoch 49/50
 50/970 [>.............................] - ETA: 0s - loss: 25.8076250/970 [======>.......................] - ETA: 0s - loss: 25.5136450/970 [============>.................] - ETA: 0s - loss: 25.6568650/970 [===================>..........] - ETA: 0s - loss: 25.8925850/970 [=========================>....] - ETA: 0s - loss: 25.7503
Epoch 00049: val_loss improved from 24.08789 to 24.08587, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 340us/sample - loss: 25.8268 - val_loss: 24.0859
Epoch 50/50
 50/970 [>.............................] - ETA: 0s - loss: 24.0256250/970 [======>.......................] - ETA: 0s - loss: 25.3916450/970 [============>.................] - ETA: 0s - loss: 25.2723650/970 [===================>..........] - ETA: 0s - loss: 25.5641850/970 [=========================>....] - ETA: 0s - loss: 25.8881
Epoch 00050: val_loss improved from 24.08587 to 24.06612, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
970/970 [==============================] - 0s 341us/sample - loss: 25.8686 - val_loss: 24.0661
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 970 samples, validate on 121 samples
Epoch 1/50
 50/970 [>.............................] - ETA: 14s - loss: 159.9106150/970 [===>..........................] - ETA: 4s - loss: 156.4511 350/970 [=========>....................] - ETA: 1s - loss: 156.4017550/970 [================>.............] - ETA: 0s - loss: 155.2465750/970 [======================>.......] - ETA: 0s - loss: 146.5172950/970 [============================>.] - ETA: 0s - loss: 132.0260
Epoch 00001: val_loss improved from inf to 58.88284, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 2s 2ms/sample - loss: 130.5810 - val_loss: 58.8828
Epoch 2/50
 50/970 [>.............................] - ETA: 0s - loss: 69.6150250/970 [======>.......................] - ETA: 0s - loss: 55.1551450/970 [============>.................] - ETA: 0s - loss: 52.3774650/970 [===================>..........] - ETA: 0s - loss: 50.9396850/970 [=========================>....] - ETA: 0s - loss: 48.8637
Epoch 00002: val_loss did not improve from 58.88284
970/970 [==============================] - 0s 318us/sample - loss: 47.9006 - val_loss: 63.4446
Epoch 3/50
 50/970 [>.............................] - ETA: 0s - loss: 38.4528250/970 [======>.......................] - ETA: 0s - loss: 39.2682450/970 [============>.................] - ETA: 0s - loss: 38.1570650/970 [===================>..........] - ETA: 0s - loss: 37.3506850/970 [=========================>....] - ETA: 0s - loss: 37.0185
Epoch 00003: val_loss did not improve from 58.88284

Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
970/970 [==============================] - 0s 308us/sample - loss: 36.7136 - val_loss: 61.5254
Epoch 4/50
 50/970 [>.............................] - ETA: 0s - loss: 34.4504250/970 [======>.......................] - ETA: 0s - loss: 33.7809450/970 [============>.................] - ETA: 0s - loss: 33.5941650/970 [===================>..........] - ETA: 0s - loss: 33.4373850/970 [=========================>....] - ETA: 0s - loss: 33.6031
Epoch 00004: val_loss did not improve from 58.88284
970/970 [==============================] - 0s 312us/sample - loss: 33.4437 - val_loss: 60.3002
Epoch 5/50
 50/970 [>.............................] - ETA: 0s - loss: 33.4465250/970 [======>.......................] - ETA: 0s - loss: 32.8295450/970 [============>.................] - ETA: 0s - loss: 31.9498650/970 [===================>..........] - ETA: 0s - loss: 32.0040850/970 [=========================>....] - ETA: 0s - loss: 32.1556
Epoch 00005: val_loss improved from 58.88284 to 56.07363, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 339us/sample - loss: 32.0243 - val_loss: 56.0736
Epoch 6/50
 50/970 [>.............................] - ETA: 0s - loss: 31.8013250/970 [======>.......................] - ETA: 0s - loss: 31.2002450/970 [============>.................] - ETA: 0s - loss: 30.6580650/970 [===================>..........] - ETA: 0s - loss: 30.9598800/970 [=======================>......] - ETA: 0s - loss: 30.8664900/970 [==========================>...] - ETA: 0s - loss: 30.9187
Epoch 00006: val_loss improved from 56.07363 to 54.55973, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 412us/sample - loss: 30.8690 - val_loss: 54.5597
Epoch 7/50
 50/970 [>.............................] - ETA: 0s - loss: 28.1042250/970 [======>.......................] - ETA: 0s - loss: 29.8576450/970 [============>.................] - ETA: 0s - loss: 29.5168650/970 [===================>..........] - ETA: 0s - loss: 29.5283850/970 [=========================>....] - ETA: 0s - loss: 29.8208
Epoch 00007: val_loss improved from 54.55973 to 52.10235, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 366us/sample - loss: 29.8577 - val_loss: 52.1023
Epoch 8/50
 50/970 [>.............................] - ETA: 0s - loss: 28.5172250/970 [======>.......................] - ETA: 0s - loss: 29.2942450/970 [============>.................] - ETA: 0s - loss: 29.2746650/970 [===================>..........] - ETA: 0s - loss: 29.1527850/970 [=========================>....] - ETA: 0s - loss: 29.1832
Epoch 00008: val_loss improved from 52.10235 to 47.10356, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 352us/sample - loss: 29.2415 - val_loss: 47.1036
Epoch 9/50
 50/970 [>.............................] - ETA: 0s - loss: 28.0229250/970 [======>.......................] - ETA: 0s - loss: 28.6331450/970 [============>.................] - ETA: 0s - loss: 28.7399650/970 [===================>..........] - ETA: 0s - loss: 28.5393850/970 [=========================>....] - ETA: 0s - loss: 28.3630
Epoch 00009: val_loss improved from 47.10356 to 44.14566, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 350us/sample - loss: 28.4161 - val_loss: 44.1457
Epoch 10/50
 50/970 [>.............................] - ETA: 0s - loss: 28.4608250/970 [======>.......................] - ETA: 0s - loss: 28.4140450/970 [============>.................] - ETA: 0s - loss: 28.1941650/970 [===================>..........] - ETA: 0s - loss: 28.4159850/970 [=========================>....] - ETA: 0s - loss: 28.3132
Epoch 00010: val_loss improved from 44.14566 to 41.18789, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 343us/sample - loss: 28.1402 - val_loss: 41.1879
Epoch 11/50
 50/970 [>.............................] - ETA: 0s - loss: 28.2472250/970 [======>.......................] - ETA: 0s - loss: 29.6631450/970 [============>.................] - ETA: 0s - loss: 29.1536650/970 [===================>..........] - ETA: 0s - loss: 28.5820850/970 [=========================>....] - ETA: 0s - loss: 28.2682
Epoch 00011: val_loss improved from 41.18789 to 35.93196, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 355us/sample - loss: 28.1828 - val_loss: 35.9320
Epoch 12/50
 50/970 [>.............................] - ETA: 0s - loss: 27.1283250/970 [======>.......................] - ETA: 0s - loss: 26.6542450/970 [============>.................] - ETA: 0s - loss: 27.4765650/970 [===================>..........] - ETA: 0s - loss: 27.5226850/970 [=========================>....] - ETA: 0s - loss: 27.4758
Epoch 00012: val_loss improved from 35.93196 to 33.57297, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 389us/sample - loss: 27.5466 - val_loss: 33.5730
Epoch 13/50
 50/970 [>.............................] - ETA: 0s - loss: 26.2403250/970 [======>.......................] - ETA: 0s - loss: 27.0363450/970 [============>.................] - ETA: 0s - loss: 26.9900650/970 [===================>..........] - ETA: 0s - loss: 26.8823850/970 [=========================>....] - ETA: 0s - loss: 26.9892
Epoch 00013: val_loss improved from 33.57297 to 32.12456, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 336us/sample - loss: 26.9819 - val_loss: 32.1246
Epoch 14/50
 50/970 [>.............................] - ETA: 0s - loss: 28.0621250/970 [======>.......................] - ETA: 0s - loss: 27.1440450/970 [============>.................] - ETA: 0s - loss: 27.3393650/970 [===================>..........] - ETA: 0s - loss: 27.1713850/970 [=========================>....] - ETA: 0s - loss: 26.9147
Epoch 00014: val_loss improved from 32.12456 to 28.75448, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 342us/sample - loss: 26.9066 - val_loss: 28.7545
Epoch 15/50
 50/970 [>.............................] - ETA: 0s - loss: 27.2792250/970 [======>.......................] - ETA: 0s - loss: 26.4320450/970 [============>.................] - ETA: 0s - loss: 26.8015650/970 [===================>..........] - ETA: 0s - loss: 26.6683850/970 [=========================>....] - ETA: 0s - loss: 26.5782
Epoch 00015: val_loss improved from 28.75448 to 28.39888, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 347us/sample - loss: 26.6476 - val_loss: 28.3989
Epoch 16/50
 50/970 [>.............................] - ETA: 0s - loss: 24.5069250/970 [======>.......................] - ETA: 0s - loss: 26.0449450/970 [============>.................] - ETA: 0s - loss: 26.4174650/970 [===================>..........] - ETA: 0s - loss: 26.6780850/970 [=========================>....] - ETA: 0s - loss: 26.4995
Epoch 00016: val_loss improved from 28.39888 to 26.86496, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 340us/sample - loss: 26.5074 - val_loss: 26.8650
Epoch 17/50
 50/970 [>.............................] - ETA: 0s - loss: 25.7505250/970 [======>.......................] - ETA: 0s - loss: 26.1970450/970 [============>.................] - ETA: 0s - loss: 26.4854650/970 [===================>..........] - ETA: 0s - loss: 26.1651850/970 [=========================>....] - ETA: 0s - loss: 26.2776
Epoch 00017: val_loss improved from 26.86496 to 25.50871, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 346us/sample - loss: 26.2927 - val_loss: 25.5087
Epoch 18/50
 50/970 [>.............................] - ETA: 0s - loss: 26.6444250/970 [======>.......................] - ETA: 0s - loss: 26.1955450/970 [============>.................] - ETA: 0s - loss: 26.3912650/970 [===================>..........] - ETA: 0s - loss: 26.6829850/970 [=========================>....] - ETA: 0s - loss: 26.4502
Epoch 00018: val_loss did not improve from 25.50871
970/970 [==============================] - 0s 320us/sample - loss: 26.5058 - val_loss: 26.5471
Epoch 19/50
 50/970 [>.............................] - ETA: 0s - loss: 26.1766250/970 [======>.......................] - ETA: 0s - loss: 25.6544450/970 [============>.................] - ETA: 0s - loss: 25.6460650/970 [===================>..........] - ETA: 0s - loss: 26.2889850/970 [=========================>....] - ETA: 0s - loss: 26.3907
Epoch 00019: val_loss improved from 25.50871 to 25.40105, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 345us/sample - loss: 26.5193 - val_loss: 25.4010
Epoch 20/50
 50/970 [>.............................] - ETA: 0s - loss: 26.4959250/970 [======>.......................] - ETA: 0s - loss: 26.7613450/970 [============>.................] - ETA: 0s - loss: 26.6387650/970 [===================>..........] - ETA: 0s - loss: 26.1919850/970 [=========================>....] - ETA: 0s - loss: 26.2193
Epoch 00020: val_loss improved from 25.40105 to 24.27027, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 345us/sample - loss: 26.1970 - val_loss: 24.2703
Epoch 21/50
 50/970 [>.............................] - ETA: 0s - loss: 26.0277250/970 [======>.......................] - ETA: 0s - loss: 25.9947450/970 [============>.................] - ETA: 0s - loss: 26.2528650/970 [===================>..........] - ETA: 0s - loss: 25.9696850/970 [=========================>....] - ETA: 0s - loss: 25.8937
Epoch 00021: val_loss improved from 24.27027 to 24.17199, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 336us/sample - loss: 25.7884 - val_loss: 24.1720
Epoch 22/50
 50/970 [>.............................] - ETA: 0s - loss: 26.0766250/970 [======>.......................] - ETA: 0s - loss: 25.6643450/970 [============>.................] - ETA: 0s - loss: 25.8884650/970 [===================>..........] - ETA: 0s - loss: 25.6364850/970 [=========================>....] - ETA: 0s - loss: 25.6885
Epoch 00022: val_loss improved from 24.17199 to 23.95415, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 334us/sample - loss: 25.8090 - val_loss: 23.9541
Epoch 23/50
 50/970 [>.............................] - ETA: 0s - loss: 24.7025250/970 [======>.......................] - ETA: 0s - loss: 25.5669450/970 [============>.................] - ETA: 0s - loss: 25.4430650/970 [===================>..........] - ETA: 0s - loss: 25.6118850/970 [=========================>....] - ETA: 0s - loss: 25.8015
Epoch 00023: val_loss did not improve from 23.95415
970/970 [==============================] - 0s 310us/sample - loss: 25.6852 - val_loss: 24.2090
Epoch 24/50
 50/970 [>.............................] - ETA: 0s - loss: 24.6379250/970 [======>.......................] - ETA: 0s - loss: 26.5046450/970 [============>.................] - ETA: 0s - loss: 26.5693650/970 [===================>..........] - ETA: 0s - loss: 26.0039850/970 [=========================>....] - ETA: 0s - loss: 25.8813
Epoch 00024: val_loss improved from 23.95415 to 23.79424, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 336us/sample - loss: 25.8267 - val_loss: 23.7942
Epoch 25/50
 50/970 [>.............................] - ETA: 0s - loss: 27.0044250/970 [======>.......................] - ETA: 0s - loss: 26.1268450/970 [============>.................] - ETA: 0s - loss: 26.0738650/970 [===================>..........] - ETA: 0s - loss: 25.8066850/970 [=========================>....] - ETA: 0s - loss: 25.7403
Epoch 00025: val_loss improved from 23.79424 to 23.68151, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 344us/sample - loss: 25.7358 - val_loss: 23.6815
Epoch 26/50
 50/970 [>.............................] - ETA: 0s - loss: 25.6021250/970 [======>.......................] - ETA: 0s - loss: 25.2448450/970 [============>.................] - ETA: 0s - loss: 25.2691650/970 [===================>..........] - ETA: 0s - loss: 25.7949850/970 [=========================>....] - ETA: 0s - loss: 25.8100
Epoch 00026: val_loss did not improve from 23.68151
970/970 [==============================] - 0s 314us/sample - loss: 25.7694 - val_loss: 26.1239
Epoch 27/50
 50/970 [>.............................] - ETA: 0s - loss: 28.3867250/970 [======>.......................] - ETA: 0s - loss: 25.7749450/970 [============>.................] - ETA: 0s - loss: 25.3445650/970 [===================>..........] - ETA: 0s - loss: 25.6327850/970 [=========================>....] - ETA: 0s - loss: 25.6655
Epoch 00027: val_loss improved from 23.68151 to 23.63695, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 344us/sample - loss: 25.7122 - val_loss: 23.6370
Epoch 28/50
 50/970 [>.............................] - ETA: 0s - loss: 26.2736250/970 [======>.......................] - ETA: 0s - loss: 25.5974450/970 [============>.................] - ETA: 0s - loss: 24.9936650/970 [===================>..........] - ETA: 0s - loss: 26.0975850/970 [=========================>....] - ETA: 0s - loss: 26.3073
Epoch 00028: val_loss did not improve from 23.63695
970/970 [==============================] - 0s 309us/sample - loss: 26.2985 - val_loss: 24.0047
Epoch 29/50
 50/970 [>.............................] - ETA: 0s - loss: 30.5406250/970 [======>.......................] - ETA: 0s - loss: 26.6339450/970 [============>.................] - ETA: 0s - loss: 26.0248650/970 [===================>..........] - ETA: 0s - loss: 25.6166850/970 [=========================>....] - ETA: 0s - loss: 25.3916
Epoch 00029: val_loss improved from 23.63695 to 23.47575, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 339us/sample - loss: 25.6736 - val_loss: 23.4758
Epoch 30/50
 50/970 [>.............................] - ETA: 0s - loss: 25.6181250/970 [======>.......................] - ETA: 0s - loss: 26.6514450/970 [============>.................] - ETA: 0s - loss: 26.7368650/970 [===================>..........] - ETA: 0s - loss: 26.2882850/970 [=========================>....] - ETA: 0s - loss: 25.9142
Epoch 00030: val_loss did not improve from 23.47575
970/970 [==============================] - 0s 316us/sample - loss: 25.8165 - val_loss: 24.1597
Epoch 31/50
 50/970 [>.............................] - ETA: 0s - loss: 23.6185250/970 [======>.......................] - ETA: 0s - loss: 26.7848450/970 [============>.................] - ETA: 0s - loss: 26.2909650/970 [===================>..........] - ETA: 0s - loss: 25.9427850/970 [=========================>....] - ETA: 0s - loss: 25.7941
Epoch 00031: val_loss improved from 23.47575 to 23.44716, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 340us/sample - loss: 25.7238 - val_loss: 23.4472
Epoch 32/50
 50/970 [>.............................] - ETA: 0s - loss: 24.2516250/970 [======>.......................] - ETA: 0s - loss: 25.4062450/970 [============>.................] - ETA: 0s - loss: 25.2388650/970 [===================>..........] - ETA: 0s - loss: 25.2793850/970 [=========================>....] - ETA: 0s - loss: 25.2679
Epoch 00032: val_loss did not improve from 23.44716
970/970 [==============================] - 0s 311us/sample - loss: 25.3245 - val_loss: 23.7352
Epoch 33/50
 50/970 [>.............................] - ETA: 0s - loss: 26.3501250/970 [======>.......................] - ETA: 0s - loss: 25.0489450/970 [============>.................] - ETA: 0s - loss: 25.4768650/970 [===================>..........] - ETA: 0s - loss: 25.3233850/970 [=========================>....] - ETA: 0s - loss: 25.1656
Epoch 00033: val_loss improved from 23.44716 to 23.22482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 339us/sample - loss: 25.5094 - val_loss: 23.2248
Epoch 34/50
 50/970 [>.............................] - ETA: 0s - loss: 24.3639250/970 [======>.......................] - ETA: 0s - loss: 24.6302450/970 [============>.................] - ETA: 0s - loss: 25.2490650/970 [===================>..........] - ETA: 0s - loss: 25.2790850/970 [=========================>....] - ETA: 0s - loss: 25.2668
Epoch 00034: val_loss did not improve from 23.22482
970/970 [==============================] - 0s 300us/sample - loss: 25.1953 - val_loss: 23.6459
Epoch 35/50
 50/970 [>.............................] - ETA: 0s - loss: 23.9932250/970 [======>.......................] - ETA: 0s - loss: 25.3154450/970 [============>.................] - ETA: 0s - loss: 25.4080650/970 [===================>..........] - ETA: 0s - loss: 25.3266850/970 [=========================>....] - ETA: 0s - loss: 25.4310
Epoch 00035: val_loss did not improve from 23.22482

Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
970/970 [==============================] - 0s 295us/sample - loss: 25.5521 - val_loss: 23.3650
Epoch 36/50
 50/970 [>.............................] - ETA: 0s - loss: 25.9189250/970 [======>.......................] - ETA: 0s - loss: 25.6014450/970 [============>.................] - ETA: 0s - loss: 25.9066650/970 [===================>..........] - ETA: 0s - loss: 25.5728850/970 [=========================>....] - ETA: 0s - loss: 25.3479
Epoch 00036: val_loss did not improve from 23.22482
970/970 [==============================] - 0s 283us/sample - loss: 25.3108 - val_loss: 23.3689
Epoch 37/50
 50/970 [>.............................] - ETA: 0s - loss: 24.0448250/970 [======>.......................] - ETA: 0s - loss: 24.8486450/970 [============>.................] - ETA: 0s - loss: 25.3095650/970 [===================>..........] - ETA: 0s - loss: 25.2249850/970 [=========================>....] - ETA: 0s - loss: 25.3696
Epoch 00037: val_loss improved from 23.22482 to 23.07629, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 317us/sample - loss: 25.4387 - val_loss: 23.0763
Epoch 38/50
 50/970 [>.............................] - ETA: 0s - loss: 25.6547250/970 [======>.......................] - ETA: 0s - loss: 24.6968500/970 [==============>...............] - ETA: 0s - loss: 24.5267700/970 [====================>.........] - ETA: 0s - loss: 25.0035900/970 [==========================>...] - ETA: 0s - loss: 25.1980
Epoch 00038: val_loss did not improve from 23.07629
970/970 [==============================] - 0s 287us/sample - loss: 25.2096 - val_loss: 23.2850
Epoch 39/50
 50/970 [>.............................] - ETA: 0s - loss: 25.9094250/970 [======>.......................] - ETA: 0s - loss: 25.9848450/970 [============>.................] - ETA: 0s - loss: 25.4952650/970 [===================>..........] - ETA: 0s - loss: 25.0220850/970 [=========================>....] - ETA: 0s - loss: 25.1947
Epoch 00039: val_loss improved from 23.07629 to 23.04707, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 312us/sample - loss: 25.2584 - val_loss: 23.0471
Epoch 40/50
 50/970 [>.............................] - ETA: 0s - loss: 24.5193250/970 [======>.......................] - ETA: 0s - loss: 24.9513450/970 [============>.................] - ETA: 0s - loss: 25.0596650/970 [===================>..........] - ETA: 0s - loss: 25.1359800/970 [=======================>......] - ETA: 0s - loss: 25.2882950/970 [============================>.] - ETA: 0s - loss: 25.3331
Epoch 00040: val_loss did not improve from 23.04707
970/970 [==============================] - 0s 329us/sample - loss: 25.3092 - val_loss: 23.2505
Epoch 41/50
 50/970 [>.............................] - ETA: 0s - loss: 24.7265250/970 [======>.......................] - ETA: 0s - loss: 25.0531450/970 [============>.................] - ETA: 0s - loss: 25.1340650/970 [===================>..........] - ETA: 0s - loss: 25.1894850/970 [=========================>....] - ETA: 0s - loss: 25.1628
Epoch 00041: val_loss did not improve from 23.04707

Epoch 00041: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
970/970 [==============================] - 0s 288us/sample - loss: 25.1638 - val_loss: 23.0952
Epoch 42/50
 50/970 [>.............................] - ETA: 0s - loss: 25.7435250/970 [======>.......................] - ETA: 0s - loss: 25.2517450/970 [============>.................] - ETA: 0s - loss: 24.8815650/970 [===================>..........] - ETA: 0s - loss: 25.1693850/970 [=========================>....] - ETA: 0s - loss: 25.1213
Epoch 00042: val_loss improved from 23.04707 to 23.02437, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 324us/sample - loss: 25.0935 - val_loss: 23.0244
Epoch 43/50
 50/970 [>.............................] - ETA: 0s - loss: 25.7609250/970 [======>.......................] - ETA: 0s - loss: 25.1597450/970 [============>.................] - ETA: 0s - loss: 24.8472650/970 [===================>..........] - ETA: 0s - loss: 25.2106850/970 [=========================>....] - ETA: 0s - loss: 25.1039
Epoch 00043: val_loss did not improve from 23.02437
970/970 [==============================] - 0s 299us/sample - loss: 25.0484 - val_loss: 23.0340
Epoch 44/50
 50/970 [>.............................] - ETA: 0s - loss: 25.7968250/970 [======>.......................] - ETA: 0s - loss: 25.3409450/970 [============>.................] - ETA: 0s - loss: 25.3025650/970 [===================>..........] - ETA: 0s - loss: 25.4598850/970 [=========================>....] - ETA: 0s - loss: 25.3090
Epoch 00044: val_loss did not improve from 23.02437

Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
970/970 [==============================] - 0s 290us/sample - loss: 25.3111 - val_loss: 23.1464
Epoch 45/50
 50/970 [>.............................] - ETA: 0s - loss: 24.4894250/970 [======>.......................] - ETA: 0s - loss: 25.1331450/970 [============>.................] - ETA: 0s - loss: 24.8907650/970 [===================>..........] - ETA: 0s - loss: 24.9494850/970 [=========================>....] - ETA: 0s - loss: 24.9119
Epoch 00045: val_loss did not improve from 23.02437
970/970 [==============================] - 0s 291us/sample - loss: 25.0698 - val_loss: 23.0359
Epoch 46/50
 50/970 [>.............................] - ETA: 0s - loss: 24.8841250/970 [======>.......................] - ETA: 0s - loss: 24.9295450/970 [============>.................] - ETA: 0s - loss: 24.8607650/970 [===================>..........] - ETA: 0s - loss: 25.0400850/970 [=========================>....] - ETA: 0s - loss: 25.0210
Epoch 00046: val_loss improved from 23.02437 to 23.01572, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 335us/sample - loss: 25.0442 - val_loss: 23.0157
Epoch 47/50
 50/970 [>.............................] - ETA: 0s - loss: 24.8466250/970 [======>.......................] - ETA: 0s - loss: 25.0240450/970 [============>.................] - ETA: 0s - loss: 24.9406650/970 [===================>..........] - ETA: 0s - loss: 25.0778850/970 [=========================>....] - ETA: 0s - loss: 25.0422
Epoch 00047: val_loss did not improve from 23.01572
970/970 [==============================] - 0s 298us/sample - loss: 25.0103 - val_loss: 23.0390
Epoch 48/50
 50/970 [>.............................] - ETA: 0s - loss: 25.6346250/970 [======>.......................] - ETA: 0s - loss: 25.1991450/970 [============>.................] - ETA: 0s - loss: 25.1444650/970 [===================>..........] - ETA: 0s - loss: 25.0371850/970 [=========================>....] - ETA: 0s - loss: 25.0222
Epoch 00048: val_loss did not improve from 23.01572

Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
970/970 [==============================] - 0s 298us/sample - loss: 25.0677 - val_loss: 23.0309
Epoch 49/50
 50/970 [>.............................] - ETA: 0s - loss: 24.2812250/970 [======>.......................] - ETA: 0s - loss: 24.5717450/970 [============>.................] - ETA: 0s - loss: 25.0518650/970 [===================>..........] - ETA: 0s - loss: 25.0665850/970 [=========================>....] - ETA: 0s - loss: 24.9998
Epoch 00049: val_loss did not improve from 23.01572
970/970 [==============================] - 0s 301us/sample - loss: 24.9985 - val_loss: 23.0216
Epoch 50/50
 50/970 [>.............................] - ETA: 0s - loss: 24.3442250/970 [======>.......................] - ETA: 0s - loss: 24.0109450/970 [============>.................] - ETA: 0s - loss: 24.2253650/970 [===================>..........] - ETA: 0s - loss: 24.6531850/970 [=========================>....] - ETA: 0s - loss: 24.8724
Epoch 00050: val_loss improved from 23.01572 to 23.00779, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
970/970 [==============================] - 0s 331us/sample - loss: 25.0195 - val_loss: 23.0078
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 1091 samples, validate on 121 samples
Epoch 1/50
  50/1091 [>.............................] - ETA: 15s - loss: nan 150/1091 [===>..........................] - ETA: 5s - loss: nan  350/1091 [========>.....................] - ETA: 1s - loss: nan 550/1091 [==============>...............] - ETA: 0s - loss: nan 750/1091 [===================>..........] - ETA: 0s - loss: nan 950/1091 [=========================>....] - ETA: 0s - loss: nan
Epoch 00001: val_loss did not improve from inf
1091/1091 [==============================] - 2s 2ms/sample - loss: nan - val_loss: nan
Epoch 2/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1091/1091 [==============================] - 0s 294us/sample - loss: nan - val_loss: nan
Epoch 3/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
1091/1091 [==============================] - 0s 293us/sample - loss: nan - val_loss: nan
Epoch 4/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1091/1091 [==============================] - 0s 285us/sample - loss: nan - val_loss: nan
Epoch 5/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
1091/1091 [==============================] - 0s 291us/sample - loss: nan - val_loss: nan
Epoch 6/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1091/1091 [==============================] - 0s 290us/sample - loss: nan - val_loss: nan
Epoch 7/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
1091/1091 [==============================] - 0s 293us/sample - loss: nan - val_loss: nan
Epoch 8/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1091/1091 [==============================] - 0s 291us/sample - loss: nan - val_loss: nan
Epoch 9/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
1091/1091 [==============================] - 0s 288us/sample - loss: nan - val_loss: nan
Epoch 10/50
  50/1091 [>.............................] - ETA: 0s - loss: nan 250/1091 [=====>........................] - ETA: 0s - loss: nan 450/1091 [===========>..................] - ETA: 0s - loss: nan 650/1091 [================>.............] - ETA: 0s - loss: nan 850/1091 [======================>.......] - ETA: 0s - loss: nan1050/1091 [===========================>..] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1091/1091 [==============================] - 0s 291us/sample - loss: nan - val_loss: nan
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00010: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8e7bc67ba8> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b8e7bc74a20> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bc74e10> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e7bc74d30> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7bc78f60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bc78828> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e7be98cc0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7be98c18> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8e7bea1208> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bec3f60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bece9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bed5710> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b8e7bee2f98> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b8e7beec2e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b8e7bef6cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bef6be0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7bf107f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bf10a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7bf10be0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bf199b0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b8e7bf19b38> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8e7bc67ba8> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b8e7bc74a20> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bc74e10> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e7bc74d30> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7bc78f60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bc78828> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e7be98cc0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7be98c18> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8e7bea1208> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bec3f60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bece9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bed5710> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b8e7bee2f98> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b8e7beec2e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b8e7bef6cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bef6be0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7bf107f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bf10a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e7bf10be0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bf199b0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b8e7bf19b38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e2ff57320> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7bf4ef60> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 105.3139
Epoch 00001: val_loss improved from inf to 46.34258, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 9ms/sample - loss: 84.0215 - val_loss: 46.3426
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.1013
Epoch 00002: val_loss improved from 46.34258 to 34.02689, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 724us/sample - loss: 37.0833 - val_loss: 34.0269
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 31.7588
Epoch 00003: val_loss did not improve from 34.02689
121/121 [==============================] - 0s 528us/sample - loss: 33.4683 - val_loss: 36.3560
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 35.8481
Epoch 00004: val_loss improved from 34.02689 to 28.97027, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 687us/sample - loss: 32.3171 - val_loss: 28.9703
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.0973
Epoch 00005: val_loss improved from 28.97027 to 26.31678, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 671us/sample - loss: 26.3250 - val_loss: 26.3168
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.6201
Epoch 00006: val_loss improved from 26.31678 to 25.66288, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 681us/sample - loss: 26.5460 - val_loss: 25.6629
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6414
Epoch 00007: val_loss did not improve from 25.66288
121/121 [==============================] - 0s 541us/sample - loss: 25.4863 - val_loss: 26.5637
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8092
Epoch 00008: val_loss improved from 25.66288 to 25.43974, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 667us/sample - loss: 25.5354 - val_loss: 25.4397
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9240
Epoch 00009: val_loss improved from 25.43974 to 25.21979, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 680us/sample - loss: 25.1743 - val_loss: 25.2198
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.3798
Epoch 00010: val_loss did not improve from 25.21979
121/121 [==============================] - 0s 491us/sample - loss: 24.8213 - val_loss: 25.3204
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2877
Epoch 00011: val_loss improved from 25.21979 to 24.86806, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 660us/sample - loss: 24.6681 - val_loss: 24.8681
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0879
Epoch 00012: val_loss improved from 24.86806 to 24.85431, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 732us/sample - loss: 24.4885 - val_loss: 24.8543
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3853
Epoch 00013: val_loss improved from 24.85431 to 24.68130, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 682us/sample - loss: 24.3410 - val_loss: 24.6813
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.9981
Epoch 00014: val_loss improved from 24.68130 to 24.61100, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 729us/sample - loss: 24.2423 - val_loss: 24.6110
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9429
Epoch 00015: val_loss improved from 24.61100 to 24.51037, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 24.1688 - val_loss: 24.5104
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8599
Epoch 00016: val_loss improved from 24.51037 to 24.39849, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 660us/sample - loss: 24.0769 - val_loss: 24.3985
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1355
Epoch 00017: val_loss did not improve from 24.39849
121/121 [==============================] - 0s 524us/sample - loss: 24.0254 - val_loss: 24.5809
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0771
Epoch 00018: val_loss improved from 24.39849 to 24.28086, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 747us/sample - loss: 24.0392 - val_loss: 24.2809
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9460
Epoch 00019: val_loss improved from 24.28086 to 24.24296, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 669us/sample - loss: 23.9743 - val_loss: 24.2430
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4849
Epoch 00020: val_loss improved from 24.24296 to 24.20829, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 674us/sample - loss: 23.8711 - val_loss: 24.2083
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.5631
Epoch 00021: val_loss did not improve from 24.20829
121/121 [==============================] - 0s 521us/sample - loss: 23.9123 - val_loss: 24.2872
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7749
Epoch 00022: val_loss improved from 24.20829 to 24.16744, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 689us/sample - loss: 23.8319 - val_loss: 24.1674
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3192
Epoch 00023: val_loss improved from 24.16744 to 24.11918, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 678us/sample - loss: 23.7909 - val_loss: 24.1192
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4842
Epoch 00024: val_loss did not improve from 24.11918
121/121 [==============================] - 0s 509us/sample - loss: 23.7481 - val_loss: 24.1330
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6441
Epoch 00025: val_loss did not improve from 24.11918

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 499us/sample - loss: 23.9863 - val_loss: 24.3264
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9521
Epoch 00026: val_loss improved from 24.11918 to 24.03695, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 651us/sample - loss: 23.7977 - val_loss: 24.0369
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0744
Epoch 00027: val_loss improved from 24.03695 to 24.01670, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 685us/sample - loss: 23.7145 - val_loss: 24.0167
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.7374
Epoch 00028: val_loss did not improve from 24.01670
121/121 [==============================] - 0s 528us/sample - loss: 23.6988 - val_loss: 24.0432
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0326
Epoch 00029: val_loss did not improve from 24.01670

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 531us/sample - loss: 23.6994 - val_loss: 24.0769
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5046
Epoch 00030: val_loss did not improve from 24.01670
121/121 [==============================] - 0s 512us/sample - loss: 23.6951 - val_loss: 24.0743
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2193
Epoch 00031: val_loss did not improve from 24.01670

Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 498us/sample - loss: 23.6825 - val_loss: 24.0797
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7537
Epoch 00032: val_loss did not improve from 24.01670
121/121 [==============================] - 0s 514us/sample - loss: 23.6789 - val_loss: 24.0331
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5220
Epoch 00033: val_loss improved from 24.01670 to 24.00229, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 667us/sample - loss: 23.6705 - val_loss: 24.0023
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5975
Epoch 00034: val_loss improved from 24.00229 to 24.00125, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 700us/sample - loss: 23.6745 - val_loss: 24.0013
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0487
Epoch 00035: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 520us/sample - loss: 23.6711 - val_loss: 24.0107
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5216
Epoch 00036: val_loss did not improve from 24.00125

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 506us/sample - loss: 23.6656 - val_loss: 24.0115
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1079
Epoch 00037: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 535us/sample - loss: 23.6646 - val_loss: 24.0120
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9074
Epoch 00038: val_loss did not improve from 24.00125

Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 532us/sample - loss: 23.6649 - val_loss: 24.0128
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.7384
Epoch 00039: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 488us/sample - loss: 23.6630 - val_loss: 24.0179
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9945
Epoch 00040: val_loss did not improve from 24.00125

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 505us/sample - loss: 23.6619 - val_loss: 24.0254
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1872
Epoch 00041: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 509us/sample - loss: 23.6601 - val_loss: 24.0324
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4445
Epoch 00042: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 505us/sample - loss: 23.6599 - val_loss: 24.0361
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6894
Epoch 00043: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 498us/sample - loss: 23.6597 - val_loss: 24.0404
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3877
Epoch 00044: val_loss did not improve from 24.00125
121/121 [==============================] - 0s 488us/sample - loss: 23.6598 - val_loss: 24.0397
Epoch 00044: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8e9248fb38> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b8e924a07f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e924a0eb8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e924a09b0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e924a06d8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e924a5940> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e928cbdd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e928cbd30> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8e928d4320> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92902278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92902ac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e9290c828> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b8e9291c4a8> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b8e9291c400> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b8e92929e10> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92929cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e92943908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92943b70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e92943cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e9294bac8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b8e9294bc50> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8e9248fb38> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b8e924a07f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e924a0eb8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e924a09b0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e924a06d8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e924a5940> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8e928cbdd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e928cbd30> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8e928d4320> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92902278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92902ac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e9290c828> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b8e9291c4a8> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b8e9291c400> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b8e92929e10> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92929cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e92943908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92943b70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8e92943cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e9294bac8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b8e9294bc50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e7b92a588> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8e92989438> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 150.0182
Epoch 00001: val_loss improved from inf to 75.57624, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 7ms/sample - loss: 121.1821 - val_loss: 75.5762
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 82.3787
Epoch 00002: val_loss improved from 75.57624 to 47.44406, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 749us/sample - loss: 64.9295 - val_loss: 47.4441
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 50.0424
Epoch 00003: val_loss improved from 47.44406 to 39.85881, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 687us/sample - loss: 46.0432 - val_loss: 39.8588
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.8996
Epoch 00004: val_loss improved from 39.85881 to 34.72929, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 658us/sample - loss: 39.6998 - val_loss: 34.7293
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.4517
Epoch 00005: val_loss improved from 34.72929 to 28.22411, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 673us/sample - loss: 33.4099 - val_loss: 28.2241
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 30.8402
Epoch 00006: val_loss improved from 28.22411 to 26.78587, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 726us/sample - loss: 27.3254 - val_loss: 26.7859
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3501
Epoch 00007: val_loss did not improve from 26.78587
121/121 [==============================] - 0s 558us/sample - loss: 27.1895 - val_loss: 27.0656
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.7116
Epoch 00008: val_loss improved from 26.78587 to 25.90606, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 26.6560 - val_loss: 25.9061
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.9005
Epoch 00009: val_loss improved from 25.90606 to 25.71900, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 658us/sample - loss: 25.9943 - val_loss: 25.7190
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.1285
Epoch 00010: val_loss improved from 25.71900 to 25.61069, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 656us/sample - loss: 26.0168 - val_loss: 25.6107
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 29.6171
Epoch 00011: val_loss improved from 25.61069 to 25.34563, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 701us/sample - loss: 25.6919 - val_loss: 25.3456
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6225
Epoch 00012: val_loss improved from 25.34563 to 25.22336, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 662us/sample - loss: 25.5120 - val_loss: 25.2234
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8381
Epoch 00013: val_loss improved from 25.22336 to 25.00487, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 660us/sample - loss: 25.2765 - val_loss: 25.0049
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5973
Epoch 00014: val_loss improved from 25.00487 to 24.84662, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 25.2179 - val_loss: 24.8466
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0407
Epoch 00015: val_loss improved from 24.84662 to 24.80836, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 702us/sample - loss: 25.0070 - val_loss: 24.8084
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.8372
Epoch 00016: val_loss improved from 24.80836 to 24.60701, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 677us/sample - loss: 24.8412 - val_loss: 24.6070
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7673
Epoch 00017: val_loss improved from 24.60701 to 24.45778, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 668us/sample - loss: 24.7516 - val_loss: 24.4578
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.6367
Epoch 00018: val_loss improved from 24.45778 to 24.35090, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 682us/sample - loss: 24.5949 - val_loss: 24.3509
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9752
Epoch 00019: val_loss improved from 24.35090 to 24.22581, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 680us/sample - loss: 24.4710 - val_loss: 24.2258
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4620
Epoch 00020: val_loss improved from 24.22581 to 24.21137, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 709us/sample - loss: 24.3847 - val_loss: 24.2114
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.5692
Epoch 00021: val_loss improved from 24.21137 to 24.02507, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 666us/sample - loss: 24.5025 - val_loss: 24.0251
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1544
Epoch 00022: val_loss improved from 24.02507 to 23.94009, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 657us/sample - loss: 24.2639 - val_loss: 23.9401
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4946
Epoch 00023: val_loss improved from 23.94009 to 23.91769, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 657us/sample - loss: 24.1877 - val_loss: 23.9177
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0571
Epoch 00024: val_loss improved from 23.91769 to 23.82810, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 714us/sample - loss: 24.0732 - val_loss: 23.8281
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1790
Epoch 00025: val_loss improved from 23.82810 to 23.78572, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 702us/sample - loss: 23.9852 - val_loss: 23.7857
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.7736
Epoch 00026: val_loss improved from 23.78572 to 23.73886, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 735us/sample - loss: 23.9294 - val_loss: 23.7389
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5129
Epoch 00027: val_loss improved from 23.73886 to 23.66094, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 695us/sample - loss: 23.9157 - val_loss: 23.6609
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6020
Epoch 00028: val_loss improved from 23.66094 to 23.61999, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 732us/sample - loss: 23.9138 - val_loss: 23.6200
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.5558
Epoch 00029: val_loss improved from 23.61999 to 23.55757, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 665us/sample - loss: 23.8425 - val_loss: 23.5576
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8969
Epoch 00030: val_loss did not improve from 23.55757
121/121 [==============================] - 0s 502us/sample - loss: 23.7508 - val_loss: 23.6270
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9908
Epoch 00031: val_loss improved from 23.55757 to 23.45918, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 668us/sample - loss: 23.8258 - val_loss: 23.4592
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6350
Epoch 00032: val_loss improved from 23.45918 to 23.41153, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 696us/sample - loss: 23.6813 - val_loss: 23.4115
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2223
Epoch 00033: val_loss improved from 23.41153 to 23.37768, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 661us/sample - loss: 23.6135 - val_loss: 23.3777
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9655
Epoch 00034: val_loss improved from 23.37768 to 23.31997, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 662us/sample - loss: 23.5514 - val_loss: 23.3200
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1232
Epoch 00035: val_loss improved from 23.31997 to 23.27781, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 667us/sample - loss: 23.5519 - val_loss: 23.2778
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2413
Epoch 00036: val_loss did not improve from 23.27781
121/121 [==============================] - 0s 484us/sample - loss: 23.4998 - val_loss: 23.3025
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3719
Epoch 00037: val_loss improved from 23.27781 to 23.23187, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 646us/sample - loss: 23.4698 - val_loss: 23.2319
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1057
Epoch 00038: val_loss improved from 23.23187 to 23.21940, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 717us/sample - loss: 23.4246 - val_loss: 23.2194
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9270
Epoch 00039: val_loss improved from 23.21940 to 23.20499, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 711us/sample - loss: 23.4116 - val_loss: 23.2050
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1150
Epoch 00040: val_loss improved from 23.20499 to 23.15497, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 23.3814 - val_loss: 23.1550
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0427
Epoch 00041: val_loss improved from 23.15497 to 23.14020, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 690us/sample - loss: 23.4258 - val_loss: 23.1402
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4114
Epoch 00042: val_loss did not improve from 23.14020
121/121 [==============================] - 0s 490us/sample - loss: 23.3429 - val_loss: 23.1500
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1419
Epoch 00043: val_loss did not improve from 23.14020

Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 509us/sample - loss: 23.3379 - val_loss: 23.2038
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.6438
Epoch 00044: val_loss improved from 23.14020 to 23.11961, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 658us/sample - loss: 23.3538 - val_loss: 23.1196
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6393
Epoch 00045: val_loss improved from 23.11961 to 23.09600, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 723us/sample - loss: 23.3244 - val_loss: 23.0960
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8082
Epoch 00046: val_loss did not improve from 23.09600
121/121 [==============================] - 0s 605us/sample - loss: 23.3515 - val_loss: 23.1808
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9584
Epoch 00047: val_loss improved from 23.09600 to 23.08965, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 734us/sample - loss: 23.2832 - val_loss: 23.0896
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4502
Epoch 00048: val_loss did not improve from 23.08965
121/121 [==============================] - 0s 528us/sample - loss: 23.2954 - val_loss: 23.1186
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6572
Epoch 00049: val_loss improved from 23.08965 to 23.06771, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 693us/sample - loss: 23.3161 - val_loss: 23.0677
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8485
Epoch 00050: val_loss did not improve from 23.06771
121/121 [==============================] - 0s 509us/sample - loss: 23.2838 - val_loss: 23.1791
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.5045
Epoch 00051: val_loss did not improve from 23.06771

Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 510us/sample - loss: 23.3248 - val_loss: 23.0833
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0523
Epoch 00052: val_loss improved from 23.06771 to 23.06194, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 673us/sample - loss: 23.2618 - val_loss: 23.0619
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.2506
Epoch 00053: val_loss improved from 23.06194 to 23.06096, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 724us/sample - loss: 23.2624 - val_loss: 23.0610
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4606
Epoch 00054: val_loss did not improve from 23.06096
121/121 [==============================] - 0s 577us/sample - loss: 23.2590 - val_loss: 23.0635
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.2289
Epoch 00055: val_loss did not improve from 23.06096

Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 562us/sample - loss: 23.2624 - val_loss: 23.0728
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7186
Epoch 00056: val_loss did not improve from 23.06096
121/121 [==============================] - 0s 514us/sample - loss: 23.2624 - val_loss: 23.0699
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8001
Epoch 00057: val_loss improved from 23.06096 to 23.05550, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 719us/sample - loss: 23.2624 - val_loss: 23.0555
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1853
Epoch 00058: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 525us/sample - loss: 23.2568 - val_loss: 23.0713
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4357
Epoch 00059: val_loss did not improve from 23.05550

Epoch 00059: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 517us/sample - loss: 23.2680 - val_loss: 23.0979
Epoch 60/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2932
Epoch 00060: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 523us/sample - loss: 23.2695 - val_loss: 23.0884
Epoch 61/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7283
Epoch 00061: val_loss did not improve from 23.05550

Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 562us/sample - loss: 23.2645 - val_loss: 23.0799
Epoch 62/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1523
Epoch 00062: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 523us/sample - loss: 23.2611 - val_loss: 23.0754
Epoch 63/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3497
Epoch 00063: val_loss did not improve from 23.05550

Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 526us/sample - loss: 23.2585 - val_loss: 23.0698
Epoch 64/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5883
Epoch 00064: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 532us/sample - loss: 23.2567 - val_loss: 23.0703
Epoch 65/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4450
Epoch 00065: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 526us/sample - loss: 23.2565 - val_loss: 23.0684
Epoch 66/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9527
Epoch 00066: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 506us/sample - loss: 23.2544 - val_loss: 23.0639
Epoch 67/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7189
Epoch 00067: val_loss did not improve from 23.05550
121/121 [==============================] - 0s 477us/sample - loss: 23.2539 - val_loss: 23.0606
Epoch 00067: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
Traceback (most recent call last):
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "Run_CNN_FineTune_ByYear.py", line 347, in <module>
    model.load_weights(Wsave_name)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 234, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1187, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 99, in NewCheckpointReader
    error_translator(e)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2009/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
