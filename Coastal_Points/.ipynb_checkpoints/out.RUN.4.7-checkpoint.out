2020-11-09 19:53:38.151397: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 19:53:38.159134: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 19:53:38.159328: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b24298f750 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 19:53:38.159350: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 19:53:38.161201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 19:53:38.286576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:53:38.302420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:53:38.458176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:53:38.517471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:53:38.634452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:53:38.723695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:53:38.784702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:53:38.945997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:53:38.956444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:53:39.021497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:53:39.283315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:53:39.283387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:53:39.283424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:53:39.324784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 19:53:39.420196: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b24370ac60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 19:53:39.420265: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 19:53:39.423427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:53:39.423529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:53:39.423550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:53:39.423572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:53:39.427761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:53:39.427781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:53:39.427797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:53:39.427813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:53:39.430919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:53:39.430962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:53:39.430976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:53:39.430986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:53:39.438718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 19:53:39.442954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:53:39.443053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:53:39.443073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:53:39.443090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:53:39.443106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:53:39.443121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:53:39.443137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:53:39.443153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:53:39.446157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:53:39.446196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:53:39.446209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:53:39.446218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:53:39.449251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F018
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
Training on
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc
['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 162.5202, 155.5039
Mean and standard deviation for "p_sfc" = 984.8862, 62.0783
Mean and standard deviation for "u_tr_p" = 11.9893, 12.4178
Mean and standard deviation for "v_tr_p" = 1.1301, 13.4873
Mean and standard deviation for "Z_p" = 5586.4391, 204.0330
Mean and standard deviation for "IWV" = 13.8156, 8.0162
2020-11-09 19:53:47.503683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:53:47.518639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:53:47.518682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:53:47.518701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:53:47.518718: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:53:47.518735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:53:47.518752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:53:47.518771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:53:47.522062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:53:47.540455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:53:47.540599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:53:47.540623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:53:47.540640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:53:47.540657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:53:47.540675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:53:47.540691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:53:47.540708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:53:47.548585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:53:47.548655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:53:47.548671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:53:47.548682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:53:47.552843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 19:53:52.811803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:53:54.939587: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 19:53:54.955688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 203.8212, 179.8134
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 728 samples, validate on 121 samples
Epoch 1/50
 50/728 [=>............................] - ETA: 1:04 - loss: 150.6200100/728 [===>..........................] - ETA: 30s - loss: 155.4305 300/728 [===========>..................] - ETA: 6s - loss: 151.9356 500/728 [===================>..........] - ETA: 2s - loss: 151.7444700/728 [===========================>..] - ETA: 0s - loss: 144.9943
Epoch 00001: val_loss improved from inf to 104.34104, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 6s 8ms/sample - loss: 143.4726 - val_loss: 104.3410
Epoch 2/50
 50/728 [=>............................] - ETA: 0s - loss: 96.4252250/728 [=========>....................] - ETA: 0s - loss: 74.5023450/728 [=================>............] - ETA: 0s - loss: 66.0293650/728 [=========================>....] - ETA: 0s - loss: 62.1211
Epoch 00002: val_loss improved from 104.34104 to 49.00052, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 437us/sample - loss: 60.1985 - val_loss: 49.0005
Epoch 3/50
 50/728 [=>............................] - ETA: 0s - loss: 39.6507250/728 [=========>....................] - ETA: 0s - loss: 38.3465450/728 [=================>............] - ETA: 0s - loss: 38.2468650/728 [=========================>....] - ETA: 0s - loss: 36.9763
Epoch 00003: val_loss did not improve from 49.00052
728/728 [==============================] - 0s 326us/sample - loss: 36.9474 - val_loss: 53.8755
Epoch 4/50
 50/728 [=>............................] - ETA: 0s - loss: 31.6486250/728 [=========>....................] - ETA: 0s - loss: 32.2712450/728 [=================>............] - ETA: 0s - loss: 32.2789650/728 [=========================>....] - ETA: 0s - loss: 31.5371
Epoch 00004: val_loss did not improve from 49.00052

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
728/728 [==============================] - 0s 322us/sample - loss: 31.3715 - val_loss: 59.7997
Epoch 5/50
 50/728 [=>............................] - ETA: 0s - loss: 28.6258250/728 [=========>....................] - ETA: 0s - loss: 29.3744450/728 [=================>............] - ETA: 0s - loss: 28.6004650/728 [=========================>....] - ETA: 0s - loss: 28.6654
Epoch 00005: val_loss did not improve from 49.00052
728/728 [==============================] - 0s 330us/sample - loss: 28.4513 - val_loss: 56.6185
Epoch 6/50
 50/728 [=>............................] - ETA: 0s - loss: 29.7674250/728 [=========>....................] - ETA: 0s - loss: 27.3790450/728 [=================>............] - ETA: 0s - loss: 26.7587650/728 [=========================>....] - ETA: 0s - loss: 26.7440
Epoch 00006: val_loss did not improve from 49.00052

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
728/728 [==============================] - 0s 317us/sample - loss: 26.5979 - val_loss: 54.3815
Epoch 7/50
 50/728 [=>............................] - ETA: 0s - loss: 26.2522250/728 [=========>....................] - ETA: 0s - loss: 26.0999450/728 [=================>............] - ETA: 0s - loss: 25.8562650/728 [=========================>....] - ETA: 0s - loss: 25.7385
Epoch 00007: val_loss did not improve from 49.00052
728/728 [==============================] - 0s 332us/sample - loss: 25.5302 - val_loss: 52.8313
Epoch 8/50
 50/728 [=>............................] - ETA: 0s - loss: 23.7174250/728 [=========>....................] - ETA: 0s - loss: 24.5965450/728 [=================>............] - ETA: 0s - loss: 24.8136650/728 [=========================>....] - ETA: 0s - loss: 24.8266
Epoch 00008: val_loss improved from 49.00052 to 47.30604, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 381us/sample - loss: 24.8912 - val_loss: 47.3060
Epoch 9/50
 50/728 [=>............................] - ETA: 0s - loss: 24.6042250/728 [=========>....................] - ETA: 0s - loss: 25.3736450/728 [=================>............] - ETA: 0s - loss: 24.9400650/728 [=========================>....] - ETA: 0s - loss: 24.6771
Epoch 00009: val_loss did not improve from 47.30604
728/728 [==============================] - 0s 321us/sample - loss: 24.8492 - val_loss: 48.5126
Epoch 10/50
 50/728 [=>............................] - ETA: 0s - loss: 23.8867250/728 [=========>....................] - ETA: 0s - loss: 23.6490450/728 [=================>............] - ETA: 0s - loss: 24.1579650/728 [=========================>....] - ETA: 0s - loss: 24.0697
Epoch 00010: val_loss improved from 47.30604 to 41.30751, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 393us/sample - loss: 24.0279 - val_loss: 41.3075
Epoch 11/50
 50/728 [=>............................] - ETA: 0s - loss: 23.9513250/728 [=========>....................] - ETA: 0s - loss: 23.4774450/728 [=================>............] - ETA: 0s - loss: 23.7486650/728 [=========================>....] - ETA: 0s - loss: 23.6683
Epoch 00011: val_loss improved from 41.30751 to 39.73590, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 374us/sample - loss: 23.5799 - val_loss: 39.7359
Epoch 12/50
 50/728 [=>............................] - ETA: 0s - loss: 22.2490250/728 [=========>....................] - ETA: 0s - loss: 23.0060450/728 [=================>............] - ETA: 0s - loss: 23.2010600/728 [=======================>......] - ETA: 0s - loss: 23.2062
Epoch 00012: val_loss improved from 39.73590 to 36.77232, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 390us/sample - loss: 23.2163 - val_loss: 36.7723
Epoch 13/50
 50/728 [=>............................] - ETA: 0s - loss: 24.8880250/728 [=========>....................] - ETA: 0s - loss: 23.4518450/728 [=================>............] - ETA: 0s - loss: 22.9519650/728 [=========================>....] - ETA: 0s - loss: 22.9952
Epoch 00013: val_loss improved from 36.77232 to 35.78057, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 381us/sample - loss: 23.0739 - val_loss: 35.7806
Epoch 14/50
 50/728 [=>............................] - ETA: 0s - loss: 21.8356250/728 [=========>....................] - ETA: 0s - loss: 22.4854450/728 [=================>............] - ETA: 0s - loss: 22.5552650/728 [=========================>....] - ETA: 0s - loss: 22.5183
Epoch 00014: val_loss improved from 35.78057 to 31.99336, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 368us/sample - loss: 22.6454 - val_loss: 31.9934
Epoch 15/50
 50/728 [=>............................] - ETA: 0s - loss: 22.2163250/728 [=========>....................] - ETA: 0s - loss: 22.2373450/728 [=================>............] - ETA: 0s - loss: 22.1423650/728 [=========================>....] - ETA: 0s - loss: 22.2293
Epoch 00015: val_loss improved from 31.99336 to 29.01666, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 381us/sample - loss: 22.3597 - val_loss: 29.0167
Epoch 16/50
 50/728 [=>............................] - ETA: 0s - loss: 21.9085250/728 [=========>....................] - ETA: 0s - loss: 22.3101450/728 [=================>............] - ETA: 0s - loss: 22.2211650/728 [=========================>....] - ETA: 0s - loss: 22.3053
Epoch 00016: val_loss did not improve from 29.01666
728/728 [==============================] - 0s 344us/sample - loss: 22.2376 - val_loss: 29.1330
Epoch 17/50
 50/728 [=>............................] - ETA: 0s - loss: 22.2030250/728 [=========>....................] - ETA: 0s - loss: 21.7575450/728 [=================>............] - ETA: 0s - loss: 22.2019650/728 [=========================>....] - ETA: 0s - loss: 22.1106
Epoch 00017: val_loss improved from 29.01666 to 27.57003, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 382us/sample - loss: 22.1137 - val_loss: 27.5700
Epoch 18/50
 50/728 [=>............................] - ETA: 0s - loss: 20.3290250/728 [=========>....................] - ETA: 0s - loss: 21.3869450/728 [=================>............] - ETA: 0s - loss: 21.7912650/728 [=========================>....] - ETA: 0s - loss: 22.2240
Epoch 00018: val_loss improved from 27.57003 to 23.97887, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 367us/sample - loss: 22.2264 - val_loss: 23.9789
Epoch 19/50
 50/728 [=>............................] - ETA: 0s - loss: 22.1464250/728 [=========>....................] - ETA: 0s - loss: 21.3062450/728 [=================>............] - ETA: 0s - loss: 22.1299650/728 [=========================>....] - ETA: 0s - loss: 21.8965
Epoch 00019: val_loss improved from 23.97887 to 23.68859, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 363us/sample - loss: 22.0100 - val_loss: 23.6886
Epoch 20/50
 50/728 [=>............................] - ETA: 0s - loss: 21.2182250/728 [=========>....................] - ETA: 0s - loss: 21.7301450/728 [=================>............] - ETA: 0s - loss: 21.5266650/728 [=========================>....] - ETA: 0s - loss: 21.8858
Epoch 00020: val_loss did not improve from 23.68859
728/728 [==============================] - 0s 344us/sample - loss: 21.7562 - val_loss: 24.1466
Epoch 21/50
 50/728 [=>............................] - ETA: 0s - loss: 21.0486250/728 [=========>....................] - ETA: 0s - loss: 20.9045450/728 [=================>............] - ETA: 0s - loss: 20.9507650/728 [=========================>....] - ETA: 0s - loss: 21.0525
Epoch 00021: val_loss improved from 23.68859 to 22.48270, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 388us/sample - loss: 21.0055 - val_loss: 22.4827
Epoch 22/50
 50/728 [=>............................] - ETA: 0s - loss: 20.6764250/728 [=========>....................] - ETA: 0s - loss: 20.9112400/728 [===============>..............] - ETA: 0s - loss: 20.9738600/728 [=======================>......] - ETA: 0s - loss: 20.9018
Epoch 00022: val_loss did not improve from 22.48270
728/728 [==============================] - 0s 351us/sample - loss: 20.9272 - val_loss: 23.0635
Epoch 23/50
 50/728 [=>............................] - ETA: 0s - loss: 23.0309250/728 [=========>....................] - ETA: 0s - loss: 21.4003450/728 [=================>............] - ETA: 0s - loss: 21.0394650/728 [=========================>....] - ETA: 0s - loss: 20.9525
Epoch 00023: val_loss improved from 22.48270 to 21.88983, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 390us/sample - loss: 20.9378 - val_loss: 21.8898
Epoch 24/50
 50/728 [=>............................] - ETA: 0s - loss: 19.8889250/728 [=========>....................] - ETA: 0s - loss: 20.2114450/728 [=================>............] - ETA: 0s - loss: 20.6585650/728 [=========================>....] - ETA: 0s - loss: 20.7610
Epoch 00024: val_loss improved from 21.88983 to 21.17428, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 385us/sample - loss: 20.8308 - val_loss: 21.1743
Epoch 25/50
 50/728 [=>............................] - ETA: 0s - loss: 19.7526250/728 [=========>....................] - ETA: 0s - loss: 19.7023450/728 [=================>............] - ETA: 0s - loss: 20.7954650/728 [=========================>....] - ETA: 0s - loss: 20.8085
Epoch 00025: val_loss improved from 21.17428 to 20.03714, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 376us/sample - loss: 20.7739 - val_loss: 20.0371
Epoch 26/50
 50/728 [=>............................] - ETA: 0s - loss: 21.1618250/728 [=========>....................] - ETA: 0s - loss: 20.3529450/728 [=================>............] - ETA: 0s - loss: 20.2565650/728 [=========================>....] - ETA: 0s - loss: 20.3718
Epoch 00026: val_loss did not improve from 20.03714
728/728 [==============================] - 0s 322us/sample - loss: 20.4502 - val_loss: 20.6872
Epoch 27/50
 50/728 [=>............................] - ETA: 0s - loss: 19.1755250/728 [=========>....................] - ETA: 0s - loss: 20.5054450/728 [=================>............] - ETA: 0s - loss: 20.3543650/728 [=========================>....] - ETA: 0s - loss: 20.3667
Epoch 00027: val_loss did not improve from 20.03714

Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
728/728 [==============================] - 0s 334us/sample - loss: 20.4722 - val_loss: 20.1139
Epoch 28/50
 50/728 [=>............................] - ETA: 0s - loss: 19.7164250/728 [=========>....................] - ETA: 0s - loss: 20.0019450/728 [=================>............] - ETA: 0s - loss: 19.8089650/728 [=========================>....] - ETA: 0s - loss: 19.9350
Epoch 00028: val_loss did not improve from 20.03714
728/728 [==============================] - 0s 327us/sample - loss: 20.1652 - val_loss: 20.3020
Epoch 29/50
 50/728 [=>............................] - ETA: 0s - loss: 19.3061250/728 [=========>....................] - ETA: 0s - loss: 20.3601450/728 [=================>............] - ETA: 0s - loss: 20.3188650/728 [=========================>....] - ETA: 0s - loss: 20.1466
Epoch 00029: val_loss improved from 20.03714 to 19.72037, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 376us/sample - loss: 20.1108 - val_loss: 19.7204
Epoch 30/50
 50/728 [=>............................] - ETA: 0s - loss: 20.2505250/728 [=========>....................] - ETA: 0s - loss: 19.8501450/728 [=================>............] - ETA: 0s - loss: 20.0241650/728 [=========================>....] - ETA: 0s - loss: 20.1211
Epoch 00030: val_loss did not improve from 19.72037
728/728 [==============================] - 0s 321us/sample - loss: 20.0686 - val_loss: 19.7300
Epoch 31/50
 50/728 [=>............................] - ETA: 0s - loss: 19.1032250/728 [=========>....................] - ETA: 0s - loss: 19.6478400/728 [===============>..............] - ETA: 0s - loss: 19.7811600/728 [=======================>......] - ETA: 0s - loss: 20.0137
Epoch 00031: val_loss improved from 19.72037 to 19.53906, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 437us/sample - loss: 20.0523 - val_loss: 19.5391
Epoch 32/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5154250/728 [=========>....................] - ETA: 0s - loss: 20.0891450/728 [=================>............] - ETA: 0s - loss: 19.9557650/728 [=========================>....] - ETA: 0s - loss: 19.9934
Epoch 00032: val_loss improved from 19.53906 to 19.50701, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 374us/sample - loss: 20.1579 - val_loss: 19.5070
Epoch 33/50
 50/728 [=>............................] - ETA: 0s - loss: 19.1195250/728 [=========>....................] - ETA: 0s - loss: 19.8326450/728 [=================>............] - ETA: 0s - loss: 20.1766650/728 [=========================>....] - ETA: 0s - loss: 20.0662
Epoch 00033: val_loss improved from 19.50701 to 19.32150, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 369us/sample - loss: 20.1567 - val_loss: 19.3215
Epoch 34/50
 50/728 [=>............................] - ETA: 0s - loss: 19.7689250/728 [=========>....................] - ETA: 0s - loss: 19.4764450/728 [=================>............] - ETA: 0s - loss: 19.9316650/728 [=========================>....] - ETA: 0s - loss: 19.8314
Epoch 00034: val_loss improved from 19.32150 to 19.25655, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 353us/sample - loss: 19.8494 - val_loss: 19.2565
Epoch 35/50
 50/728 [=>............................] - ETA: 0s - loss: 20.6681250/728 [=========>....................] - ETA: 0s - loss: 19.9967450/728 [=================>............] - ETA: 0s - loss: 19.9266650/728 [=========================>....] - ETA: 0s - loss: 19.8750
Epoch 00035: val_loss improved from 19.25655 to 19.19257, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 339us/sample - loss: 19.8494 - val_loss: 19.1926
Epoch 36/50
 50/728 [=>............................] - ETA: 0s - loss: 19.2621250/728 [=========>....................] - ETA: 0s - loss: 19.5034450/728 [=================>............] - ETA: 0s - loss: 19.9661650/728 [=========================>....] - ETA: 0s - loss: 19.7872
Epoch 00036: val_loss improved from 19.19257 to 19.16793, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 330us/sample - loss: 19.7277 - val_loss: 19.1679
Epoch 37/50
 50/728 [=>............................] - ETA: 0s - loss: 21.1526250/728 [=========>....................] - ETA: 0s - loss: 20.2204450/728 [=================>............] - ETA: 0s - loss: 19.8723650/728 [=========================>....] - ETA: 0s - loss: 19.8153
Epoch 00037: val_loss improved from 19.16793 to 19.08715, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 346us/sample - loss: 19.7695 - val_loss: 19.0871
Epoch 38/50
 50/728 [=>............................] - ETA: 0s - loss: 20.1854250/728 [=========>....................] - ETA: 0s - loss: 19.6824450/728 [=================>............] - ETA: 0s - loss: 19.6926650/728 [=========================>....] - ETA: 0s - loss: 19.6894
Epoch 00038: val_loss improved from 19.08715 to 18.98160, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 369us/sample - loss: 19.6129 - val_loss: 18.9816
Epoch 39/50
 50/728 [=>............................] - ETA: 0s - loss: 18.9106250/728 [=========>....................] - ETA: 0s - loss: 18.9780450/728 [=================>............] - ETA: 0s - loss: 19.5466650/728 [=========================>....] - ETA: 0s - loss: 19.7236
Epoch 00039: val_loss improved from 18.98160 to 18.85543, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 345us/sample - loss: 19.6659 - val_loss: 18.8554
Epoch 40/50
 50/728 [=>............................] - ETA: 0s - loss: 18.8965250/728 [=========>....................] - ETA: 0s - loss: 19.4559450/728 [=================>............] - ETA: 0s - loss: 19.6115650/728 [=========================>....] - ETA: 0s - loss: 19.4989
Epoch 00040: val_loss did not improve from 18.85543
728/728 [==============================] - 0s 329us/sample - loss: 19.4441 - val_loss: 18.8645
Epoch 41/50
 50/728 [=>............................] - ETA: 0s - loss: 19.4315250/728 [=========>....................] - ETA: 0s - loss: 19.3673450/728 [=================>............] - ETA: 0s - loss: 19.7432650/728 [=========================>....] - ETA: 0s - loss: 19.6114
Epoch 00041: val_loss improved from 18.85543 to 18.78671, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 360us/sample - loss: 19.5742 - val_loss: 18.7867
Epoch 42/50
 50/728 [=>............................] - ETA: 0s - loss: 19.5117250/728 [=========>....................] - ETA: 0s - loss: 19.6535450/728 [=================>............] - ETA: 0s - loss: 19.6177650/728 [=========================>....] - ETA: 0s - loss: 19.4443
Epoch 00042: val_loss improved from 18.78671 to 18.74606, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 376us/sample - loss: 19.5343 - val_loss: 18.7461
Epoch 43/50
 50/728 [=>............................] - ETA: 0s - loss: 19.8794250/728 [=========>....................] - ETA: 0s - loss: 19.4385450/728 [=================>............] - ETA: 0s - loss: 19.3119650/728 [=========================>....] - ETA: 0s - loss: 19.5045
Epoch 00043: val_loss improved from 18.74606 to 18.73008, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 363us/sample - loss: 19.5249 - val_loss: 18.7301
Epoch 44/50
 50/728 [=>............................] - ETA: 0s - loss: 20.1636250/728 [=========>....................] - ETA: 0s - loss: 19.2126450/728 [=================>............] - ETA: 0s - loss: 19.2845650/728 [=========================>....] - ETA: 0s - loss: 19.3515
Epoch 00044: val_loss improved from 18.73008 to 18.71585, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 419us/sample - loss: 19.3316 - val_loss: 18.7158
Epoch 45/50
 50/728 [=>............................] - ETA: 0s - loss: 19.4966250/728 [=========>....................] - ETA: 0s - loss: 19.0197450/728 [=================>............] - ETA: 0s - loss: 19.4837650/728 [=========================>....] - ETA: 0s - loss: 19.3124
Epoch 00045: val_loss improved from 18.71585 to 18.60953, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 366us/sample - loss: 19.4318 - val_loss: 18.6095
Epoch 46/50
 50/728 [=>............................] - ETA: 0s - loss: 18.3317250/728 [=========>....................] - ETA: 0s - loss: 18.9272450/728 [=================>............] - ETA: 0s - loss: 19.2025650/728 [=========================>....] - ETA: 0s - loss: 19.2699
Epoch 00046: val_loss improved from 18.60953 to 18.55611, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 372us/sample - loss: 19.3248 - val_loss: 18.5561
Epoch 47/50
 50/728 [=>............................] - ETA: 0s - loss: 19.0872250/728 [=========>....................] - ETA: 0s - loss: 19.3863450/728 [=================>............] - ETA: 0s - loss: 19.5893650/728 [=========================>....] - ETA: 0s - loss: 19.4411
Epoch 00047: val_loss improved from 18.55611 to 18.52397, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 352us/sample - loss: 19.3992 - val_loss: 18.5240
Epoch 48/50
 50/728 [=>............................] - ETA: 0s - loss: 18.9812250/728 [=========>....................] - ETA: 0s - loss: 19.2968450/728 [=================>............] - ETA: 0s - loss: 19.1996650/728 [=========================>....] - ETA: 0s - loss: 19.2868
Epoch 00048: val_loss improved from 18.52397 to 18.43874, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 380us/sample - loss: 19.2287 - val_loss: 18.4387
Epoch 49/50
 50/728 [=>............................] - ETA: 0s - loss: 19.2961250/728 [=========>....................] - ETA: 0s - loss: 19.0212450/728 [=================>............] - ETA: 0s - loss: 18.8811650/728 [=========================>....] - ETA: 0s - loss: 19.2912
Epoch 00049: val_loss did not improve from 18.43874
728/728 [==============================] - 0s 337us/sample - loss: 19.2871 - val_loss: 18.4943
Epoch 50/50
 50/728 [=>............................] - ETA: 0s - loss: 18.5116250/728 [=========>....................] - ETA: 0s - loss: 19.0069450/728 [=================>............] - ETA: 0s - loss: 19.1447650/728 [=========================>....] - ETA: 0s - loss: 19.3804
Epoch 00050: val_loss improved from 18.43874 to 18.36028, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 384us/sample - loss: 19.3246 - val_loss: 18.3603
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 728 samples, validate on 121 samples
Epoch 1/50
 50/728 [=>............................] - ETA: 11s - loss: nan200/728 [=======>......................] - ETA: 2s - loss: nan 400/728 [===============>..............] - ETA: 0s - loss: nan600/728 [=======================>......] - ETA: 0s - loss: nan700/728 [===========================>..] - ETA: 0s - loss: nan
Epoch 00001: val_loss did not improve from inf
728/728 [==============================] - 2s 2ms/sample - loss: nan - val_loss: nan
Epoch 2/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
728/728 [==============================] - 0s 338us/sample - loss: nan - val_loss: nan
Epoch 3/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
728/728 [==============================] - 0s 343us/sample - loss: nan - val_loss: nan
Epoch 4/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
728/728 [==============================] - 0s 337us/sample - loss: nan - val_loss: nan
Epoch 5/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
728/728 [==============================] - 0s 325us/sample - loss: nan - val_loss: nan
Epoch 6/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
728/728 [==============================] - 0s 327us/sample - loss: nan - val_loss: nan
Epoch 7/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan600/728 [=======================>......] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
728/728 [==============================] - 0s 344us/sample - loss: nan - val_loss: nan
Epoch 8/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
728/728 [==============================] - 0s 322us/sample - loss: nan - val_loss: nan
Epoch 9/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
728/728 [==============================] - 0s 328us/sample - loss: nan - val_loss: nan
Epoch 10/50
 50/728 [=>............................] - ETA: 0s - loss: nan250/728 [=========>....................] - ETA: 0s - loss: nan450/728 [=================>............] - ETA: 0s - loss: nan650/728 [=========================>....] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
728/728 [==============================] - 0s 333us/sample - loss: nan - val_loss: nan
Epoch 00010: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 849 samples, validate on 121 samples
Epoch 1/50
 50/849 [>.............................] - ETA: 13s - loss: 159.7389200/849 [======>.......................] - ETA: 2s - loss: 158.7623 400/849 [=============>................] - ETA: 1s - loss: 156.2320600/849 [====================>.........] - ETA: 0s - loss: 152.8268800/849 [===========================>..] - ETA: 0s - loss: 148.0623
Epoch 00001: val_loss improved from inf to 118.45812, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 2s 3ms/sample - loss: 145.4694 - val_loss: 118.4581
Epoch 2/50
 50/849 [>.............................] - ETA: 0s - loss: 96.9490250/849 [=======>......................] - ETA: 0s - loss: 80.1818450/849 [==============>...............] - ETA: 0s - loss: 73.6570650/849 [=====================>........] - ETA: 0s - loss: 66.3580
Epoch 00002: val_loss improved from 118.45812 to 78.52440, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 406us/sample - loss: 60.3190 - val_loss: 78.5244
Epoch 3/50
 50/849 [>.............................] - ETA: 0s - loss: 37.6208250/849 [=======>......................] - ETA: 0s - loss: 40.5402450/849 [==============>...............] - ETA: 0s - loss: 39.0264650/849 [=====================>........] - ETA: 0s - loss: 37.3856
Epoch 00003: val_loss improved from 78.52440 to 67.73680, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 359us/sample - loss: 36.2352 - val_loss: 67.7368
Epoch 4/50
 50/849 [>.............................] - ETA: 0s - loss: 32.9951250/849 [=======>......................] - ETA: 0s - loss: 32.2643450/849 [==============>...............] - ETA: 0s - loss: 31.1398650/849 [=====================>........] - ETA: 0s - loss: 30.0901
Epoch 00004: val_loss did not improve from 67.73680
849/849 [==============================] - 0s 318us/sample - loss: 29.1395 - val_loss: 71.4450
Epoch 5/50
 50/849 [>.............................] - ETA: 0s - loss: 26.2247250/849 [=======>......................] - ETA: 0s - loss: 25.5576450/849 [==============>...............] - ETA: 0s - loss: 25.3516650/849 [=====================>........] - ETA: 0s - loss: 25.0801
Epoch 00005: val_loss improved from 67.73680 to 59.77018, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 353us/sample - loss: 24.9323 - val_loss: 59.7702
Epoch 6/50
 50/849 [>.............................] - ETA: 0s - loss: 23.3337250/849 [=======>......................] - ETA: 0s - loss: 23.3496450/849 [==============>...............] - ETA: 0s - loss: 23.1660650/849 [=====================>........] - ETA: 0s - loss: 23.0828
Epoch 00006: val_loss improved from 59.77018 to 52.94474, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 353us/sample - loss: 23.0013 - val_loss: 52.9447
Epoch 7/50
 50/849 [>.............................] - ETA: 0s - loss: 21.6260250/849 [=======>......................] - ETA: 0s - loss: 21.6575450/849 [==============>...............] - ETA: 0s - loss: 22.7811650/849 [=====================>........] - ETA: 0s - loss: 23.2046
Epoch 00007: val_loss improved from 52.94474 to 52.92663, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 366us/sample - loss: 22.9296 - val_loss: 52.9266
Epoch 8/50
 50/849 [>.............................] - ETA: 0s - loss: 23.0118250/849 [=======>......................] - ETA: 0s - loss: 22.9483450/849 [==============>...............] - ETA: 0s - loss: 22.5515650/849 [=====================>........] - ETA: 0s - loss: 21.9549
Epoch 00008: val_loss improved from 52.92663 to 41.27956, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 359us/sample - loss: 21.5057 - val_loss: 41.2796
Epoch 9/50
 50/849 [>.............................] - ETA: 0s - loss: 20.2380250/849 [=======>......................] - ETA: 0s - loss: 19.8318450/849 [==============>...............] - ETA: 0s - loss: 19.9538650/849 [=====================>........] - ETA: 0s - loss: 20.0397
Epoch 00009: val_loss improved from 41.27956 to 35.55303, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 364us/sample - loss: 20.4065 - val_loss: 35.5530
Epoch 10/50
 50/849 [>.............................] - ETA: 0s - loss: 19.3978250/849 [=======>......................] - ETA: 0s - loss: 21.0782450/849 [==============>...............] - ETA: 0s - loss: 20.9227650/849 [=====================>........] - ETA: 0s - loss: 20.7547
Epoch 00010: val_loss improved from 35.55303 to 30.58426, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 350us/sample - loss: 20.5727 - val_loss: 30.5843
Epoch 11/50
 50/849 [>.............................] - ETA: 0s - loss: 19.9967250/849 [=======>......................] - ETA: 0s - loss: 19.9931450/849 [==============>...............] - ETA: 0s - loss: 19.8059650/849 [=====================>........] - ETA: 0s - loss: 19.4643
Epoch 00011: val_loss improved from 30.58426 to 27.51833, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 360us/sample - loss: 19.4212 - val_loss: 27.5183
Epoch 12/50
 50/849 [>.............................] - ETA: 0s - loss: 19.1685250/849 [=======>......................] - ETA: 0s - loss: 19.2129450/849 [==============>...............] - ETA: 0s - loss: 19.8342650/849 [=====================>........] - ETA: 0s - loss: 19.9610
Epoch 00012: val_loss did not improve from 27.51833
849/849 [==============================] - 0s 321us/sample - loss: 20.0290 - val_loss: 32.2805
Epoch 13/50
 50/849 [>.............................] - ETA: 0s - loss: 19.8858250/849 [=======>......................] - ETA: 0s - loss: 19.5618450/849 [==============>...............] - ETA: 0s - loss: 19.3301650/849 [=====================>........] - ETA: 0s - loss: 19.1385
Epoch 00013: val_loss improved from 27.51833 to 22.82755, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 351us/sample - loss: 19.2185 - val_loss: 22.8275
Epoch 14/50
 50/849 [>.............................] - ETA: 0s - loss: 18.8813250/849 [=======>......................] - ETA: 0s - loss: 18.3305450/849 [==============>...............] - ETA: 0s - loss: 18.4762650/849 [=====================>........] - ETA: 0s - loss: 18.7251
Epoch 00014: val_loss improved from 22.82755 to 21.51540, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 361us/sample - loss: 18.8520 - val_loss: 21.5154
Epoch 15/50
 50/849 [>.............................] - ETA: 0s - loss: 18.8209250/849 [=======>......................] - ETA: 0s - loss: 18.5619450/849 [==============>...............] - ETA: 0s - loss: 18.5842650/849 [=====================>........] - ETA: 0s - loss: 18.6131
Epoch 00015: val_loss did not improve from 21.51540
849/849 [==============================] - 0s 328us/sample - loss: 18.5847 - val_loss: 22.5050
Epoch 16/50
 50/849 [>.............................] - ETA: 0s - loss: 20.5865250/849 [=======>......................] - ETA: 0s - loss: 19.0697450/849 [==============>...............] - ETA: 0s - loss: 18.9996650/849 [=====================>........] - ETA: 0s - loss: 19.4528
Epoch 00016: val_loss improved from 21.51540 to 19.40631, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 385us/sample - loss: 19.4950 - val_loss: 19.4063
Epoch 17/50
 50/849 [>.............................] - ETA: 0s - loss: 17.5957250/849 [=======>......................] - ETA: 0s - loss: 18.4089450/849 [==============>...............] - ETA: 0s - loss: 18.3680650/849 [=====================>........] - ETA: 0s - loss: 18.3003
Epoch 00017: val_loss did not improve from 19.40631
849/849 [==============================] - 0s 314us/sample - loss: 18.2313 - val_loss: 19.8082
Epoch 18/50
 50/849 [>.............................] - ETA: 0s - loss: 17.0018250/849 [=======>......................] - ETA: 0s - loss: 18.8962450/849 [==============>...............] - ETA: 0s - loss: 18.6160650/849 [=====================>........] - ETA: 0s - loss: 18.4147
Epoch 00018: val_loss improved from 19.40631 to 18.80665, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 424us/sample - loss: 18.2418 - val_loss: 18.8067
Epoch 19/50
 50/849 [>.............................] - ETA: 0s - loss: 16.9822250/849 [=======>......................] - ETA: 0s - loss: 18.7221450/849 [==============>...............] - ETA: 0s - loss: 19.0816650/849 [=====================>........] - ETA: 0s - loss: 18.9545
Epoch 00019: val_loss improved from 18.80665 to 18.47399, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 356us/sample - loss: 18.8860 - val_loss: 18.4740
Epoch 20/50
 50/849 [>.............................] - ETA: 0s - loss: 18.7471250/849 [=======>......................] - ETA: 0s - loss: 18.3175400/849 [=============>................] - ETA: 0s - loss: 18.4941600/849 [====================>.........] - ETA: 0s - loss: 18.2039800/849 [===========================>..] - ETA: 0s - loss: 18.2359
Epoch 00020: val_loss did not improve from 18.47399
849/849 [==============================] - 0s 338us/sample - loss: 18.2397 - val_loss: 18.8552
Epoch 21/50
 50/849 [>.............................] - ETA: 0s - loss: 19.0811250/849 [=======>......................] - ETA: 0s - loss: 18.7840450/849 [==============>...............] - ETA: 0s - loss: 18.7540650/849 [=====================>........] - ETA: 0s - loss: 18.4119
Epoch 00021: val_loss improved from 18.47399 to 17.45447, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 363us/sample - loss: 18.2730 - val_loss: 17.4545
Epoch 22/50
 50/849 [>.............................] - ETA: 0s - loss: 18.7274250/849 [=======>......................] - ETA: 0s - loss: 17.5201450/849 [==============>...............] - ETA: 0s - loss: 17.7118650/849 [=====================>........] - ETA: 0s - loss: 17.5969
Epoch 00022: val_loss improved from 17.45447 to 17.16872, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 357us/sample - loss: 17.6216 - val_loss: 17.1687
Epoch 23/50
 50/849 [>.............................] - ETA: 0s - loss: 19.8114250/849 [=======>......................] - ETA: 0s - loss: 18.1052450/849 [==============>...............] - ETA: 0s - loss: 19.1873650/849 [=====================>........] - ETA: 0s - loss: 19.6910
Epoch 00023: val_loss did not improve from 17.16872
849/849 [==============================] - 0s 323us/sample - loss: 19.5179 - val_loss: 17.6882
Epoch 24/50
 50/849 [>.............................] - ETA: 0s - loss: 18.1704250/849 [=======>......................] - ETA: 0s - loss: 17.8211450/849 [==============>...............] - ETA: 0s - loss: 17.9865650/849 [=====================>........] - ETA: 0s - loss: 18.4717
Epoch 00024: val_loss improved from 17.16872 to 16.90083, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 371us/sample - loss: 18.3727 - val_loss: 16.9008
Epoch 25/50
 50/849 [>.............................] - ETA: 0s - loss: 21.3524250/849 [=======>......................] - ETA: 0s - loss: 19.5398450/849 [==============>...............] - ETA: 0s - loss: 18.6600650/849 [=====================>........] - ETA: 0s - loss: 18.5085
Epoch 00025: val_loss did not improve from 16.90083
849/849 [==============================] - 0s 319us/sample - loss: 18.4365 - val_loss: 17.7073
Epoch 26/50
 50/849 [>.............................] - ETA: 0s - loss: 18.8053250/849 [=======>......................] - ETA: 0s - loss: 17.8348450/849 [==============>...............] - ETA: 0s - loss: 17.9657650/849 [=====================>........] - ETA: 0s - loss: 17.8749
Epoch 00026: val_loss improved from 16.90083 to 16.48191, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 352us/sample - loss: 17.8755 - val_loss: 16.4819
Epoch 27/50
 50/849 [>.............................] - ETA: 0s - loss: 18.0120250/849 [=======>......................] - ETA: 0s - loss: 17.3735450/849 [==============>...............] - ETA: 0s - loss: 17.4974650/849 [=====================>........] - ETA: 0s - loss: 17.4086
Epoch 00027: val_loss did not improve from 16.48191
849/849 [==============================] - 0s 329us/sample - loss: 17.3776 - val_loss: 16.5050
Epoch 28/50
 50/849 [>.............................] - ETA: 0s - loss: 17.0633250/849 [=======>......................] - ETA: 0s - loss: 17.6746450/849 [==============>...............] - ETA: 0s - loss: 17.8859650/849 [=====================>........] - ETA: 0s - loss: 18.3898800/849 [===========================>..] - ETA: 0s - loss: 18.4760
Epoch 00028: val_loss improved from 16.48191 to 16.26455, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 372us/sample - loss: 18.4727 - val_loss: 16.2646
Epoch 29/50
 50/849 [>.............................] - ETA: 0s - loss: 17.4800250/849 [=======>......................] - ETA: 0s - loss: 18.0544450/849 [==============>...............] - ETA: 0s - loss: 17.8083650/849 [=====================>........] - ETA: 0s - loss: 18.1298
Epoch 00029: val_loss did not improve from 16.26455
849/849 [==============================] - 0s 312us/sample - loss: 18.0101 - val_loss: 16.4647
Epoch 30/50
 50/849 [>.............................] - ETA: 0s - loss: 17.1094250/849 [=======>......................] - ETA: 0s - loss: 16.9486450/849 [==============>...............] - ETA: 0s - loss: 17.2261650/849 [=====================>........] - ETA: 0s - loss: 17.3408
Epoch 00030: val_loss did not improve from 16.26455

Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
849/849 [==============================] - 0s 315us/sample - loss: 17.4990 - val_loss: 17.0156
Epoch 31/50
 50/849 [>.............................] - ETA: 0s - loss: 17.4439250/849 [=======>......................] - ETA: 0s - loss: 18.0051450/849 [==============>...............] - ETA: 0s - loss: 17.8542650/849 [=====================>........] - ETA: 0s - loss: 17.6397
Epoch 00031: val_loss improved from 16.26455 to 16.10326, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 372us/sample - loss: 17.7100 - val_loss: 16.1033
Epoch 32/50
 50/849 [>.............................] - ETA: 0s - loss: 19.8697250/849 [=======>......................] - ETA: 0s - loss: 18.2120450/849 [==============>...............] - ETA: 0s - loss: 17.8710650/849 [=====================>........] - ETA: 0s - loss: 17.5649
Epoch 00032: val_loss did not improve from 16.10326
849/849 [==============================] - 0s 319us/sample - loss: 17.5156 - val_loss: 16.3255
Epoch 33/50
 50/849 [>.............................] - ETA: 0s - loss: 15.6892250/849 [=======>......................] - ETA: 0s - loss: 16.3468450/849 [==============>...............] - ETA: 0s - loss: 16.6112650/849 [=====================>........] - ETA: 0s - loss: 16.8736
Epoch 00033: val_loss improved from 16.10326 to 15.87613, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 367us/sample - loss: 17.2928 - val_loss: 15.8761
Epoch 34/50
 50/849 [>.............................] - ETA: 0s - loss: 17.0550250/849 [=======>......................] - ETA: 0s - loss: 17.0420450/849 [==============>...............] - ETA: 0s - loss: 17.2631650/849 [=====================>........] - ETA: 0s - loss: 17.0358
Epoch 00034: val_loss did not improve from 15.87613
849/849 [==============================] - 0s 314us/sample - loss: 17.3148 - val_loss: 16.2508
Epoch 35/50
 50/849 [>.............................] - ETA: 0s - loss: 18.1871250/849 [=======>......................] - ETA: 0s - loss: 17.2002450/849 [==============>...............] - ETA: 0s - loss: 17.1095650/849 [=====================>........] - ETA: 0s - loss: 17.2983
Epoch 00035: val_loss did not improve from 15.87613

Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
849/849 [==============================] - 0s 324us/sample - loss: 17.1844 - val_loss: 16.2121
Epoch 36/50
 50/849 [>.............................] - ETA: 0s - loss: 18.7696250/849 [=======>......................] - ETA: 0s - loss: 17.9662450/849 [==============>...............] - ETA: 0s - loss: 17.7031650/849 [=====================>........] - ETA: 0s - loss: 17.5411
Epoch 00036: val_loss did not improve from 15.87613
849/849 [==============================] - 0s 306us/sample - loss: 17.3907 - val_loss: 15.9603
Epoch 37/50
 50/849 [>.............................] - ETA: 0s - loss: 16.5693250/849 [=======>......................] - ETA: 0s - loss: 16.5054450/849 [==============>...............] - ETA: 0s - loss: 16.8323650/849 [=====================>........] - ETA: 0s - loss: 17.1590
Epoch 00037: val_loss improved from 15.87613 to 15.87149, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 373us/sample - loss: 17.1500 - val_loss: 15.8715
Epoch 38/50
 50/849 [>.............................] - ETA: 0s - loss: 17.3905250/849 [=======>......................] - ETA: 0s - loss: 17.0099450/849 [==============>...............] - ETA: 0s - loss: 17.5220600/849 [====================>.........] - ETA: 0s - loss: 17.3685750/849 [=========================>....] - ETA: 0s - loss: 17.1849
Epoch 00038: val_loss did not improve from 15.87149
849/849 [==============================] - 0s 384us/sample - loss: 17.2079 - val_loss: 15.9974
Epoch 39/50
 50/849 [>.............................] - ETA: 0s - loss: 19.3259250/849 [=======>......................] - ETA: 0s - loss: 17.4054450/849 [==============>...............] - ETA: 0s - loss: 17.4554650/849 [=====================>........] - ETA: 0s - loss: 17.5244
Epoch 00039: val_loss did not improve from 15.87149

Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
849/849 [==============================] - 0s 323us/sample - loss: 17.5387 - val_loss: 15.9687
Epoch 40/50
 50/849 [>.............................] - ETA: 0s - loss: 16.2952250/849 [=======>......................] - ETA: 0s - loss: 16.9184450/849 [==============>...............] - ETA: 0s - loss: 16.8005650/849 [=====================>........] - ETA: 0s - loss: 16.7862
Epoch 00040: val_loss improved from 15.87149 to 15.84341, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 364us/sample - loss: 16.8420 - val_loss: 15.8434
Epoch 41/50
 50/849 [>.............................] - ETA: 0s - loss: 17.5414250/849 [=======>......................] - ETA: 0s - loss: 17.0728450/849 [==============>...............] - ETA: 0s - loss: 17.0750650/849 [=====================>........] - ETA: 0s - loss: 17.2108
Epoch 00041: val_loss improved from 15.84341 to 15.84203, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 359us/sample - loss: 17.1263 - val_loss: 15.8420
Epoch 42/50
 50/849 [>.............................] - ETA: 0s - loss: 17.2257250/849 [=======>......................] - ETA: 0s - loss: 17.1722450/849 [==============>...............] - ETA: 0s - loss: 17.3610650/849 [=====================>........] - ETA: 0s - loss: 17.3652
Epoch 00042: val_loss improved from 15.84203 to 15.82493, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 362us/sample - loss: 17.2432 - val_loss: 15.8249
Epoch 43/50
 50/849 [>.............................] - ETA: 0s - loss: 16.1946250/849 [=======>......................] - ETA: 0s - loss: 16.9589450/849 [==============>...............] - ETA: 0s - loss: 17.0399650/849 [=====================>........] - ETA: 0s - loss: 16.9694
Epoch 00043: val_loss improved from 15.82493 to 15.80386, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 352us/sample - loss: 16.9175 - val_loss: 15.8039
Epoch 44/50
 50/849 [>.............................] - ETA: 0s - loss: 17.1494250/849 [=======>......................] - ETA: 0s - loss: 16.8015450/849 [==============>...............] - ETA: 0s - loss: 17.1590650/849 [=====================>........] - ETA: 0s - loss: 17.0631800/849 [===========================>..] - ETA: 0s - loss: 16.9109
Epoch 00044: val_loss did not improve from 15.80386
849/849 [==============================] - 0s 329us/sample - loss: 16.9227 - val_loss: 15.8463
Epoch 45/50
 50/849 [>.............................] - ETA: 0s - loss: 16.8900250/849 [=======>......................] - ETA: 0s - loss: 16.8695450/849 [==============>...............] - ETA: 0s - loss: 16.9005650/849 [=====================>........] - ETA: 0s - loss: 16.9237
Epoch 00045: val_loss did not improve from 15.80386

Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
849/849 [==============================] - 0s 318us/sample - loss: 16.9771 - val_loss: 15.8615
Epoch 46/50
 50/849 [>.............................] - ETA: 0s - loss: 19.2452250/849 [=======>......................] - ETA: 0s - loss: 17.1817450/849 [==============>...............] - ETA: 0s - loss: 16.9094650/849 [=====================>........] - ETA: 0s - loss: 16.9742
Epoch 00046: val_loss improved from 15.80386 to 15.79055, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 355us/sample - loss: 16.9161 - val_loss: 15.7905
Epoch 47/50
 50/849 [>.............................] - ETA: 0s - loss: 16.1499250/849 [=======>......................] - ETA: 0s - loss: 16.9752450/849 [==============>...............] - ETA: 0s - loss: 16.9852650/849 [=====================>........] - ETA: 0s - loss: 16.9638
Epoch 00047: val_loss did not improve from 15.79055
849/849 [==============================] - 0s 299us/sample - loss: 16.8976 - val_loss: 15.8046
Epoch 48/50
 50/849 [>.............................] - ETA: 0s - loss: 16.5842250/849 [=======>......................] - ETA: 0s - loss: 16.9409450/849 [==============>...............] - ETA: 0s - loss: 16.8157650/849 [=====================>........] - ETA: 0s - loss: 16.8843
Epoch 00048: val_loss did not improve from 15.79055

Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
849/849 [==============================] - 0s 311us/sample - loss: 16.8219 - val_loss: 15.8200
Epoch 49/50
 50/849 [>.............................] - ETA: 0s - loss: 16.8427250/849 [=======>......................] - ETA: 0s - loss: 17.1712450/849 [==============>...............] - ETA: 0s - loss: 16.9344650/849 [=====================>........] - ETA: 0s - loss: 17.0464
Epoch 00049: val_loss did not improve from 15.79055
849/849 [==============================] - 0s 308us/sample - loss: 16.9815 - val_loss: 15.8117
Epoch 50/50
 50/849 [>.............................] - ETA: 0s - loss: 16.3483250/849 [=======>......................] - ETA: 0s - loss: 16.9199450/849 [==============>...............] - ETA: 0s - loss: 16.7252650/849 [=====================>........] - ETA: 0s - loss: 16.7119
Epoch 00050: val_loss did not improve from 15.79055

Epoch 00050: ReduceLROnPlateau reducing learning rate to 1e-05.
849/849 [==============================] - 0s 333us/sample - loss: 16.7785 - val_loss: 15.8336
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b370960ee48> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3709614c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3709614f60> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3709614f98> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b370960b9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37096115c0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b370400dcc0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b370400dc18> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3704016208> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704037f60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37040419b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370404b710> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3704056f98> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b370405f2e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3704068cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704068be0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37040837f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704083a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3704083be0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370408b9b0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b370408bb38> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b370960ee48> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3709614c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3709614f60> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3709614f98> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b370960b9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37096115c0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b370400dcc0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b370400dc18> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3704016208> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704037f60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37040419b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370404b710> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3704056f98> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b370405f2e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3704068cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704068be0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37040837f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704083a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3704083be0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370408b9b0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b370408bb38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b368feb42b0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37040c0f60> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 112.5598
Epoch 00001: val_loss improved from inf to 59.16896, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 11ms/sample - loss: 86.8064 - val_loss: 59.1690
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 56.4792
Epoch 00002: val_loss improved from 59.16896 to 38.48785, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 831us/sample - loss: 47.7235 - val_loss: 38.4878
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 35.5114
Epoch 00003: val_loss improved from 38.48785 to 30.51632, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 843us/sample - loss: 33.2636 - val_loss: 30.5163
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 32.3709
Epoch 00004: val_loss improved from 30.51632 to 24.85260, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 739us/sample - loss: 27.3083 - val_loss: 24.8526
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7586
Epoch 00005: val_loss improved from 24.85260 to 20.21168, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 748us/sample - loss: 21.7554 - val_loss: 20.2117
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1603
Epoch 00006: val_loss did not improve from 20.21168
121/121 [==============================] - 0s 590us/sample - loss: 20.3550 - val_loss: 20.4256
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2583
Epoch 00007: val_loss did not improve from 20.21168

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 555us/sample - loss: 19.7643 - val_loss: 20.2471
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2759
Epoch 00008: val_loss improved from 20.21168 to 19.81193, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 768us/sample - loss: 19.6774 - val_loss: 19.8119
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0816
Epoch 00009: val_loss improved from 19.81193 to 19.49469, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 808us/sample - loss: 19.3063 - val_loss: 19.4947
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.6700
Epoch 00010: val_loss improved from 19.49469 to 19.39202, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 834us/sample - loss: 19.1937 - val_loss: 19.3920
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8724
Epoch 00011: val_loss improved from 19.39202 to 19.33595, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 787us/sample - loss: 19.0753 - val_loss: 19.3360
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.2001
Epoch 00012: val_loss improved from 19.33595 to 19.26454, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 810us/sample - loss: 18.9974 - val_loss: 19.2645
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.3251
Epoch 00013: val_loss improved from 19.26454 to 19.19217, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 722us/sample - loss: 18.9296 - val_loss: 19.1922
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4062
Epoch 00014: val_loss improved from 19.19217 to 19.11999, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 729us/sample - loss: 18.8656 - val_loss: 19.1200
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8994
Epoch 00015: val_loss improved from 19.11999 to 19.07005, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 738us/sample - loss: 18.8072 - val_loss: 19.0701
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4340
Epoch 00016: val_loss improved from 19.07005 to 19.01372, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 713us/sample - loss: 18.7478 - val_loss: 19.0137
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.6082
Epoch 00017: val_loss improved from 19.01372 to 18.94513, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 767us/sample - loss: 18.6930 - val_loss: 18.9451
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5643
Epoch 00018: val_loss improved from 18.94513 to 18.88774, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 846us/sample - loss: 18.6526 - val_loss: 18.8877
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9003
Epoch 00019: val_loss improved from 18.88774 to 18.85800, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 811us/sample - loss: 18.5937 - val_loss: 18.8580
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4101
Epoch 00020: val_loss improved from 18.85800 to 18.82460, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 18.5526 - val_loss: 18.8246
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8362
Epoch 00021: val_loss did not improve from 18.82460
121/121 [==============================] - 0s 553us/sample - loss: 18.5351 - val_loss: 18.8311
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.7440
Epoch 00022: val_loss improved from 18.82460 to 18.72644, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 709us/sample - loss: 18.4647 - val_loss: 18.7264
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0964
Epoch 00023: val_loss improved from 18.72644 to 18.68392, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 790us/sample - loss: 18.4565 - val_loss: 18.6839
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5463
Epoch 00024: val_loss did not improve from 18.68392
121/121 [==============================] - 0s 563us/sample - loss: 18.3785 - val_loss: 18.7182
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8598
Epoch 00025: val_loss did not improve from 18.68392

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 537us/sample - loss: 18.4305 - val_loss: 18.7201
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.1628
Epoch 00026: val_loss improved from 18.68392 to 18.61716, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 691us/sample - loss: 18.3519 - val_loss: 18.6172
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3541
Epoch 00027: val_loss improved from 18.61716 to 18.58825, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 777us/sample - loss: 18.3085 - val_loss: 18.5883
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.5639
Epoch 00028: val_loss did not improve from 18.58825
121/121 [==============================] - 0s 567us/sample - loss: 18.2964 - val_loss: 18.5886
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.9676
Epoch 00029: val_loss improved from 18.58825 to 18.56581, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 757us/sample - loss: 18.2809 - val_loss: 18.5658
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6293
Epoch 00030: val_loss did not improve from 18.56581
121/121 [==============================] - 0s 627us/sample - loss: 18.2692 - val_loss: 18.5671
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.3621
Epoch 00031: val_loss improved from 18.56581 to 18.55479, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 802us/sample - loss: 18.2638 - val_loss: 18.5548
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.5452
Epoch 00032: val_loss improved from 18.55479 to 18.52323, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 779us/sample - loss: 18.2434 - val_loss: 18.5232
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.7280
Epoch 00033: val_loss improved from 18.52323 to 18.51029, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 805us/sample - loss: 18.2309 - val_loss: 18.5103
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9719
Epoch 00034: val_loss improved from 18.51029 to 18.49739, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 755us/sample - loss: 18.2221 - val_loss: 18.4974
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4684
Epoch 00035: val_loss improved from 18.49739 to 18.49586, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 788us/sample - loss: 18.2100 - val_loss: 18.4959
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4335
Epoch 00036: val_loss improved from 18.49586 to 18.48782, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 817us/sample - loss: 18.1990 - val_loss: 18.4878
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8667
Epoch 00037: val_loss improved from 18.48782 to 18.48453, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 756us/sample - loss: 18.1821 - val_loss: 18.4845
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.8896
Epoch 00038: val_loss improved from 18.48453 to 18.46482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 712us/sample - loss: 18.1732 - val_loss: 18.4648
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.5109
Epoch 00039: val_loss did not improve from 18.46482
121/121 [==============================] - 0s 537us/sample - loss: 18.1710 - val_loss: 18.4678
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8434100/121 [=======================>......] - ETA: 0s - loss: 18.0028
Epoch 00040: val_loss improved from 18.46482 to 18.45827, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 18.1595 - val_loss: 18.4583
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 15.7956
Epoch 00041: val_loss improved from 18.45827 to 18.41733, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 731us/sample - loss: 18.1400 - val_loss: 18.4173
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8850
Epoch 00042: val_loss improved from 18.41733 to 18.41218, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 703us/sample - loss: 18.1325 - val_loss: 18.4122
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8353
Epoch 00043: val_loss improved from 18.41218 to 18.40935, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 754us/sample - loss: 18.1363 - val_loss: 18.4094
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.1347
Epoch 00044: val_loss did not improve from 18.40935
121/121 [==============================] - 0s 540us/sample - loss: 18.1114 - val_loss: 18.4117
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.5174
Epoch 00045: val_loss improved from 18.40935 to 18.40189, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 690us/sample - loss: 18.1012 - val_loss: 18.4019
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4377
Epoch 00046: val_loss improved from 18.40189 to 18.37130, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 707us/sample - loss: 18.0879 - val_loss: 18.3713
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.1522
Epoch 00047: val_loss improved from 18.37130 to 18.36501, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 718us/sample - loss: 18.0864 - val_loss: 18.3650
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.1372
Epoch 00048: val_loss did not improve from 18.36501
121/121 [==============================] - 0s 579us/sample - loss: 18.0716 - val_loss: 18.3740
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.8441
Epoch 00049: val_loss improved from 18.36501 to 18.34597, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 734us/sample - loss: 18.0604 - val_loss: 18.3460
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8485
Epoch 00050: val_loss improved from 18.34597 to 18.34124, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 672us/sample - loss: 18.0474 - val_loss: 18.3412
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.1730
Epoch 00051: val_loss improved from 18.34124 to 18.33592, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 743us/sample - loss: 18.0383 - val_loss: 18.3359
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.5955
Epoch 00052: val_loss improved from 18.33592 to 18.31803, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 705us/sample - loss: 18.0291 - val_loss: 18.3180
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0905
Epoch 00053: val_loss did not improve from 18.31803
121/121 [==============================] - 0s 501us/sample - loss: 18.0163 - val_loss: 18.3191
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7958
Epoch 00054: val_loss improved from 18.31803 to 18.30924, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 680us/sample - loss: 18.0104 - val_loss: 18.3092
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9820
Epoch 00055: val_loss improved from 18.30924 to 18.29477, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 760us/sample - loss: 17.9984 - val_loss: 18.2948
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.6564
Epoch 00056: val_loss improved from 18.29477 to 18.28669, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 699us/sample - loss: 18.0016 - val_loss: 18.2867
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4652
Epoch 00057: val_loss improved from 18.28669 to 18.27319, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 17.9981 - val_loss: 18.2732
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.1017
Epoch 00058: val_loss improved from 18.27319 to 18.27027, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 17.9769 - val_loss: 18.2703
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.1286
Epoch 00059: val_loss improved from 18.27027 to 18.26736, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 17.9623 - val_loss: 18.2674
Epoch 60/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.1343
Epoch 00060: val_loss improved from 18.26736 to 18.25483, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 729us/sample - loss: 17.9594 - val_loss: 18.2548
Epoch 61/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2951
Epoch 00061: val_loss improved from 18.25483 to 18.24433, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 703us/sample - loss: 17.9419 - val_loss: 18.2443
Epoch 62/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0293
Epoch 00062: val_loss improved from 18.24433 to 18.23051, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 680us/sample - loss: 17.9343 - val_loss: 18.2305
Epoch 63/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4429
Epoch 00063: val_loss improved from 18.23051 to 18.22478, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 724us/sample - loss: 17.9267 - val_loss: 18.2248
Epoch 64/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.5285
Epoch 00064: val_loss did not improve from 18.22478
121/121 [==============================] - 0s 523us/sample - loss: 17.9155 - val_loss: 18.2373
Epoch 65/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4387
Epoch 00065: val_loss did not improve from 18.22478

Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 528us/sample - loss: 17.9424 - val_loss: 18.3194
Epoch 66/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7565
Epoch 00066: val_loss did not improve from 18.22478
121/121 [==============================] - 0s 517us/sample - loss: 17.9509 - val_loss: 18.2643
Epoch 67/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.4163
Epoch 00067: val_loss improved from 18.22478 to 18.22384, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 674us/sample - loss: 17.9057 - val_loss: 18.2238
Epoch 68/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.6849
Epoch 00068: val_loss improved from 18.22384 to 18.21926, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 693us/sample - loss: 17.9038 - val_loss: 18.2193
Epoch 69/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.0165
Epoch 00069: val_loss improved from 18.21926 to 18.20354, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 694us/sample - loss: 17.8976 - val_loss: 18.2035
Epoch 70/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6954
Epoch 00070: val_loss improved from 18.20354 to 18.20121, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 17.8879 - val_loss: 18.2012
Epoch 71/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.1769
Epoch 00071: val_loss did not improve from 18.20121
121/121 [==============================] - 0s 531us/sample - loss: 17.8876 - val_loss: 18.2105
Epoch 72/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9463
Epoch 00072: val_loss did not improve from 18.20121

Epoch 00072: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 511us/sample - loss: 17.8868 - val_loss: 18.2069
Epoch 73/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8140
Epoch 00073: val_loss improved from 18.20121 to 18.19807, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 687us/sample - loss: 17.8803 - val_loss: 18.1981
Epoch 74/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7185
Epoch 00074: val_loss improved from 18.19807 to 18.19314, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 726us/sample - loss: 17.8770 - val_loss: 18.1931
Epoch 75/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.5886
Epoch 00075: val_loss improved from 18.19314 to 18.19070, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 17.8753 - val_loss: 18.1907
Epoch 76/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.5288
Epoch 00076: val_loss improved from 18.19070 to 18.18804, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 752us/sample - loss: 17.8741 - val_loss: 18.1880
Epoch 77/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.0965
Epoch 00077: val_loss did not improve from 18.18804
121/121 [==============================] - 0s 544us/sample - loss: 17.8740 - val_loss: 18.1884
Epoch 78/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.6579
Epoch 00078: val_loss improved from 18.18804 to 18.18765, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 657us/sample - loss: 17.8734 - val_loss: 18.1876
Epoch 79/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.4701
Epoch 00079: val_loss improved from 18.18765 to 18.18498, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 692us/sample - loss: 17.8720 - val_loss: 18.1850
Epoch 80/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.6432
Epoch 00080: val_loss improved from 18.18498 to 18.18349, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 794us/sample - loss: 17.8705 - val_loss: 18.1835
Epoch 81/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9396
Epoch 00081: val_loss improved from 18.18349 to 18.18137, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 768us/sample - loss: 17.8693 - val_loss: 18.1814
Epoch 82/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0215
Epoch 00082: val_loss improved from 18.18137 to 18.17865, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 749us/sample - loss: 17.8682 - val_loss: 18.1787
Epoch 83/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.6122
Epoch 00083: val_loss did not improve from 18.17865
121/121 [==============================] - 0s 556us/sample - loss: 17.8660 - val_loss: 18.1788
Epoch 84/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7823
Epoch 00084: val_loss improved from 18.17865 to 18.17783, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 711us/sample - loss: 17.8643 - val_loss: 18.1778
Epoch 85/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4450
Epoch 00085: val_loss improved from 18.17783 to 18.17732, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 727us/sample - loss: 17.8632 - val_loss: 18.1773
Epoch 86/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0215
Epoch 00086: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 520us/sample - loss: 17.8615 - val_loss: 18.1788
Epoch 87/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.1306
Epoch 00087: val_loss did not improve from 18.17732

Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 527us/sample - loss: 17.8606 - val_loss: 18.1872
Epoch 88/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.7560
Epoch 00088: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 556us/sample - loss: 17.8613 - val_loss: 18.1889
Epoch 89/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.5506
Epoch 00089: val_loss did not improve from 18.17732

Epoch 00089: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 660us/sample - loss: 17.8607 - val_loss: 18.1856
Epoch 90/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2693
Epoch 00090: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 547us/sample - loss: 17.8596 - val_loss: 18.1836
Epoch 91/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8277
Epoch 00091: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 493us/sample - loss: 17.8596 - val_loss: 18.1865
Epoch 92/200
 20/121 [===>..........................] - ETA: 0s - loss: 16.8355
Epoch 00092: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 526us/sample - loss: 17.8597 - val_loss: 18.1873
Epoch 93/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0827
Epoch 00093: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 560us/sample - loss: 17.8596 - val_loss: 18.1853
Epoch 94/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2971
Epoch 00094: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 595us/sample - loss: 17.8581 - val_loss: 18.1818
Epoch 95/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8296
Epoch 00095: val_loss did not improve from 18.17732
121/121 [==============================] - 0s 631us/sample - loss: 17.8565 - val_loss: 18.1780
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00095: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b37047ff8d0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b37047f75f8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37047f72e8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b37047f7780> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3704803d68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704803630> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b370483aac8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b370483aa20> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3704841198> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704865c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370486d7b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704878518> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b370487fda0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b370488b0f0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3704893e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37048939b0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37048af5f8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37048af860> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37048af9e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37048b87b8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b37048b8940> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b37047ff8d0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b37047f75f8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37047f72e8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b37047f7780> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3704803d68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704803630> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b370483aac8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b370483aa20> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3704841198> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704865c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370486d7b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704878518> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b370487fda0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b370488b0f0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3704893e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37048939b0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37048af5f8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37048af860> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37048af9e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37048b87b8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b37048b8940> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b36cffde4a8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3704900160> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 82.2609
Epoch 00001: val_loss improved from inf to 29.34788, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 7ms/sample - loss: 55.1238 - val_loss: 29.3479
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 30.1013
Epoch 00002: val_loss improved from 29.34788 to 28.79357, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 2ms/sample - loss: 29.5027 - val_loss: 28.7936
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.6780
Epoch 00003: val_loss improved from 28.79357 to 21.70754, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 818us/sample - loss: 28.3624 - val_loss: 21.7075
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1783
Epoch 00004: val_loss did not improve from 21.70754
121/121 [==============================] - 0s 712us/sample - loss: 21.3886 - val_loss: 22.3345
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9870
Epoch 00005: val_loss improved from 21.70754 to 20.17093, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 912us/sample - loss: 21.6361 - val_loss: 20.1709
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.3875
Epoch 00006: val_loss improved from 20.17093 to 20.12865, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 834us/sample - loss: 20.6921 - val_loss: 20.1286
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2772
Epoch 00007: val_loss did not improve from 20.12865
121/121 [==============================] - 0s 578us/sample - loss: 20.4204 - val_loss: 20.2827
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6867
Epoch 00008: val_loss improved from 20.12865 to 20.03629, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 849us/sample - loss: 20.3786 - val_loss: 20.0363
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1345
Epoch 00009: val_loss improved from 20.03629 to 19.90371, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 797us/sample - loss: 20.3330 - val_loss: 19.9037
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1041
Epoch 00010: val_loss did not improve from 19.90371
121/121 [==============================] - 0s 632us/sample - loss: 20.1723 - val_loss: 19.9251
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0089
Epoch 00011: val_loss improved from 19.90371 to 19.73984, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 991us/sample - loss: 20.0839 - val_loss: 19.7398
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8778
Epoch 00012: val_loss did not improve from 19.73984
121/121 [==============================] - 0s 613us/sample - loss: 19.9896 - val_loss: 19.7477
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9184
Epoch 00013: val_loss improved from 19.73984 to 19.59103, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 867us/sample - loss: 19.9439 - val_loss: 19.5910
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8922
Epoch 00014: val_loss did not improve from 19.59103
121/121 [==============================] - 0s 605us/sample - loss: 19.9790 - val_loss: 19.6293
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0388
Epoch 00015: val_loss improved from 19.59103 to 19.50653, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 811us/sample - loss: 19.9096 - val_loss: 19.5065
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8077
Epoch 00016: val_loss improved from 19.50653 to 19.45727, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 834us/sample - loss: 19.8044 - val_loss: 19.4573
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1325
Epoch 00017: val_loss did not improve from 19.45727
121/121 [==============================] - 0s 577us/sample - loss: 19.7525 - val_loss: 19.5305
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7329
Epoch 00018: val_loss improved from 19.45727 to 19.36369, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 845us/sample - loss: 19.7468 - val_loss: 19.3637
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3884
Epoch 00019: val_loss did not improve from 19.36369
121/121 [==============================] - 0s 607us/sample - loss: 19.7234 - val_loss: 19.3908
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2807
Epoch 00020: val_loss improved from 19.36369 to 19.28836, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 736us/sample - loss: 19.6541 - val_loss: 19.2884
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6082
Epoch 00021: val_loss did not improve from 19.28836
121/121 [==============================] - 0s 744us/sample - loss: 19.5911 - val_loss: 19.4089
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3708
Epoch 00022: val_loss improved from 19.28836 to 19.22310, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 810us/sample - loss: 19.5873 - val_loss: 19.2231
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8764
Epoch 00023: val_loss did not improve from 19.22310
121/121 [==============================] - 0s 674us/sample - loss: 19.5131 - val_loss: 19.3119
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9745
Epoch 00024: val_loss improved from 19.22310 to 19.14776, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 940us/sample - loss: 19.5156 - val_loss: 19.1478
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7518
Epoch 00025: val_loss improved from 19.14776 to 19.10704, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 774us/sample - loss: 19.4867 - val_loss: 19.1070
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4084
Epoch 00026: val_loss improved from 19.10704 to 19.10528, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 780us/sample - loss: 19.4431 - val_loss: 19.1053
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6898
Epoch 00027: val_loss did not improve from 19.10528
121/121 [==============================] - 0s 649us/sample - loss: 19.4280 - val_loss: 19.1099
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2269
Epoch 00028: val_loss improved from 19.10528 to 19.01205, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 841us/sample - loss: 19.3602 - val_loss: 19.0120
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5686
Epoch 00029: val_loss did not improve from 19.01205
121/121 [==============================] - 0s 598us/sample - loss: 19.3423 - val_loss: 19.0436
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9241
Epoch 00030: val_loss did not improve from 19.01205

Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 561us/sample - loss: 19.4266 - val_loss: 19.0580
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8052
Epoch 00031: val_loss improved from 19.01205 to 18.92459, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 748us/sample - loss: 19.2486 - val_loss: 18.9246
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9323
Epoch 00032: val_loss did not improve from 18.92459
121/121 [==============================] - 0s 565us/sample - loss: 19.2355 - val_loss: 18.9516
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1237
Epoch 00033: val_loss did not improve from 18.92459

Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 619us/sample - loss: 19.2301 - val_loss: 18.9327
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8817
Epoch 00034: val_loss improved from 18.92459 to 18.88578, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 759us/sample - loss: 19.1709 - val_loss: 18.8858
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4999
Epoch 00035: val_loss improved from 18.88578 to 18.87820, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 769us/sample - loss: 19.1540 - val_loss: 18.8782
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.1610
Epoch 00036: val_loss did not improve from 18.87820
121/121 [==============================] - 0s 558us/sample - loss: 19.1437 - val_loss: 18.9085
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3149
Epoch 00037: val_loss improved from 18.87820 to 18.84868, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 19.1423 - val_loss: 18.8487
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9991
Epoch 00038: val_loss improved from 18.84868 to 18.83508, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 746us/sample - loss: 19.1195 - val_loss: 18.8351
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9313
Epoch 00039: val_loss did not improve from 18.83508
121/121 [==============================] - 0s 513us/sample - loss: 19.1137 - val_loss: 18.8494
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8324
Epoch 00040: val_loss did not improve from 18.83508

Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 604us/sample - loss: 19.1273 - val_loss: 18.9259
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8084
Epoch 00041: val_loss did not improve from 18.83508
121/121 [==============================] - 0s 579us/sample - loss: 19.1286 - val_loss: 18.8742
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0967
Epoch 00042: val_loss improved from 18.83508 to 18.83185, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 771us/sample - loss: 19.1019 - val_loss: 18.8319
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.3428
Epoch 00043: val_loss improved from 18.83185 to 18.81391, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 819us/sample - loss: 19.0895 - val_loss: 18.8139
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2744
Epoch 00044: val_loss improved from 18.81391 to 18.80864, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 737us/sample - loss: 19.0857 - val_loss: 18.8086
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3757
Epoch 00045: val_loss improved from 18.80864 to 18.80169, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 748us/sample - loss: 19.0853 - val_loss: 18.8017
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0811
Epoch 00046: val_loss improved from 18.80169 to 18.80109, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 776us/sample - loss: 19.0839 - val_loss: 18.8011
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0785
Epoch 00047: val_loss improved from 18.80109 to 18.79972, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 720us/sample - loss: 19.0782 - val_loss: 18.7997
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4448
Epoch 00048: val_loss improved from 18.79972 to 18.79436, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 19.0747 - val_loss: 18.7944
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4630
Epoch 00049: val_loss improved from 18.79436 to 18.77951, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 714us/sample - loss: 19.0722 - val_loss: 18.7795
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2026
Epoch 00050: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 541us/sample - loss: 19.0739 - val_loss: 18.7876
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0095
Epoch 00051: val_loss did not improve from 18.77951

Epoch 00051: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 544us/sample - loss: 19.0638 - val_loss: 18.7978
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8325
Epoch 00052: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 599us/sample - loss: 19.0596 - val_loss: 18.7925
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0302
Epoch 00053: val_loss did not improve from 18.77951

Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 571us/sample - loss: 19.0580 - val_loss: 18.7940
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2300
Epoch 00054: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 567us/sample - loss: 19.0573 - val_loss: 18.7927
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0260
Epoch 00055: val_loss did not improve from 18.77951

Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 558us/sample - loss: 19.0567 - val_loss: 18.7926
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1847
Epoch 00056: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 527us/sample - loss: 19.0562 - val_loss: 18.7905
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.0142
Epoch 00057: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 626us/sample - loss: 19.0563 - val_loss: 18.7879
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.5872
Epoch 00058: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 534us/sample - loss: 19.0556 - val_loss: 18.7869
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.6344
Epoch 00059: val_loss did not improve from 18.77951
121/121 [==============================] - 0s 571us/sample - loss: 19.0551 - val_loss: 18.7859
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00059: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/train/F018_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/validate/F018_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F018/test/F018_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3704f7d710> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b370500b748> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370500bb38> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b370500bcf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3705014c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705014550> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3705028b38> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3705028a90> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3705035080> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705057da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705060828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370506a588> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3705075e10> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3705080160> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3705087ef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705087a20> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37050a2668> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37050a28d0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37050a2a58> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37050ae828> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b37050ae9b0> False
Model: "model_8"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_5[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 57, 2)    578         conv2d_4[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3704f7d710> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b370500b748> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370500bb38> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b370500bcf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3705014c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705014550> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3705028b38> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3705028a90> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3705035080> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705057da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705060828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b370506a588> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3705075e10> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3705080160> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3705087ef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3705087a20> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37050a2668> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37050a28d0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b37050a2a58> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37050ae828> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b37050ae9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37040a08d0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b37050f2278> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/200
 20/242 [=>............................] - ETA: 12s - loss: 93.0287180/242 [=====================>........] - ETA: 0s - loss: 50.1253 
Epoch 00001: val_loss improved from inf to 24.75145, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 2s 7ms/sample - loss: 43.2162 - val_loss: 24.7515
Epoch 2/200
 20/242 [=>............................] - ETA: 0s - loss: 23.9134220/242 [==========================>...] - ETA: 0s - loss: 25.0747
Epoch 00002: val_loss improved from 24.75145 to 19.71045, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 478us/sample - loss: 24.7438 - val_loss: 19.7104
Epoch 3/200
 20/242 [=>............................] - ETA: 0s - loss: 21.3139
Epoch 00003: val_loss improved from 19.71045 to 19.09688, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 453us/sample - loss: 20.0118 - val_loss: 19.0969
Epoch 4/200
 20/242 [=>............................] - ETA: 0s - loss: 19.7468240/242 [============================>.] - ETA: 0s - loss: 19.0944
Epoch 00004: val_loss improved from 19.09688 to 18.48044, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 453us/sample - loss: 19.0928 - val_loss: 18.4804
Epoch 5/200
 20/242 [=>............................] - ETA: 0s - loss: 21.6706240/242 [============================>.] - ETA: 0s - loss: 18.4610
Epoch 00005: val_loss improved from 18.48044 to 18.06798, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 479us/sample - loss: 18.4800 - val_loss: 18.0680
Epoch 6/200
 20/242 [=>............................] - ETA: 0s - loss: 16.6308240/242 [============================>.] - ETA: 0s - loss: 17.9731
Epoch 00006: val_loss improved from 18.06798 to 17.65599, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 456us/sample - loss: 17.9791 - val_loss: 17.6560
Epoch 7/200
 20/242 [=>............................] - ETA: 0s - loss: 17.0542240/242 [============================>.] - ETA: 0s - loss: 17.5289
Epoch 00007: val_loss improved from 17.65599 to 17.29573, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 462us/sample - loss: 17.5538 - val_loss: 17.2957
Epoch 8/200
 20/242 [=>............................] - ETA: 0s - loss: 15.3996240/242 [============================>.] - ETA: 0s - loss: 17.2008
Epoch 00008: val_loss improved from 17.29573 to 16.98946, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 460us/sample - loss: 17.2016 - val_loss: 16.9895
Epoch 9/200
 20/242 [=>............................] - ETA: 0s - loss: 19.5033220/242 [==========================>...] - ETA: 0s - loss: 16.8473
Epoch 00009: val_loss improved from 16.98946 to 16.70941, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 470us/sample - loss: 16.8847 - val_loss: 16.7094
Epoch 10/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4291220/242 [==========================>...] - ETA: 0s - loss: 16.6202
Epoch 00010: val_loss improved from 16.70941 to 16.56070, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 483us/sample - loss: 16.6250 - val_loss: 16.5607
Epoch 11/200
 20/242 [=>............................] - ETA: 0s - loss: 17.9957240/242 [============================>.] - ETA: 0s - loss: 16.5088
Epoch 00011: val_loss improved from 16.56070 to 16.44954, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 458us/sample - loss: 16.4688 - val_loss: 16.4495
Epoch 12/200
 20/242 [=>............................] - ETA: 0s - loss: 16.6854240/242 [============================>.] - ETA: 0s - loss: 16.3290
Epoch 00012: val_loss improved from 16.44954 to 16.25702, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 454us/sample - loss: 16.3278 - val_loss: 16.2570
Epoch 13/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1669240/242 [============================>.] - ETA: 0s - loss: 16.2138
Epoch 00013: val_loss improved from 16.25702 to 16.18964, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 472us/sample - loss: 16.2164 - val_loss: 16.1896
Epoch 14/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1065220/242 [==========================>...] - ETA: 0s - loss: 16.1795
Epoch 00014: val_loss improved from 16.18964 to 16.12138, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 476us/sample - loss: 16.1692 - val_loss: 16.1214
Epoch 15/200
 20/242 [=>............................] - ETA: 0s - loss: 17.5977220/242 [==========================>...] - ETA: 0s - loss: 15.8982
Epoch 00015: val_loss improved from 16.12138 to 16.08525, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 488us/sample - loss: 16.0950 - val_loss: 16.0853
Epoch 16/200
 20/242 [=>............................] - ETA: 0s - loss: 15.0512240/242 [============================>.] - ETA: 0s - loss: 16.0312
Epoch 00016: val_loss improved from 16.08525 to 16.05366, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 457us/sample - loss: 16.0527 - val_loss: 16.0537
Epoch 17/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4564240/242 [============================>.] - ETA: 0s - loss: 16.1140
Epoch 00017: val_loss did not improve from 16.05366
242/242 [==============================] - 0s 391us/sample - loss: 16.0927 - val_loss: 16.1371
Epoch 18/200
 20/242 [=>............................] - ETA: 0s - loss: 15.9373240/242 [============================>.] - ETA: 0s - loss: 16.0418
Epoch 00018: val_loss improved from 16.05366 to 15.97593, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 457us/sample - loss: 16.0460 - val_loss: 15.9759
Epoch 19/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7122240/242 [============================>.] - ETA: 0s - loss: 15.9420
Epoch 00019: val_loss improved from 15.97593 to 15.95440, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 470us/sample - loss: 15.9633 - val_loss: 15.9544
Epoch 20/200
 20/242 [=>............................] - ETA: 0s - loss: 16.2786220/242 [==========================>...] - ETA: 0s - loss: 15.8896
Epoch 00020: val_loss improved from 15.95440 to 15.95425, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 481us/sample - loss: 15.9436 - val_loss: 15.9543
Epoch 21/200
 20/242 [=>............................] - ETA: 0s - loss: 16.3381240/242 [============================>.] - ETA: 0s - loss: 15.9331
Epoch 00021: val_loss improved from 15.95425 to 15.91947, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 461us/sample - loss: 15.9246 - val_loss: 15.9195
Epoch 22/200
 20/242 [=>............................] - ETA: 0s - loss: 15.0640240/242 [============================>.] - ETA: 0s - loss: 15.9208
Epoch 00022: val_loss did not improve from 15.91947
242/242 [==============================] - 0s 376us/sample - loss: 15.9123 - val_loss: 15.9211
Epoch 23/200
 20/242 [=>............................] - ETA: 0s - loss: 14.4095
Epoch 00023: val_loss did not improve from 15.91947

Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 353us/sample - loss: 15.9057 - val_loss: 15.9260
Epoch 24/200
 20/242 [=>............................] - ETA: 0s - loss: 15.8528240/242 [============================>.] - ETA: 0s - loss: 15.8980
Epoch 00024: val_loss improved from 15.91947 to 15.88792, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 447us/sample - loss: 15.8830 - val_loss: 15.8879
Epoch 25/200
 20/242 [=>............................] - ETA: 0s - loss: 18.3937240/242 [============================>.] - ETA: 0s - loss: 15.8776
Epoch 00025: val_loss improved from 15.88792 to 15.87509, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 455us/sample - loss: 15.8625 - val_loss: 15.8751
Epoch 26/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4371
Epoch 00026: val_loss improved from 15.87509 to 15.86663, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 439us/sample - loss: 15.8554 - val_loss: 15.8666
Epoch 27/200
 20/242 [=>............................] - ETA: 0s - loss: 15.3409240/242 [============================>.] - ETA: 0s - loss: 15.8761
Epoch 00027: val_loss improved from 15.86663 to 15.86334, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 449us/sample - loss: 15.8609 - val_loss: 15.8633
Epoch 28/200
 20/242 [=>............................] - ETA: 0s - loss: 15.2391200/242 [=======================>......] - ETA: 0s - loss: 15.7657
Epoch 00028: val_loss did not improve from 15.86334
242/242 [==============================] - 0s 453us/sample - loss: 15.8514 - val_loss: 15.9425
Epoch 29/200
 20/242 [=>............................] - ETA: 0s - loss: 14.7455180/242 [=====================>........] - ETA: 0s - loss: 15.7831
Epoch 00029: val_loss did not improve from 15.86334

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 528us/sample - loss: 15.8768 - val_loss: 15.9241
Epoch 30/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1299180/242 [=====================>........] - ETA: 0s - loss: 15.8337
Epoch 00030: val_loss improved from 15.86334 to 15.85544, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 608us/sample - loss: 15.8704 - val_loss: 15.8554
Epoch 31/200
 20/242 [=>............................] - ETA: 0s - loss: 17.2449180/242 [=====================>........] - ETA: 0s - loss: 15.9908
Epoch 00031: val_loss did not improve from 15.85544
242/242 [==============================] - 0s 476us/sample - loss: 15.8624 - val_loss: 15.8654
Epoch 32/200
 20/242 [=>............................] - ETA: 0s - loss: 18.5817200/242 [=======================>......] - ETA: 0s - loss: 15.9426
Epoch 00032: val_loss improved from 15.85544 to 15.84829, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 602us/sample - loss: 15.8416 - val_loss: 15.8483
Epoch 33/200
 20/242 [=>............................] - ETA: 0s - loss: 14.7115180/242 [=====================>........] - ETA: 0s - loss: 15.4394
Epoch 00033: val_loss did not improve from 15.84829
242/242 [==============================] - 0s 533us/sample - loss: 15.8306 - val_loss: 15.8526
Epoch 34/200
 20/242 [=>............................] - ETA: 0s - loss: 14.9784180/242 [=====================>........] - ETA: 0s - loss: 15.5878
Epoch 00034: val_loss improved from 15.84829 to 15.84422, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 607us/sample - loss: 15.8326 - val_loss: 15.8442
Epoch 35/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4156180/242 [=====================>........] - ETA: 0s - loss: 15.7313
Epoch 00035: val_loss did not improve from 15.84422
242/242 [==============================] - 0s 507us/sample - loss: 15.8408 - val_loss: 15.8513
Epoch 36/200
 20/242 [=>............................] - ETA: 0s - loss: 15.2739200/242 [=======================>......] - ETA: 0s - loss: 15.8734
Epoch 00036: val_loss did not improve from 15.84422

Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 456us/sample - loss: 15.8281 - val_loss: 15.8475
Epoch 37/200
 20/242 [=>............................] - ETA: 0s - loss: 16.2249180/242 [=====================>........] - ETA: 0s - loss: 16.1197
Epoch 00037: val_loss did not improve from 15.84422
242/242 [==============================] - 0s 555us/sample - loss: 15.8298 - val_loss: 15.8509
Epoch 38/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4253160/242 [==================>...........] - ETA: 0s - loss: 15.9082
Epoch 00038: val_loss improved from 15.84422 to 15.84024, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 716us/sample - loss: 15.8264 - val_loss: 15.8402
Epoch 39/200
 20/242 [=>............................] - ETA: 0s - loss: 13.0034180/242 [=====================>........] - ETA: 0s - loss: 15.9820
Epoch 00039: val_loss did not improve from 15.84024
242/242 [==============================] - 0s 480us/sample - loss: 15.8262 - val_loss: 15.8422
Epoch 40/200
 20/242 [=>............................] - ETA: 0s - loss: 15.9036180/242 [=====================>........] - ETA: 0s - loss: 15.7485
Epoch 00040: val_loss did not improve from 15.84024

Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 484us/sample - loss: 15.8218 - val_loss: 15.8426
Epoch 41/200
 20/242 [=>............................] - ETA: 0s - loss: 15.3071200/242 [=======================>......] - ETA: 0s - loss: 15.8710
Epoch 00041: val_loss did not improve from 15.84024
242/242 [==============================] - 0s 477us/sample - loss: 15.8211 - val_loss: 15.8455
Epoch 42/200
 20/242 [=>............................] - ETA: 0s - loss: 14.8434180/242 [=====================>........] - ETA: 0s - loss: 15.8518
Epoch 00042: val_loss did not improve from 15.84024

Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 504us/sample - loss: 15.8213 - val_loss: 15.8437
Epoch 43/200
 20/242 [=>............................] - ETA: 0s - loss: 16.2856180/242 [=====================>........] - ETA: 0s - loss: 15.9335
Epoch 00043: val_loss did not improve from 15.84024
242/242 [==============================] - 0s 542us/sample - loss: 15.8213 - val_loss: 15.8414
Epoch 44/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7487200/242 [=======================>......] - ETA: 0s - loss: 15.9105
Epoch 00044: val_loss did not improve from 15.84024

Epoch 00044: ReduceLROnPlateau reducing learning rate to 1e-05.
242/242 [==============================] - 0s 437us/sample - loss: 15.8211 - val_loss: 15.8412
Epoch 45/200
 20/242 [=>............................] - ETA: 0s - loss: 15.0818200/242 [=======================>......] - ETA: 0s - loss: 15.7547
Epoch 00045: val_loss did not improve from 15.84024
242/242 [==============================] - 0s 502us/sample - loss: 15.8207 - val_loss: 15.8405
Epoch 46/200
 20/242 [=>............................] - ETA: 0s - loss: 16.7945180/242 [=====================>........] - ETA: 0s - loss: 15.7024
Epoch 00046: val_loss improved from 15.84024 to 15.84004, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 606us/sample - loss: 15.8204 - val_loss: 15.8400
Epoch 47/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7149200/242 [=======================>......] - ETA: 0s - loss: 15.8762
Epoch 00047: val_loss improved from 15.84004 to 15.83905, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 523us/sample - loss: 15.8201 - val_loss: 15.8391
Epoch 48/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1677220/242 [==========================>...] - ETA: 0s - loss: 15.9211
Epoch 00048: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 407us/sample - loss: 15.8196 - val_loss: 15.8407
Epoch 49/200
 20/242 [=>............................] - ETA: 0s - loss: 14.2947220/242 [==========================>...] - ETA: 0s - loss: 15.7269
Epoch 00049: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 425us/sample - loss: 15.8200 - val_loss: 15.8416
Epoch 50/200
 20/242 [=>............................] - ETA: 0s - loss: 17.0373220/242 [==========================>...] - ETA: 0s - loss: 15.8152
Epoch 00050: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 399us/sample - loss: 15.8202 - val_loss: 15.8425
Epoch 51/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7981220/242 [==========================>...] - ETA: 0s - loss: 15.7350
Epoch 00051: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 412us/sample - loss: 15.8196 - val_loss: 15.8415
Epoch 52/200
 20/242 [=>............................] - ETA: 0s - loss: 13.7124160/242 [==================>...........] - ETA: 0s - loss: 15.7632
Epoch 00052: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 634us/sample - loss: 15.8190 - val_loss: 15.8400
Epoch 53/200
 20/242 [=>............................] - ETA: 0s - loss: 16.6458240/242 [============================>.] - ETA: 0s - loss: 15.8120
Epoch 00053: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 398us/sample - loss: 15.8191 - val_loss: 15.8400
Epoch 54/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4858220/242 [==========================>...] - ETA: 0s - loss: 15.9114
Epoch 00054: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 399us/sample - loss: 15.8190 - val_loss: 15.8406
Epoch 55/200
 20/242 [=>............................] - ETA: 0s - loss: 16.2398220/242 [==========================>...] - ETA: 0s - loss: 16.0356
Epoch 00055: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 431us/sample - loss: 15.8193 - val_loss: 15.8419
Epoch 56/200
 20/242 [=>............................] - ETA: 0s - loss: 14.4061220/242 [==========================>...] - ETA: 0s - loss: 15.8371
Epoch 00056: val_loss did not improve from 15.83905
242/242 [==============================] - 0s 419us/sample - loss: 15.8187 - val_loss: 15.8399
Epoch 57/200
 20/242 [=>............................] - ETA: 0s - loss: 17.4692220/242 [==========================>...] - ETA: 0s - loss: 15.8878
Epoch 00057: val_loss improved from 15.83905 to 15.83890, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 497us/sample - loss: 15.8187 - val_loss: 15.8389
Epoch 58/200
 20/242 [=>............................] - ETA: 0s - loss: 15.6324220/242 [==========================>...] - ETA: 0s - loss: 15.7471
Epoch 00058: val_loss improved from 15.83890 to 15.83805, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 494us/sample - loss: 15.8187 - val_loss: 15.8380
Epoch 59/200
 20/242 [=>............................] - ETA: 0s - loss: 16.4033220/242 [==========================>...] - ETA: 0s - loss: 15.8230
Epoch 00059: val_loss did not improve from 15.83805
242/242 [==============================] - 0s 397us/sample - loss: 15.8184 - val_loss: 15.8382
Epoch 60/200
 20/242 [=>............................] - ETA: 0s - loss: 13.6868220/242 [==========================>...] - ETA: 0s - loss: 15.6544
Epoch 00060: val_loss improved from 15.83805 to 15.83800, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 487us/sample - loss: 15.8184 - val_loss: 15.8380
Epoch 61/200
 20/242 [=>............................] - ETA: 0s - loss: 15.2768220/242 [==========================>...] - ETA: 0s - loss: 15.8678
Epoch 00061: val_loss improved from 15.83800 to 15.83778, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 505us/sample - loss: 15.8183 - val_loss: 15.8378
Epoch 62/200
 20/242 [=>............................] - ETA: 0s - loss: 15.3915220/242 [==========================>...] - ETA: 0s - loss: 15.5329
Epoch 00062: val_loss did not improve from 15.83778
242/242 [==============================] - 0s 408us/sample - loss: 15.8181 - val_loss: 15.8382
Epoch 63/200
 20/242 [=>............................] - ETA: 0s - loss: 14.6178240/242 [============================>.] - ETA: 0s - loss: 15.8369
Epoch 00063: val_loss did not improve from 15.83778
242/242 [==============================] - 0s 378us/sample - loss: 15.8179 - val_loss: 15.8390
Epoch 64/200
 20/242 [=>............................] - ETA: 0s - loss: 16.4815220/242 [==========================>...] - ETA: 0s - loss: 15.9593
Epoch 00064: val_loss did not improve from 15.83778
242/242 [==============================] - 0s 403us/sample - loss: 15.8178 - val_loss: 15.8390
Epoch 65/200
 20/242 [=>............................] - ETA: 0s - loss: 16.4072220/242 [==========================>...] - ETA: 0s - loss: 15.7373
Epoch 00065: val_loss improved from 15.83778 to 15.83775, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 528us/sample - loss: 15.8176 - val_loss: 15.8377
Epoch 66/200
 20/242 [=>............................] - ETA: 0s - loss: 16.9411220/242 [==========================>...] - ETA: 0s - loss: 15.8399
Epoch 00066: val_loss improved from 15.83775 to 15.83740, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 514us/sample - loss: 15.8177 - val_loss: 15.8374
Epoch 67/200
 20/242 [=>............................] - ETA: 0s - loss: 14.9787220/242 [==========================>...] - ETA: 0s - loss: 15.8660
Epoch 00067: val_loss did not improve from 15.83740
242/242 [==============================] - 0s 423us/sample - loss: 15.8174 - val_loss: 15.8375
Epoch 68/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7581220/242 [==========================>...] - ETA: 0s - loss: 15.7499
Epoch 00068: val_loss improved from 15.83740 to 15.83722, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 513us/sample - loss: 15.8173 - val_loss: 15.8372
Epoch 69/200
 20/242 [=>............................] - ETA: 0s - loss: 17.2134220/242 [==========================>...] - ETA: 0s - loss: 15.9443
Epoch 00069: val_loss did not improve from 15.83722
242/242 [==============================] - 0s 435us/sample - loss: 15.8176 - val_loss: 15.8374
Epoch 70/200
 20/242 [=>............................] - ETA: 0s - loss: 16.8919240/242 [============================>.] - ETA: 0s - loss: 15.8137
Epoch 00070: val_loss did not improve from 15.83722
242/242 [==============================] - 0s 386us/sample - loss: 15.8173 - val_loss: 15.8384
Epoch 71/200
 20/242 [=>............................] - ETA: 0s - loss: 14.9254240/242 [============================>.] - ETA: 0s - loss: 15.8471
Epoch 00071: val_loss did not improve from 15.83722
242/242 [==============================] - 0s 392us/sample - loss: 15.8169 - val_loss: 15.8376
Epoch 72/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7458240/242 [============================>.] - ETA: 0s - loss: 15.8029
Epoch 00072: val_loss improved from 15.83722 to 15.83689, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 499us/sample - loss: 15.8168 - val_loss: 15.8369
Epoch 73/200
 20/242 [=>............................] - ETA: 0s - loss: 15.9416200/242 [=======================>......] - ETA: 0s - loss: 16.1611
Epoch 00073: val_loss improved from 15.83689 to 15.83458, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F018/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 514us/sample - loss: 15.8177 - val_loss: 15.8346
Epoch 74/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4358220/242 [==========================>...] - ETA: 0s - loss: 15.8984
Epoch 00074: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 407us/sample - loss: 15.8173 - val_loss: 15.8360
Epoch 75/200
 20/242 [=>............................] - ETA: 0s - loss: 15.7559220/242 [==========================>...] - ETA: 0s - loss: 15.7916
Epoch 00075: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 419us/sample - loss: 15.8166 - val_loss: 15.8371
Epoch 76/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4311200/242 [=======================>......] - ETA: 0s - loss: 15.6683
Epoch 00076: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 424us/sample - loss: 15.8169 - val_loss: 15.8361
Epoch 77/200
 20/242 [=>............................] - ETA: 0s - loss: 17.4355220/242 [==========================>...] - ETA: 0s - loss: 15.7414
Epoch 00077: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 403us/sample - loss: 15.8163 - val_loss: 15.8362
Epoch 78/200
 20/242 [=>............................] - ETA: 0s - loss: 16.3009220/242 [==========================>...] - ETA: 0s - loss: 15.8943
Epoch 00078: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 404us/sample - loss: 15.8163 - val_loss: 15.8368
Epoch 79/200
 20/242 [=>............................] - ETA: 0s - loss: 15.4869220/242 [==========================>...] - ETA: 0s - loss: 16.0197
Epoch 00079: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 404us/sample - loss: 15.8163 - val_loss: 15.8363
Epoch 80/200
 20/242 [=>............................] - ETA: 0s - loss: 16.0224240/242 [============================>.] - ETA: 0s - loss: 15.7962
Epoch 00080: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 397us/sample - loss: 15.8159 - val_loss: 15.8363
Epoch 81/200
 20/242 [=>............................] - ETA: 0s - loss: 15.6440220/242 [==========================>...] - ETA: 0s - loss: 15.7609
Epoch 00081: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 441us/sample - loss: 15.8162 - val_loss: 15.8378
Epoch 82/200
 20/242 [=>............................] - ETA: 0s - loss: 15.3233220/242 [==========================>...] - ETA: 0s - loss: 15.5638
Epoch 00082: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 412us/sample - loss: 15.8156 - val_loss: 15.8356
Epoch 83/200
 20/242 [=>............................] - ETA: 0s - loss: 18.1617240/242 [============================>.] - ETA: 0s - loss: 15.7921
Epoch 00083: val_loss did not improve from 15.83458
242/242 [==============================] - 0s 389us/sample - loss: 15.8156 - val_loss: 15.8349
Epoch 00083: early stopping
done
