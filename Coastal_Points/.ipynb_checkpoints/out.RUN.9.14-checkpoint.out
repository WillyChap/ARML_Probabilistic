2020-11-09 20:54:57.651158: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 20:54:57.996691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 20:54:57.996937: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563d905ed4c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 20:54:57.996964: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 20:54:58.016196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 20:54:58.123129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:54:58.143766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:54:58.286310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:54:58.344694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:54:58.440097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:54:58.560761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:54:58.631223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:54:58.743897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:54:58.747296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:54:58.747437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:54:58.945539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:54:58.945609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:54:58.945640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:54:58.951794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:54:58.954276: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563d91368ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 20:54:58.954310: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 20:54:58.957044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:54:58.957165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:54:58.957185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:54:58.957201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:54:58.960942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:54:58.960962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:54:58.960977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:54:58.960992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:54:58.965173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:54:58.965219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:54:58.965231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:54:58.965241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:54:58.968312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:54:58.970235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:54:58.970293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:54:58.970311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:54:58.970326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:54:58.970341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:54:58.970356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:54:58.970370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:54:58.970385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:54:58.977198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:54:58.977234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:54:58.977245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:54:58.977254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:54:58.980295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 159.2613, 153.0905
Mean and standard deviation for "p_sfc" = 984.2210, 62.0846
Mean and standard deviation for "u_tr_p" = 12.6598, 12.3688
Mean and standard deviation for "v_tr_p" = 1.2479, 13.3425
Mean and standard deviation for "Z_p" = 5574.4522, 203.7064
Mean and standard deviation for "IWV" = 13.3759, 7.7243
2020-11-09 20:55:10.266306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:55:10.266828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:55:10.266855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:55:10.266871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:55:10.266889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:55:10.266904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:55:10.266921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:55:10.266937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:55:10.270162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:55:10.364881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:55:10.365033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:55:10.365056: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:55:10.365073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:55:10.365089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:55:10.365104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:55:10.365120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:55:10.386296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:55:10.391023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:55:10.391086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:55:10.391101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:55:10.391110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:55:10.394225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:55:18.055664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:55:20.374584: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 20:55:20.389587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 202.8885, 181.8474
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 1576 samples, validate on 121 samples
Epoch 1/50
  50/1576 [..............................] - ETA: 2:15 - loss: 146.7442 100/1576 [>.............................] - ETA: 1:06 - loss: 154.7945 300/1576 [====>.........................] - ETA: 19s - loss: 157.2027  500/1576 [========>.....................] - ETA: 9s - loss: 155.6803  700/1576 [============>.................] - ETA: 5s - loss: 153.7032 900/1576 [================>.............] - ETA: 3s - loss: 148.33291100/1576 [===================>..........] - ETA: 2s - loss: 136.76371300/1576 [=======================>......] - ETA: 1s - loss: 124.53651500/1576 [===========================>..] - ETA: 0s - loss: 114.7791
Epoch 00001: val_loss improved from inf to 78.79187, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 6s 4ms/sample - loss: 111.4120 - val_loss: 78.7919
Epoch 2/50
  50/1576 [..............................] - ETA: 0s - loss: 47.0662 250/1576 [===>..........................] - ETA: 0s - loss: 45.8955 450/1576 [=======>......................] - ETA: 0s - loss: 43.8421 650/1576 [===========>..................] - ETA: 0s - loss: 43.0716 850/1576 [===============>..............] - ETA: 0s - loss: 42.10731050/1576 [==================>...........] - ETA: 0s - loss: 41.22621250/1576 [======================>.......] - ETA: 0s - loss: 40.43671450/1576 [==========================>...] - ETA: 0s - loss: 39.8319
Epoch 00002: val_loss improved from 78.79187 to 77.17863, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 1s 346us/sample - loss: 39.4210 - val_loss: 77.1786
Epoch 3/50
  50/1576 [..............................] - ETA: 0s - loss: 35.5235 250/1576 [===>..........................] - ETA: 0s - loss: 33.8268 450/1576 [=======>......................] - ETA: 0s - loss: 33.5560 650/1576 [===========>..................] - ETA: 0s - loss: 33.2373 850/1576 [===============>..............] - ETA: 0s - loss: 33.21451050/1576 [==================>...........] - ETA: 0s - loss: 33.08951250/1576 [======================>.......] - ETA: 0s - loss: 32.64831450/1576 [==========================>...] - ETA: 0s - loss: 32.3198
Epoch 00003: val_loss improved from 77.17863 to 71.70432, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 298us/sample - loss: 32.1110 - val_loss: 71.7043
Epoch 4/50
  50/1576 [..............................] - ETA: 0s - loss: 29.2850 250/1576 [===>..........................] - ETA: 0s - loss: 29.9594 450/1576 [=======>......................] - ETA: 0s - loss: 29.7998 650/1576 [===========>..................] - ETA: 0s - loss: 29.6889 850/1576 [===============>..............] - ETA: 0s - loss: 29.90511050/1576 [==================>...........] - ETA: 0s - loss: 29.77671250/1576 [======================>.......] - ETA: 0s - loss: 29.64921450/1576 [==========================>...] - ETA: 0s - loss: 29.6344
Epoch 00004: val_loss improved from 71.70432 to 55.62630, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 315us/sample - loss: 29.8368 - val_loss: 55.6263
Epoch 5/50
  50/1576 [..............................] - ETA: 0s - loss: 29.4748 250/1576 [===>..........................] - ETA: 0s - loss: 29.0480 450/1576 [=======>......................] - ETA: 0s - loss: 28.4342 650/1576 [===========>..................] - ETA: 0s - loss: 28.4213 850/1576 [===============>..............] - ETA: 0s - loss: 28.59791050/1576 [==================>...........] - ETA: 0s - loss: 28.55141250/1576 [======================>.......] - ETA: 0s - loss: 28.63051450/1576 [==========================>...] - ETA: 0s - loss: 28.6618
Epoch 00005: val_loss improved from 55.62630 to 41.72507, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 317us/sample - loss: 28.7873 - val_loss: 41.7251
Epoch 6/50
  50/1576 [..............................] - ETA: 0s - loss: 29.7408 250/1576 [===>..........................] - ETA: 0s - loss: 29.6824 450/1576 [=======>......................] - ETA: 0s - loss: 30.2061 650/1576 [===========>..................] - ETA: 0s - loss: 30.1739 850/1576 [===============>..............] - ETA: 0s - loss: 29.70711050/1576 [==================>...........] - ETA: 0s - loss: 29.43241250/1576 [======================>.......] - ETA: 0s - loss: 29.33361450/1576 [==========================>...] - ETA: 0s - loss: 29.1032
Epoch 00006: val_loss improved from 41.72507 to 39.35252, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 315us/sample - loss: 28.8826 - val_loss: 39.3525
Epoch 7/50
  50/1576 [..............................] - ETA: 0s - loss: 29.3343 250/1576 [===>..........................] - ETA: 0s - loss: 27.6357 450/1576 [=======>......................] - ETA: 0s - loss: 27.6587 650/1576 [===========>..................] - ETA: 0s - loss: 27.3342 850/1576 [===============>..............] - ETA: 0s - loss: 27.08351050/1576 [==================>...........] - ETA: 0s - loss: 27.04101250/1576 [======================>.......] - ETA: 0s - loss: 27.13311450/1576 [==========================>...] - ETA: 0s - loss: 27.1605
Epoch 00007: val_loss improved from 39.35252 to 33.19642, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 1s 321us/sample - loss: 27.1487 - val_loss: 33.1964
Epoch 8/50
  50/1576 [..............................] - ETA: 0s - loss: 25.6576 250/1576 [===>..........................] - ETA: 0s - loss: 26.7616 450/1576 [=======>......................] - ETA: 0s - loss: 27.0508 650/1576 [===========>..................] - ETA: 0s - loss: 27.0144 850/1576 [===============>..............] - ETA: 0s - loss: 27.13441050/1576 [==================>...........] - ETA: 0s - loss: 27.08841250/1576 [======================>.......] - ETA: 0s - loss: 27.42731450/1576 [==========================>...] - ETA: 0s - loss: 27.4303
Epoch 00008: val_loss improved from 33.19642 to 29.95105, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 309us/sample - loss: 27.5191 - val_loss: 29.9511
Epoch 9/50
  50/1576 [..............................] - ETA: 0s - loss: 26.9442 250/1576 [===>..........................] - ETA: 0s - loss: 26.7107 450/1576 [=======>......................] - ETA: 0s - loss: 27.8832 650/1576 [===========>..................] - ETA: 0s - loss: 27.8689 850/1576 [===============>..............] - ETA: 0s - loss: 27.84811050/1576 [==================>...........] - ETA: 0s - loss: 27.77411250/1576 [======================>.......] - ETA: 0s - loss: 27.67911450/1576 [==========================>...] - ETA: 0s - loss: 27.7737
Epoch 00009: val_loss improved from 29.95105 to 26.86284, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 301us/sample - loss: 27.7046 - val_loss: 26.8628
Epoch 10/50
  50/1576 [..............................] - ETA: 0s - loss: 27.9980 250/1576 [===>..........................] - ETA: 0s - loss: 27.0953 450/1576 [=======>......................] - ETA: 0s - loss: 26.7693 650/1576 [===========>..................] - ETA: 0s - loss: 26.7560 850/1576 [===============>..............] - ETA: 0s - loss: 26.55061050/1576 [==================>...........] - ETA: 0s - loss: 26.41171250/1576 [======================>.......] - ETA: 0s - loss: 26.47301450/1576 [==========================>...] - ETA: 0s - loss: 26.6484
Epoch 00010: val_loss did not improve from 26.86284
1576/1576 [==============================] - 0s 285us/sample - loss: 26.6326 - val_loss: 27.9786
Epoch 11/50
  50/1576 [..............................] - ETA: 0s - loss: 24.9295 250/1576 [===>..........................] - ETA: 0s - loss: 27.1500 450/1576 [=======>......................] - ETA: 0s - loss: 27.5254 650/1576 [===========>..................] - ETA: 0s - loss: 27.7175 850/1576 [===============>..............] - ETA: 0s - loss: 27.72071050/1576 [==================>...........] - ETA: 0s - loss: 27.96191250/1576 [======================>.......] - ETA: 0s - loss: 27.98501450/1576 [==========================>...] - ETA: 0s - loss: 27.8884
Epoch 00011: val_loss did not improve from 26.86284

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1576/1576 [==============================] - 0s 288us/sample - loss: 27.9066 - val_loss: 27.4242
Epoch 12/50
  50/1576 [..............................] - ETA: 0s - loss: 26.5231 250/1576 [===>..........................] - ETA: 0s - loss: 26.6294 450/1576 [=======>......................] - ETA: 0s - loss: 26.4567 650/1576 [===========>..................] - ETA: 0s - loss: 26.8158 850/1576 [===============>..............] - ETA: 0s - loss: 26.78731050/1576 [==================>...........] - ETA: 0s - loss: 26.49041250/1576 [======================>.......] - ETA: 0s - loss: 26.43021450/1576 [==========================>...] - ETA: 0s - loss: 26.3471
Epoch 00012: val_loss improved from 26.86284 to 25.24444, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 313us/sample - loss: 26.4172 - val_loss: 25.2444
Epoch 13/50
  50/1576 [..............................] - ETA: 0s - loss: 26.1485 250/1576 [===>..........................] - ETA: 0s - loss: 25.9515 450/1576 [=======>......................] - ETA: 0s - loss: 25.9050 650/1576 [===========>..................] - ETA: 0s - loss: 26.5601 850/1576 [===============>..............] - ETA: 0s - loss: 26.61391050/1576 [==================>...........] - ETA: 0s - loss: 26.53081250/1576 [======================>.......] - ETA: 0s - loss: 26.44041450/1576 [==========================>...] - ETA: 0s - loss: 26.3598
Epoch 00013: val_loss improved from 25.24444 to 24.55766, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 308us/sample - loss: 26.3628 - val_loss: 24.5577
Epoch 14/50
  50/1576 [..............................] - ETA: 0s - loss: 26.4849 250/1576 [===>..........................] - ETA: 0s - loss: 26.7105 450/1576 [=======>......................] - ETA: 0s - loss: 26.2857 650/1576 [===========>..................] - ETA: 0s - loss: 26.3598 850/1576 [===============>..............] - ETA: 0s - loss: 26.36271050/1576 [==================>...........] - ETA: 0s - loss: 26.58221250/1576 [======================>.......] - ETA: 0s - loss: 26.60221450/1576 [==========================>...] - ETA: 0s - loss: 26.5286
Epoch 00014: val_loss did not improve from 24.55766
1576/1576 [==============================] - 0s 286us/sample - loss: 26.4024 - val_loss: 24.6585
Epoch 15/50
  50/1576 [..............................] - ETA: 0s - loss: 26.8275 250/1576 [===>..........................] - ETA: 0s - loss: 26.2443 450/1576 [=======>......................] - ETA: 0s - loss: 26.4166 650/1576 [===========>..................] - ETA: 0s - loss: 26.1297 850/1576 [===============>..............] - ETA: 0s - loss: 25.87821050/1576 [==================>...........] - ETA: 0s - loss: 25.97981250/1576 [======================>.......] - ETA: 0s - loss: 26.06411450/1576 [==========================>...] - ETA: 0s - loss: 26.1567
Epoch 00015: val_loss did not improve from 24.55766

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1576/1576 [==============================] - 0s 293us/sample - loss: 26.1373 - val_loss: 24.6064
Epoch 16/50
  50/1576 [..............................] - ETA: 0s - loss: 26.2907 250/1576 [===>..........................] - ETA: 0s - loss: 26.6027 450/1576 [=======>......................] - ETA: 0s - loss: 26.4729 650/1576 [===========>..................] - ETA: 0s - loss: 26.4788 850/1576 [===============>..............] - ETA: 0s - loss: 26.21251050/1576 [==================>...........] - ETA: 0s - loss: 26.22831250/1576 [======================>.......] - ETA: 0s - loss: 26.56121450/1576 [==========================>...] - ETA: 0s - loss: 26.4003
Epoch 00016: val_loss improved from 24.55766 to 24.13098, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 314us/sample - loss: 26.3955 - val_loss: 24.1310
Epoch 17/50
  50/1576 [..............................] - ETA: 0s - loss: 26.2577 250/1576 [===>..........................] - ETA: 0s - loss: 26.4709 450/1576 [=======>......................] - ETA: 0s - loss: 26.1148 650/1576 [===========>..................] - ETA: 0s - loss: 26.0682 850/1576 [===============>..............] - ETA: 0s - loss: 26.19451050/1576 [==================>...........] - ETA: 0s - loss: 26.33201250/1576 [======================>.......] - ETA: 0s - loss: 26.33371450/1576 [==========================>...] - ETA: 0s - loss: 26.2015
Epoch 00017: val_loss did not improve from 24.13098
1576/1576 [==============================] - 0s 290us/sample - loss: 26.2211 - val_loss: 24.5039
Epoch 18/50
  50/1576 [..............................] - ETA: 0s - loss: 25.4432 250/1576 [===>..........................] - ETA: 0s - loss: 26.8332 450/1576 [=======>......................] - ETA: 0s - loss: 26.7180 650/1576 [===========>..................] - ETA: 0s - loss: 26.4034 850/1576 [===============>..............] - ETA: 0s - loss: 26.27101050/1576 [==================>...........] - ETA: 0s - loss: 26.33861250/1576 [======================>.......] - ETA: 0s - loss: 26.16661450/1576 [==========================>...] - ETA: 0s - loss: 26.2340
Epoch 00018: val_loss improved from 24.13098 to 23.92340, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 307us/sample - loss: 26.2054 - val_loss: 23.9234
Epoch 19/50
  50/1576 [..............................] - ETA: 0s - loss: 27.4033 250/1576 [===>..........................] - ETA: 0s - loss: 26.3324 450/1576 [=======>......................] - ETA: 0s - loss: 25.8709 650/1576 [===========>..................] - ETA: 0s - loss: 25.9444 850/1576 [===============>..............] - ETA: 0s - loss: 26.25741050/1576 [==================>...........] - ETA: 0s - loss: 26.12991250/1576 [======================>.......] - ETA: 0s - loss: 26.21601450/1576 [==========================>...] - ETA: 0s - loss: 26.1661
Epoch 00019: val_loss did not improve from 23.92340
1576/1576 [==============================] - 0s 287us/sample - loss: 26.1359 - val_loss: 24.0268
Epoch 20/50
  50/1576 [..............................] - ETA: 0s - loss: 26.0810 250/1576 [===>..........................] - ETA: 0s - loss: 25.3160 450/1576 [=======>......................] - ETA: 0s - loss: 26.1381 650/1576 [===========>..................] - ETA: 0s - loss: 26.2736 850/1576 [===============>..............] - ETA: 0s - loss: 26.20791050/1576 [==================>...........] - ETA: 0s - loss: 26.23021250/1576 [======================>.......] - ETA: 0s - loss: 26.24361450/1576 [==========================>...] - ETA: 0s - loss: 26.0979
Epoch 00020: val_loss did not improve from 23.92340

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1576/1576 [==============================] - 0s 285us/sample - loss: 25.9745 - val_loss: 24.0342
Epoch 21/50
  50/1576 [..............................] - ETA: 0s - loss: 25.7999 250/1576 [===>..........................] - ETA: 0s - loss: 25.8237 450/1576 [=======>......................] - ETA: 0s - loss: 26.2157 650/1576 [===========>..................] - ETA: 0s - loss: 26.1255 850/1576 [===============>..............] - ETA: 0s - loss: 26.06681050/1576 [==================>...........] - ETA: 0s - loss: 25.96931250/1576 [======================>.......] - ETA: 0s - loss: 26.15421400/1576 [=========================>....] - ETA: 0s - loss: 26.1133
Epoch 00021: val_loss improved from 23.92340 to 23.83840, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 1s 336us/sample - loss: 26.0758 - val_loss: 23.8384
Epoch 22/50
  50/1576 [..............................] - ETA: 0s - loss: 25.7710 250/1576 [===>..........................] - ETA: 0s - loss: 25.6672 450/1576 [=======>......................] - ETA: 0s - loss: 25.9033 650/1576 [===========>..................] - ETA: 0s - loss: 25.6906 850/1576 [===============>..............] - ETA: 0s - loss: 25.92501050/1576 [==================>...........] - ETA: 0s - loss: 25.90461250/1576 [======================>.......] - ETA: 0s - loss: 25.83601450/1576 [==========================>...] - ETA: 0s - loss: 25.9350
Epoch 00022: val_loss did not improve from 23.83840
1576/1576 [==============================] - 0s 286us/sample - loss: 26.0223 - val_loss: 23.8745
Epoch 23/50
  50/1576 [..............................] - ETA: 0s - loss: 24.6425 250/1576 [===>..........................] - ETA: 0s - loss: 26.5190 450/1576 [=======>......................] - ETA: 0s - loss: 26.4723 650/1576 [===========>..................] - ETA: 0s - loss: 26.3641 850/1576 [===============>..............] - ETA: 0s - loss: 26.14401100/1576 [===================>..........] - ETA: 0s - loss: 26.01731300/1576 [=======================>......] - ETA: 0s - loss: 25.95261500/1576 [===========================>..] - ETA: 0s - loss: 26.0079
Epoch 00023: val_loss did not improve from 23.83840

Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1576/1576 [==============================] - 0s 284us/sample - loss: 26.0012 - val_loss: 23.9594
Epoch 24/50
  50/1576 [..............................] - ETA: 0s - loss: 22.9917 250/1576 [===>..........................] - ETA: 0s - loss: 25.6820 450/1576 [=======>......................] - ETA: 0s - loss: 26.4503 650/1576 [===========>..................] - ETA: 0s - loss: 26.1895 850/1576 [===============>..............] - ETA: 0s - loss: 26.30391050/1576 [==================>...........] - ETA: 0s - loss: 26.18731250/1576 [======================>.......] - ETA: 0s - loss: 26.10951450/1576 [==========================>...] - ETA: 0s - loss: 25.9671
Epoch 00024: val_loss improved from 23.83840 to 23.81895, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 307us/sample - loss: 26.0769 - val_loss: 23.8190
Epoch 25/50
  50/1576 [..............................] - ETA: 0s - loss: 24.2149 250/1576 [===>..........................] - ETA: 0s - loss: 25.2746 450/1576 [=======>......................] - ETA: 0s - loss: 25.3529 650/1576 [===========>..................] - ETA: 0s - loss: 25.6397 850/1576 [===============>..............] - ETA: 0s - loss: 25.88231050/1576 [==================>...........] - ETA: 0s - loss: 25.87571250/1576 [======================>.......] - ETA: 0s - loss: 25.93731450/1576 [==========================>...] - ETA: 0s - loss: 25.9445
Epoch 00025: val_loss improved from 23.81895 to 23.81308, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 1s 325us/sample - loss: 25.9276 - val_loss: 23.8131
Epoch 26/50
  50/1576 [..............................] - ETA: 0s - loss: 26.6422 250/1576 [===>..........................] - ETA: 0s - loss: 25.7960 450/1576 [=======>......................] - ETA: 0s - loss: 25.5284 650/1576 [===========>..................] - ETA: 0s - loss: 25.2794 850/1576 [===============>..............] - ETA: 0s - loss: 25.45381050/1576 [==================>...........] - ETA: 0s - loss: 25.83631250/1576 [======================>.......] - ETA: 0s - loss: 25.91351450/1576 [==========================>...] - ETA: 0s - loss: 26.0360
Epoch 00026: val_loss improved from 23.81308 to 23.81103, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 305us/sample - loss: 26.0496 - val_loss: 23.8110
Epoch 27/50
  50/1576 [..............................] - ETA: 0s - loss: 25.1324 250/1576 [===>..........................] - ETA: 0s - loss: 25.7515 450/1576 [=======>......................] - ETA: 0s - loss: 25.9167 650/1576 [===========>..................] - ETA: 0s - loss: 25.9782 850/1576 [===============>..............] - ETA: 0s - loss: 25.79161050/1576 [==================>...........] - ETA: 0s - loss: 25.79381250/1576 [======================>.......] - ETA: 0s - loss: 26.03881450/1576 [==========================>...] - ETA: 0s - loss: 25.9287
Epoch 00027: val_loss improved from 23.81103 to 23.80927, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 310us/sample - loss: 25.9528 - val_loss: 23.8093
Epoch 28/50
  50/1576 [..............................] - ETA: 0s - loss: 24.9834 250/1576 [===>..........................] - ETA: 0s - loss: 25.7320 450/1576 [=======>......................] - ETA: 0s - loss: 25.4702 650/1576 [===========>..................] - ETA: 0s - loss: 25.6837 850/1576 [===============>..............] - ETA: 0s - loss: 25.73211050/1576 [==================>...........] - ETA: 0s - loss: 25.64001250/1576 [======================>.......] - ETA: 0s - loss: 25.84231450/1576 [==========================>...] - ETA: 0s - loss: 25.8316
Epoch 00028: val_loss improved from 23.80927 to 23.80849, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 308us/sample - loss: 25.9800 - val_loss: 23.8085
Epoch 29/50
  50/1576 [..............................] - ETA: 0s - loss: 27.2237 250/1576 [===>..........................] - ETA: 0s - loss: 25.8197 450/1576 [=======>......................] - ETA: 0s - loss: 25.7423 650/1576 [===========>..................] - ETA: 0s - loss: 25.5770 850/1576 [===============>..............] - ETA: 0s - loss: 25.79461050/1576 [==================>...........] - ETA: 0s - loss: 25.82171250/1576 [======================>.......] - ETA: 0s - loss: 25.90531450/1576 [==========================>...] - ETA: 0s - loss: 25.9970
Epoch 00029: val_loss improved from 23.80849 to 23.80240, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 305us/sample - loss: 25.9717 - val_loss: 23.8024
Epoch 30/50
  50/1576 [..............................] - ETA: 0s - loss: 27.5476 250/1576 [===>..........................] - ETA: 0s - loss: 26.5698 450/1576 [=======>......................] - ETA: 0s - loss: 26.3462 650/1576 [===========>..................] - ETA: 0s - loss: 25.8883 850/1576 [===============>..............] - ETA: 0s - loss: 25.92201050/1576 [==================>...........] - ETA: 0s - loss: 25.84471250/1576 [======================>.......] - ETA: 0s - loss: 25.94441450/1576 [==========================>...] - ETA: 0s - loss: 25.8573
Epoch 00030: val_loss did not improve from 23.80240
1576/1576 [==============================] - 0s 283us/sample - loss: 25.9466 - val_loss: 23.8081
Epoch 31/50
  50/1576 [..............................] - ETA: 0s - loss: 25.7167 250/1576 [===>..........................] - ETA: 0s - loss: 26.4311 450/1576 [=======>......................] - ETA: 0s - loss: 26.6439 650/1576 [===========>..................] - ETA: 0s - loss: 26.4871 850/1576 [===============>..............] - ETA: 0s - loss: 26.17691050/1576 [==================>...........] - ETA: 0s - loss: 26.16831250/1576 [======================>.......] - ETA: 0s - loss: 26.07951450/1576 [==========================>...] - ETA: 0s - loss: 26.1029
Epoch 00031: val_loss improved from 23.80240 to 23.79427, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 312us/sample - loss: 26.0692 - val_loss: 23.7943
Epoch 32/50
  50/1576 [..............................] - ETA: 0s - loss: 25.5515 250/1576 [===>..........................] - ETA: 0s - loss: 26.0375 450/1576 [=======>......................] - ETA: 0s - loss: 26.0136 650/1576 [===========>..................] - ETA: 0s - loss: 26.0347 850/1576 [===============>..............] - ETA: 0s - loss: 25.91901050/1576 [==================>...........] - ETA: 0s - loss: 26.10161250/1576 [======================>.......] - ETA: 0s - loss: 26.09851450/1576 [==========================>...] - ETA: 0s - loss: 25.9530
Epoch 00032: val_loss improved from 23.79427 to 23.79228, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 306us/sample - loss: 26.0704 - val_loss: 23.7923
Epoch 33/50
  50/1576 [..............................] - ETA: 0s - loss: 25.7176 250/1576 [===>..........................] - ETA: 0s - loss: 25.0225 450/1576 [=======>......................] - ETA: 0s - loss: 24.9296 650/1576 [===========>..................] - ETA: 0s - loss: 25.3181 850/1576 [===============>..............] - ETA: 0s - loss: 25.52531050/1576 [==================>...........] - ETA: 0s - loss: 25.69101250/1576 [======================>.......] - ETA: 0s - loss: 25.88841450/1576 [==========================>...] - ETA: 0s - loss: 25.9794
Epoch 00033: val_loss improved from 23.79228 to 23.79103, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 310us/sample - loss: 26.0456 - val_loss: 23.7910
Epoch 34/50
  50/1576 [..............................] - ETA: 0s - loss: 26.5437 250/1576 [===>..........................] - ETA: 0s - loss: 26.3139 450/1576 [=======>......................] - ETA: 0s - loss: 25.9652 650/1576 [===========>..................] - ETA: 0s - loss: 25.8504 850/1576 [===============>..............] - ETA: 0s - loss: 25.98251050/1576 [==================>...........] - ETA: 0s - loss: 25.96031250/1576 [======================>.......] - ETA: 0s - loss: 25.93841450/1576 [==========================>...] - ETA: 0s - loss: 25.9458
Epoch 00034: val_loss improved from 23.79103 to 23.78966, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 308us/sample - loss: 25.9664 - val_loss: 23.7897
Epoch 35/50
  50/1576 [..............................] - ETA: 0s - loss: 25.0901 250/1576 [===>..........................] - ETA: 0s - loss: 25.3104 450/1576 [=======>......................] - ETA: 0s - loss: 26.0305 650/1576 [===========>..................] - ETA: 0s - loss: 25.8481 850/1576 [===============>..............] - ETA: 0s - loss: 26.14461050/1576 [==================>...........] - ETA: 0s - loss: 26.18881250/1576 [======================>.......] - ETA: 0s - loss: 26.04831450/1576 [==========================>...] - ETA: 0s - loss: 26.1209
Epoch 00035: val_loss improved from 23.78966 to 23.78467, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 305us/sample - loss: 26.1375 - val_loss: 23.7847
Epoch 36/50
  50/1576 [..............................] - ETA: 0s - loss: 29.4351 250/1576 [===>..........................] - ETA: 0s - loss: 26.2833 450/1576 [=======>......................] - ETA: 0s - loss: 26.4476 650/1576 [===========>..................] - ETA: 0s - loss: 26.1986 850/1576 [===============>..............] - ETA: 0s - loss: 26.23591050/1576 [==================>...........] - ETA: 0s - loss: 26.04331250/1576 [======================>.......] - ETA: 0s - loss: 26.08181450/1576 [==========================>...] - ETA: 0s - loss: 26.0020
Epoch 00036: val_loss did not improve from 23.78467
1576/1576 [==============================] - 0s 289us/sample - loss: 26.1004 - val_loss: 23.7863
Epoch 37/50
  50/1576 [..............................] - ETA: 0s - loss: 24.1973 250/1576 [===>..........................] - ETA: 0s - loss: 25.2769 450/1576 [=======>......................] - ETA: 0s - loss: 25.5054 650/1576 [===========>..................] - ETA: 0s - loss: 25.8044 850/1576 [===============>..............] - ETA: 0s - loss: 25.75521050/1576 [==================>...........] - ETA: 0s - loss: 25.74541300/1576 [=======================>......] - ETA: 0s - loss: 25.83841500/1576 [===========================>..] - ETA: 0s - loss: 25.8906
Epoch 00037: val_loss improved from 23.78467 to 23.78120, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 299us/sample - loss: 25.8963 - val_loss: 23.7812
Epoch 38/50
  50/1576 [..............................] - ETA: 0s - loss: 26.8519 250/1576 [===>..........................] - ETA: 0s - loss: 25.6957 450/1576 [=======>......................] - ETA: 0s - loss: 25.6688 650/1576 [===========>..................] - ETA: 0s - loss: 25.7973 850/1576 [===============>..............] - ETA: 0s - loss: 25.55711050/1576 [==================>...........] - ETA: 0s - loss: 26.04171250/1576 [======================>.......] - ETA: 0s - loss: 26.06801450/1576 [==========================>...] - ETA: 0s - loss: 26.0179
Epoch 00038: val_loss improved from 23.78120 to 23.78111, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 303us/sample - loss: 26.0136 - val_loss: 23.7811
Epoch 39/50
  50/1576 [..............................] - ETA: 0s - loss: 24.1309 250/1576 [===>..........................] - ETA: 0s - loss: 25.9695 450/1576 [=======>......................] - ETA: 0s - loss: 26.3770 650/1576 [===========>..................] - ETA: 0s - loss: 26.2664 850/1576 [===============>..............] - ETA: 0s - loss: 26.07291050/1576 [==================>...........] - ETA: 0s - loss: 25.97091250/1576 [======================>.......] - ETA: 0s - loss: 25.96051450/1576 [==========================>...] - ETA: 0s - loss: 25.9569
Epoch 00039: val_loss did not improve from 23.78111

Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1576/1576 [==============================] - 0s 293us/sample - loss: 25.8857 - val_loss: 23.7898
Epoch 40/50
  50/1576 [..............................] - ETA: 0s - loss: 26.6545 250/1576 [===>..........................] - ETA: 0s - loss: 26.5832 450/1576 [=======>......................] - ETA: 0s - loss: 26.1728 650/1576 [===========>..................] - ETA: 0s - loss: 26.1505 850/1576 [===============>..............] - ETA: 0s - loss: 25.85861050/1576 [==================>...........] - ETA: 0s - loss: 25.97561250/1576 [======================>.......] - ETA: 0s - loss: 26.09661450/1576 [==========================>...] - ETA: 0s - loss: 26.0583
Epoch 00040: val_loss improved from 23.78111 to 23.77112, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 309us/sample - loss: 25.9920 - val_loss: 23.7711
Epoch 41/50
  50/1576 [..............................] - ETA: 0s - loss: 24.0020 250/1576 [===>..........................] - ETA: 0s - loss: 25.5480 450/1576 [=======>......................] - ETA: 0s - loss: 25.8355 650/1576 [===========>..................] - ETA: 0s - loss: 25.9659 850/1576 [===============>..............] - ETA: 0s - loss: 26.02981100/1576 [===================>..........] - ETA: 0s - loss: 25.83781350/1576 [========================>.....] - ETA: 0s - loss: 25.95631550/1576 [============================>.] - ETA: 0s - loss: 25.9818
Epoch 00041: val_loss did not improve from 23.77112
1576/1576 [==============================] - 0s 271us/sample - loss: 25.9747 - val_loss: 23.7711
Epoch 42/50
  50/1576 [..............................] - ETA: 0s - loss: 25.0596 250/1576 [===>..........................] - ETA: 0s - loss: 26.2006 450/1576 [=======>......................] - ETA: 0s - loss: 25.9070 650/1576 [===========>..................] - ETA: 0s - loss: 25.9158 850/1576 [===============>..............] - ETA: 0s - loss: 26.03541050/1576 [==================>...........] - ETA: 0s - loss: 25.92811250/1576 [======================>.......] - ETA: 0s - loss: 25.96071400/1576 [=========================>....] - ETA: 0s - loss: 25.86141550/1576 [============================>.] - ETA: 0s - loss: 25.9417
Epoch 00042: val_loss improved from 23.77112 to 23.76785, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 1s 333us/sample - loss: 25.9568 - val_loss: 23.7678
Epoch 43/50
  50/1576 [..............................] - ETA: 0s - loss: 26.7574 250/1576 [===>..........................] - ETA: 0s - loss: 26.8702 450/1576 [=======>......................] - ETA: 0s - loss: 26.1835 650/1576 [===========>..................] - ETA: 0s - loss: 25.9574 850/1576 [===============>..............] - ETA: 0s - loss: 25.89871050/1576 [==================>...........] - ETA: 0s - loss: 25.84041250/1576 [======================>.......] - ETA: 0s - loss: 25.77341450/1576 [==========================>...] - ETA: 0s - loss: 25.9206
Epoch 00043: val_loss improved from 23.76785 to 23.76638, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 317us/sample - loss: 25.9169 - val_loss: 23.7664
Epoch 44/50
  50/1576 [..............................] - ETA: 0s - loss: 25.3623 250/1576 [===>..........................] - ETA: 0s - loss: 25.7803 450/1576 [=======>......................] - ETA: 0s - loss: 26.4253 650/1576 [===========>..................] - ETA: 0s - loss: 26.4357 850/1576 [===============>..............] - ETA: 0s - loss: 26.39551050/1576 [==================>...........] - ETA: 0s - loss: 26.40691250/1576 [======================>.......] - ETA: 0s - loss: 26.32641450/1576 [==========================>...] - ETA: 0s - loss: 26.1949
Epoch 00044: val_loss improved from 23.76638 to 23.76435, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 315us/sample - loss: 26.1361 - val_loss: 23.7643
Epoch 45/50
  50/1576 [..............................] - ETA: 0s - loss: 27.1933 250/1576 [===>..........................] - ETA: 0s - loss: 26.8657 450/1576 [=======>......................] - ETA: 0s - loss: 26.4460 650/1576 [===========>..................] - ETA: 0s - loss: 26.1536 850/1576 [===============>..............] - ETA: 0s - loss: 26.09551050/1576 [==================>...........] - ETA: 0s - loss: 26.37931250/1576 [======================>.......] - ETA: 0s - loss: 26.22461450/1576 [==========================>...] - ETA: 0s - loss: 26.1560
Epoch 00045: val_loss did not improve from 23.76435
1576/1576 [==============================] - 0s 287us/sample - loss: 26.1077 - val_loss: 23.7708
Epoch 46/50
  50/1576 [..............................] - ETA: 0s - loss: 31.1320 250/1576 [===>..........................] - ETA: 0s - loss: 26.7826 450/1576 [=======>......................] - ETA: 0s - loss: 26.4495 650/1576 [===========>..................] - ETA: 0s - loss: 26.2603 850/1576 [===============>..............] - ETA: 0s - loss: 26.42191050/1576 [==================>...........] - ETA: 0s - loss: 26.08301250/1576 [======================>.......] - ETA: 0s - loss: 26.02831450/1576 [==========================>...] - ETA: 0s - loss: 26.0406
Epoch 00046: val_loss did not improve from 23.76435

Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-05.
1576/1576 [==============================] - 0s 292us/sample - loss: 26.0450 - val_loss: 23.7671
Epoch 47/50
  50/1576 [..............................] - ETA: 0s - loss: 26.3977 250/1576 [===>..........................] - ETA: 0s - loss: 26.1536 450/1576 [=======>......................] - ETA: 0s - loss: 26.1178 650/1576 [===========>..................] - ETA: 0s - loss: 25.9471 900/1576 [================>.............] - ETA: 0s - loss: 25.81931100/1576 [===================>..........] - ETA: 0s - loss: 26.00551300/1576 [=======================>......] - ETA: 0s - loss: 25.91271500/1576 [===========================>..] - ETA: 0s - loss: 26.0423
Epoch 00047: val_loss did not improve from 23.76435
1576/1576 [==============================] - 0s 274us/sample - loss: 25.9696 - val_loss: 23.7649
Epoch 48/50
  50/1576 [..............................] - ETA: 0s - loss: 25.7986 250/1576 [===>..........................] - ETA: 0s - loss: 26.0918 450/1576 [=======>......................] - ETA: 0s - loss: 26.0771 650/1576 [===========>..................] - ETA: 0s - loss: 25.8484 850/1576 [===============>..............] - ETA: 0s - loss: 25.89991050/1576 [==================>...........] - ETA: 0s - loss: 26.04301250/1576 [======================>.......] - ETA: 0s - loss: 26.00981450/1576 [==========================>...] - ETA: 0s - loss: 26.0936
Epoch 00048: val_loss improved from 23.76435 to 23.76112, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 314us/sample - loss: 25.9910 - val_loss: 23.7611
Epoch 49/50
  50/1576 [..............................] - ETA: 0s - loss: 25.4644 250/1576 [===>..........................] - ETA: 0s - loss: 25.4499 450/1576 [=======>......................] - ETA: 0s - loss: 26.0430 650/1576 [===========>..................] - ETA: 0s - loss: 25.9365 850/1576 [===============>..............] - ETA: 0s - loss: 25.91381050/1576 [==================>...........] - ETA: 0s - loss: 25.86761250/1576 [======================>.......] - ETA: 0s - loss: 25.86821450/1576 [==========================>...] - ETA: 0s - loss: 25.9296
Epoch 00049: val_loss improved from 23.76112 to 23.75948, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1576/1576 [==============================] - 0s 301us/sample - loss: 25.9099 - val_loss: 23.7595
Epoch 50/50
  50/1576 [..............................] - ETA: 0s - loss: 25.8435 250/1576 [===>..........................] - ETA: 0s - loss: 25.8779 450/1576 [=======>......................] - ETA: 0s - loss: 25.9036 650/1576 [===========>..................] - ETA: 0s - loss: 26.1827 900/1576 [================>.............] - ETA: 0s - loss: 25.79411100/1576 [===================>..........] - ETA: 0s - loss: 25.77031300/1576 [=======================>......] - ETA: 0s - loss: 25.89961500/1576 [===========================>..] - ETA: 0s - loss: 26.0307
Epoch 00050: val_loss did not improve from 23.75948
1576/1576 [==============================] - 0s 281us/sample - loss: 26.0130 - val_loss: 23.7613
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 1576 samples, validate on 121 samples
Epoch 1/50
  50/1576 [..............................] - ETA: 1:58 - loss: 149.7579 200/1576 [==>...........................] - ETA: 27s - loss: 158.8956  400/1576 [======>.......................] - ETA: 11s - loss: 155.2807 600/1576 [==========>...................] - ETA: 6s - loss: 150.5957  800/1576 [==============>...............] - ETA: 3s - loss: 140.13181000/1576 [==================>...........] - ETA: 2s - loss: 124.16531200/1576 [=====================>........] - ETA: 1s - loss: 113.74041400/1576 [=========================>....] - ETA: 0s - loss: 104.1883
Epoch 00001: val_loss improved from inf to 79.31521, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 5s 3ms/sample - loss: 98.1701 - val_loss: 79.3152
Epoch 2/50
  50/1576 [..............................] - ETA: 0s - loss: 50.6901 250/1576 [===>..........................] - ETA: 0s - loss: 42.8114 450/1576 [=======>......................] - ETA: 0s - loss: 42.0459 650/1576 [===========>..................] - ETA: 0s - loss: 40.8217 850/1576 [===============>..............] - ETA: 0s - loss: 39.86021050/1576 [==================>...........] - ETA: 0s - loss: 39.00141250/1576 [======================>.......] - ETA: 0s - loss: 38.43291450/1576 [==========================>...] - ETA: 0s - loss: 37.9144
Epoch 00002: val_loss improved from 79.31521 to 75.39855, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 1s 319us/sample - loss: 37.4413 - val_loss: 75.3986
Epoch 3/50
  50/1576 [..............................] - ETA: 0s - loss: 35.3634 250/1576 [===>..........................] - ETA: 0s - loss: 33.4364 450/1576 [=======>......................] - ETA: 0s - loss: 32.9417 650/1576 [===========>..................] - ETA: 0s - loss: 32.4226 850/1576 [===============>..............] - ETA: 0s - loss: 32.34371050/1576 [==================>...........] - ETA: 0s - loss: 32.15491250/1576 [======================>.......] - ETA: 0s - loss: 32.04761450/1576 [==========================>...] - ETA: 0s - loss: 31.9108
Epoch 00003: val_loss improved from 75.39855 to 62.05054, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 315us/sample - loss: 31.8816 - val_loss: 62.0505
Epoch 4/50
  50/1576 [..............................] - ETA: 0s - loss: 31.7595 250/1576 [===>..........................] - ETA: 0s - loss: 30.0251 450/1576 [=======>......................] - ETA: 0s - loss: 29.9751 650/1576 [===========>..................] - ETA: 0s - loss: 29.6786 850/1576 [===============>..............] - ETA: 0s - loss: 29.48191050/1576 [==================>...........] - ETA: 0s - loss: 29.27401250/1576 [======================>.......] - ETA: 0s - loss: 29.20831450/1576 [==========================>...] - ETA: 0s - loss: 29.0951
Epoch 00004: val_loss improved from 62.05054 to 55.68915, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 310us/sample - loss: 29.0416 - val_loss: 55.6891
Epoch 5/50
  50/1576 [..............................] - ETA: 0s - loss: 29.3634 250/1576 [===>..........................] - ETA: 0s - loss: 28.8487 450/1576 [=======>......................] - ETA: 0s - loss: 28.8237 650/1576 [===========>..................] - ETA: 0s - loss: 28.4997 850/1576 [===============>..............] - ETA: 0s - loss: 28.61541050/1576 [==================>...........] - ETA: 0s - loss: 28.52571250/1576 [======================>.......] - ETA: 0s - loss: 28.47811450/1576 [==========================>...] - ETA: 0s - loss: 28.1988
Epoch 00005: val_loss improved from 55.68915 to 48.65015, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 310us/sample - loss: 28.0892 - val_loss: 48.6501
Epoch 6/50
  50/1576 [..............................] - ETA: 0s - loss: 27.2287 250/1576 [===>..........................] - ETA: 0s - loss: 27.1471 450/1576 [=======>......................] - ETA: 0s - loss: 27.4046 650/1576 [===========>..................] - ETA: 0s - loss: 27.6430 850/1576 [===============>..............] - ETA: 0s - loss: 27.58231050/1576 [==================>...........] - ETA: 0s - loss: 27.64331250/1576 [======================>.......] - ETA: 0s - loss: 27.61881450/1576 [==========================>...] - ETA: 0s - loss: 27.4640
Epoch 00006: val_loss improved from 48.65015 to 37.13621, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 308us/sample - loss: 27.4279 - val_loss: 37.1362
Epoch 7/50
  50/1576 [..............................] - ETA: 0s - loss: 26.2009 250/1576 [===>..........................] - ETA: 0s - loss: 26.3695 400/1576 [======>.......................] - ETA: 0s - loss: 26.3366 600/1576 [==========>...................] - ETA: 0s - loss: 26.6376 800/1576 [==============>...............] - ETA: 0s - loss: 26.67641000/1576 [==================>...........] - ETA: 0s - loss: 26.51511200/1576 [=====================>........] - ETA: 0s - loss: 26.83941400/1576 [=========================>....] - ETA: 0s - loss: 27.0181
Epoch 00007: val_loss improved from 37.13621 to 29.56904, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 1s 336us/sample - loss: 27.0815 - val_loss: 29.5690
Epoch 8/50
  50/1576 [..............................] - ETA: 0s - loss: 26.2441 250/1576 [===>..........................] - ETA: 0s - loss: 26.6877 450/1576 [=======>......................] - ETA: 0s - loss: 27.2015 650/1576 [===========>..................] - ETA: 0s - loss: 27.1401 850/1576 [===============>..............] - ETA: 0s - loss: 27.03521050/1576 [==================>...........] - ETA: 0s - loss: 27.30131250/1576 [======================>.......] - ETA: 0s - loss: 27.20511450/1576 [==========================>...] - ETA: 0s - loss: 27.0845
Epoch 00008: val_loss improved from 29.56904 to 28.07446, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 306us/sample - loss: 26.9414 - val_loss: 28.0745
Epoch 9/50
  50/1576 [..............................] - ETA: 0s - loss: 25.8355 250/1576 [===>..........................] - ETA: 0s - loss: 27.1444 450/1576 [=======>......................] - ETA: 0s - loss: 26.9841 650/1576 [===========>..................] - ETA: 0s - loss: 27.2177 850/1576 [===============>..............] - ETA: 0s - loss: 26.88181050/1576 [==================>...........] - ETA: 0s - loss: 26.98951250/1576 [======================>.......] - ETA: 0s - loss: 26.76081450/1576 [==========================>...] - ETA: 0s - loss: 26.6897
Epoch 00009: val_loss improved from 28.07446 to 25.68821, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 305us/sample - loss: 26.6651 - val_loss: 25.6882
Epoch 10/50
  50/1576 [..............................] - ETA: 0s - loss: 29.3777 250/1576 [===>..........................] - ETA: 0s - loss: 27.7942 450/1576 [=======>......................] - ETA: 0s - loss: 26.9446 650/1576 [===========>..................] - ETA: 0s - loss: 26.8666 850/1576 [===============>..............] - ETA: 0s - loss: 26.84191050/1576 [==================>...........] - ETA: 0s - loss: 26.83141250/1576 [======================>.......] - ETA: 0s - loss: 26.82751450/1576 [==========================>...] - ETA: 0s - loss: 26.7317
Epoch 00010: val_loss improved from 25.68821 to 25.10240, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 314us/sample - loss: 26.8139 - val_loss: 25.1024
Epoch 11/50
  50/1576 [..............................] - ETA: 0s - loss: 26.0586 250/1576 [===>..........................] - ETA: 0s - loss: 27.5395 450/1576 [=======>......................] - ETA: 0s - loss: 26.9317 650/1576 [===========>..................] - ETA: 0s - loss: 26.6191 850/1576 [===============>..............] - ETA: 0s - loss: 26.76301050/1576 [==================>...........] - ETA: 0s - loss: 26.78171250/1576 [======================>.......] - ETA: 0s - loss: 26.63161450/1576 [==========================>...] - ETA: 0s - loss: 26.4158
Epoch 00011: val_loss improved from 25.10240 to 24.84377, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 309us/sample - loss: 26.4663 - val_loss: 24.8438
Epoch 12/50
  50/1576 [..............................] - ETA: 0s - loss: 26.9542 250/1576 [===>..........................] - ETA: 0s - loss: 25.6198 450/1576 [=======>......................] - ETA: 0s - loss: 26.0528 650/1576 [===========>..................] - ETA: 0s - loss: 26.1254 850/1576 [===============>..............] - ETA: 0s - loss: 26.05881050/1576 [==================>...........] - ETA: 0s - loss: 26.20091250/1576 [======================>.......] - ETA: 0s - loss: 26.19351450/1576 [==========================>...] - ETA: 0s - loss: 26.2258
Epoch 00012: val_loss improved from 24.84377 to 23.97051, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 307us/sample - loss: 26.2279 - val_loss: 23.9705
Epoch 13/50
  50/1576 [..............................] - ETA: 0s - loss: 25.6956 250/1576 [===>..........................] - ETA: 0s - loss: 25.4232 450/1576 [=======>......................] - ETA: 0s - loss: 25.4965 650/1576 [===========>..................] - ETA: 0s - loss: 25.7310 850/1576 [===============>..............] - ETA: 0s - loss: 25.89861050/1576 [==================>...........] - ETA: 0s - loss: 25.90541250/1576 [======================>.......] - ETA: 0s - loss: 26.08681450/1576 [==========================>...] - ETA: 0s - loss: 25.9983
Epoch 00013: val_loss did not improve from 23.97051
1576/1576 [==============================] - 0s 286us/sample - loss: 26.0613 - val_loss: 25.5046
Epoch 14/50
  50/1576 [..............................] - ETA: 0s - loss: 27.2140 250/1576 [===>..........................] - ETA: 0s - loss: 26.6607 450/1576 [=======>......................] - ETA: 0s - loss: 26.2255 650/1576 [===========>..................] - ETA: 0s - loss: 26.3477 850/1576 [===============>..............] - ETA: 0s - loss: 25.93221050/1576 [==================>...........] - ETA: 0s - loss: 25.97891250/1576 [======================>.......] - ETA: 0s - loss: 25.88451450/1576 [==========================>...] - ETA: 0s - loss: 25.8212
Epoch 00014: val_loss improved from 23.97051 to 23.59375, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 306us/sample - loss: 25.8652 - val_loss: 23.5938
Epoch 15/50
  50/1576 [..............................] - ETA: 0s - loss: 25.6955 250/1576 [===>..........................] - ETA: 0s - loss: 25.9241 450/1576 [=======>......................] - ETA: 0s - loss: 26.3047 650/1576 [===========>..................] - ETA: 0s - loss: 26.3564 850/1576 [===============>..............] - ETA: 0s - loss: 26.42171050/1576 [==================>...........] - ETA: 0s - loss: 26.29441250/1576 [======================>.......] - ETA: 0s - loss: 26.42311450/1576 [==========================>...] - ETA: 0s - loss: 26.3202
Epoch 00015: val_loss improved from 23.59375 to 23.12194, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 312us/sample - loss: 26.2290 - val_loss: 23.1219
Epoch 16/50
  50/1576 [..............................] - ETA: 0s - loss: 23.6942 250/1576 [===>..........................] - ETA: 0s - loss: 25.0652 450/1576 [=======>......................] - ETA: 0s - loss: 25.6941 650/1576 [===========>..................] - ETA: 0s - loss: 25.7064 850/1576 [===============>..............] - ETA: 0s - loss: 26.02751050/1576 [==================>...........] - ETA: 0s - loss: 25.84141250/1576 [======================>.......] - ETA: 0s - loss: 25.76421450/1576 [==========================>...] - ETA: 0s - loss: 25.8098
Epoch 00016: val_loss did not improve from 23.12194
1576/1576 [==============================] - 0s 289us/sample - loss: 25.9188 - val_loss: 24.0616
Epoch 17/50
  50/1576 [..............................] - ETA: 0s - loss: 26.4760 250/1576 [===>..........................] - ETA: 0s - loss: 26.6763 450/1576 [=======>......................] - ETA: 0s - loss: 26.1133 650/1576 [===========>..................] - ETA: 0s - loss: 26.2577 850/1576 [===============>..............] - ETA: 0s - loss: 26.49291050/1576 [==================>...........] - ETA: 0s - loss: 26.31881250/1576 [======================>.......] - ETA: 0s - loss: 26.13471450/1576 [==========================>...] - ETA: 0s - loss: 26.1391
Epoch 00017: val_loss did not improve from 23.12194

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1576/1576 [==============================] - 0s 287us/sample - loss: 26.0684 - val_loss: 25.0659
Epoch 18/50
  50/1576 [..............................] - ETA: 0s - loss: 27.7774 250/1576 [===>..........................] - ETA: 0s - loss: 27.5000 450/1576 [=======>......................] - ETA: 0s - loss: 26.8598 650/1576 [===========>..................] - ETA: 0s - loss: 26.8166 850/1576 [===============>..............] - ETA: 0s - loss: 26.50021050/1576 [==================>...........] - ETA: 0s - loss: 26.26401250/1576 [======================>.......] - ETA: 0s - loss: 26.26251450/1576 [==========================>...] - ETA: 0s - loss: 26.0358
Epoch 00018: val_loss improved from 23.12194 to 22.95419, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 309us/sample - loss: 26.0115 - val_loss: 22.9542
Epoch 19/50
  50/1576 [..............................] - ETA: 0s - loss: 27.0517 250/1576 [===>..........................] - ETA: 0s - loss: 26.0796 450/1576 [=======>......................] - ETA: 0s - loss: 25.7427 650/1576 [===========>..................] - ETA: 0s - loss: 25.6121 850/1576 [===============>..............] - ETA: 0s - loss: 25.45761050/1576 [==================>...........] - ETA: 0s - loss: 25.49821250/1576 [======================>.......] - ETA: 0s - loss: 25.47771450/1576 [==========================>...] - ETA: 0s - loss: 25.4375
Epoch 00019: val_loss improved from 22.95419 to 22.90561, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 308us/sample - loss: 25.5747 - val_loss: 22.9056
Epoch 20/50
  50/1576 [..............................] - ETA: 0s - loss: 25.9434 250/1576 [===>..........................] - ETA: 0s - loss: 25.8731 450/1576 [=======>......................] - ETA: 0s - loss: 26.0813 650/1576 [===========>..................] - ETA: 0s - loss: 25.7327 850/1576 [===============>..............] - ETA: 0s - loss: 25.47321050/1576 [==================>...........] - ETA: 0s - loss: 25.56301250/1576 [======================>.......] - ETA: 0s - loss: 25.71391450/1576 [==========================>...] - ETA: 0s - loss: 25.6589
Epoch 00020: val_loss did not improve from 22.90561
1576/1576 [==============================] - 0s 291us/sample - loss: 25.7004 - val_loss: 24.9217
Epoch 21/50
  50/1576 [..............................] - ETA: 0s - loss: 24.8708 250/1576 [===>..........................] - ETA: 0s - loss: 26.2922 450/1576 [=======>......................] - ETA: 0s - loss: 26.3008 650/1576 [===========>..................] - ETA: 0s - loss: 26.1621 850/1576 [===============>..............] - ETA: 0s - loss: 26.15761050/1576 [==================>...........] - ETA: 0s - loss: 25.81741250/1576 [======================>.......] - ETA: 0s - loss: 25.96041450/1576 [==========================>...] - ETA: 0s - loss: 25.8981
Epoch 00021: val_loss did not improve from 22.90561

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1576/1576 [==============================] - 0s 288us/sample - loss: 25.8173 - val_loss: 22.9845
Epoch 22/50
  50/1576 [..............................] - ETA: 0s - loss: 25.0651 250/1576 [===>..........................] - ETA: 0s - loss: 26.1012 450/1576 [=======>......................] - ETA: 0s - loss: 25.2841 650/1576 [===========>..................] - ETA: 0s - loss: 25.5416 850/1576 [===============>..............] - ETA: 0s - loss: 25.62611050/1576 [==================>...........] - ETA: 0s - loss: 25.76491250/1576 [======================>.......] - ETA: 0s - loss: 25.78571450/1576 [==========================>...] - ETA: 0s - loss: 25.7028
Epoch 00022: val_loss did not improve from 22.90561
1576/1576 [==============================] - 0s 290us/sample - loss: 25.6039 - val_loss: 23.3948
Epoch 23/50
  50/1576 [..............................] - ETA: 0s - loss: 24.0194 250/1576 [===>..........................] - ETA: 0s - loss: 24.7662 450/1576 [=======>......................] - ETA: 0s - loss: 25.1234 650/1576 [===========>..................] - ETA: 0s - loss: 25.1622 850/1576 [===============>..............] - ETA: 0s - loss: 25.56141050/1576 [==================>...........] - ETA: 0s - loss: 25.53981250/1576 [======================>.......] - ETA: 0s - loss: 25.54651450/1576 [==========================>...] - ETA: 0s - loss: 25.5841
Epoch 00023: val_loss improved from 22.90561 to 22.86654, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 315us/sample - loss: 25.5929 - val_loss: 22.8665
Epoch 24/50
  50/1576 [..............................] - ETA: 0s - loss: 24.7119 250/1576 [===>..........................] - ETA: 0s - loss: 24.7050 450/1576 [=======>......................] - ETA: 0s - loss: 24.8162 650/1576 [===========>..................] - ETA: 0s - loss: 25.4639 850/1576 [===============>..............] - ETA: 0s - loss: 25.41701050/1576 [==================>...........] - ETA: 0s - loss: 25.39721250/1576 [======================>.......] - ETA: 0s - loss: 25.33941450/1576 [==========================>...] - ETA: 0s - loss: 25.3394
Epoch 00024: val_loss improved from 22.86654 to 22.79369, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 317us/sample - loss: 25.3660 - val_loss: 22.7937
Epoch 25/50
  50/1576 [..............................] - ETA: 0s - loss: 24.4251 250/1576 [===>..........................] - ETA: 0s - loss: 25.1824 450/1576 [=======>......................] - ETA: 0s - loss: 25.5607 650/1576 [===========>..................] - ETA: 0s - loss: 25.5105 850/1576 [===============>..............] - ETA: 0s - loss: 25.45801050/1576 [==================>...........] - ETA: 0s - loss: 25.42041250/1576 [======================>.......] - ETA: 0s - loss: 25.23311450/1576 [==========================>...] - ETA: 0s - loss: 25.2418
Epoch 00025: val_loss did not improve from 22.79369
1576/1576 [==============================] - 0s 293us/sample - loss: 25.3959 - val_loss: 22.9209
Epoch 26/50
  50/1576 [..............................] - ETA: 0s - loss: 28.6122 250/1576 [===>..........................] - ETA: 0s - loss: 25.5785 450/1576 [=======>......................] - ETA: 0s - loss: 25.6816 650/1576 [===========>..................] - ETA: 0s - loss: 25.9337 850/1576 [===============>..............] - ETA: 0s - loss: 25.87801050/1576 [==================>...........] - ETA: 0s - loss: 25.66751250/1576 [======================>.......] - ETA: 0s - loss: 25.67061450/1576 [==========================>...] - ETA: 0s - loss: 25.6584
Epoch 00026: val_loss improved from 22.79369 to 22.76902, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 312us/sample - loss: 25.5347 - val_loss: 22.7690
Epoch 27/50
  50/1576 [..............................] - ETA: 0s - loss: 23.8860 250/1576 [===>..........................] - ETA: 0s - loss: 25.7510 450/1576 [=======>......................] - ETA: 0s - loss: 25.9461 650/1576 [===========>..................] - ETA: 0s - loss: 26.0544 850/1576 [===============>..............] - ETA: 0s - loss: 25.81821050/1576 [==================>...........] - ETA: 0s - loss: 25.70551250/1576 [======================>.......] - ETA: 0s - loss: 25.56091450/1576 [==========================>...] - ETA: 0s - loss: 25.4678
Epoch 00027: val_loss improved from 22.76902 to 22.70594, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 1s 329us/sample - loss: 25.4722 - val_loss: 22.7059
Epoch 28/50
  50/1576 [..............................] - ETA: 0s - loss: 28.1410 250/1576 [===>..........................] - ETA: 0s - loss: 25.9876 450/1576 [=======>......................] - ETA: 0s - loss: 25.9222 650/1576 [===========>..................] - ETA: 0s - loss: 25.5082 850/1576 [===============>..............] - ETA: 0s - loss: 25.53401050/1576 [==================>...........] - ETA: 0s - loss: 25.40231250/1576 [======================>.......] - ETA: 0s - loss: 25.42681450/1576 [==========================>...] - ETA: 0s - loss: 25.3898
Epoch 00028: val_loss did not improve from 22.70594
1576/1576 [==============================] - 0s 284us/sample - loss: 25.3557 - val_loss: 22.7458
Epoch 29/50
  50/1576 [..............................] - ETA: 0s - loss: 24.3822 250/1576 [===>..........................] - ETA: 0s - loss: 25.5187 450/1576 [=======>......................] - ETA: 0s - loss: 25.5360 650/1576 [===========>..................] - ETA: 0s - loss: 25.3071 850/1576 [===============>..............] - ETA: 0s - loss: 25.48001050/1576 [==================>...........] - ETA: 0s - loss: 25.46981250/1576 [======================>.......] - ETA: 0s - loss: 25.37901450/1576 [==========================>...] - ETA: 0s - loss: 25.2372
Epoch 00029: val_loss did not improve from 22.70594

Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1576/1576 [==============================] - 0s 289us/sample - loss: 25.2444 - val_loss: 22.7541
Epoch 30/50
  50/1576 [..............................] - ETA: 0s - loss: 25.1298 250/1576 [===>..........................] - ETA: 0s - loss: 24.6662 450/1576 [=======>......................] - ETA: 0s - loss: 25.0376 650/1576 [===========>..................] - ETA: 0s - loss: 25.0057 850/1576 [===============>..............] - ETA: 0s - loss: 25.21351050/1576 [==================>...........] - ETA: 0s - loss: 25.10181250/1576 [======================>.......] - ETA: 0s - loss: 25.14751450/1576 [==========================>...] - ETA: 0s - loss: 25.2292
Epoch 00030: val_loss improved from 22.70594 to 22.69839, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 308us/sample - loss: 25.2951 - val_loss: 22.6984
Epoch 31/50
  50/1576 [..............................] - ETA: 0s - loss: 25.9701 250/1576 [===>..........................] - ETA: 0s - loss: 25.1116 450/1576 [=======>......................] - ETA: 0s - loss: 24.9912 650/1576 [===========>..................] - ETA: 0s - loss: 25.5123 850/1576 [===============>..............] - ETA: 0s - loss: 25.33931050/1576 [==================>...........] - ETA: 0s - loss: 25.34201250/1576 [======================>.......] - ETA: 0s - loss: 25.31201450/1576 [==========================>...] - ETA: 0s - loss: 25.2868
Epoch 00031: val_loss did not improve from 22.69839
1576/1576 [==============================] - 0s 288us/sample - loss: 25.2957 - val_loss: 22.7364
Epoch 32/50
  50/1576 [..............................] - ETA: 0s - loss: 24.5714 250/1576 [===>..........................] - ETA: 0s - loss: 25.8132 450/1576 [=======>......................] - ETA: 0s - loss: 25.5596 650/1576 [===========>..................] - ETA: 0s - loss: 25.6060 850/1576 [===============>..............] - ETA: 0s - loss: 25.44861050/1576 [==================>...........] - ETA: 0s - loss: 25.58171250/1576 [======================>.......] - ETA: 0s - loss: 25.47501450/1576 [==========================>...] - ETA: 0s - loss: 25.4221
Epoch 00032: val_loss did not improve from 22.69839

Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1576/1576 [==============================] - 0s 287us/sample - loss: 25.3086 - val_loss: 22.9283
Epoch 33/50
  50/1576 [..............................] - ETA: 0s - loss: 27.4835 250/1576 [===>..........................] - ETA: 0s - loss: 25.6519 450/1576 [=======>......................] - ETA: 0s - loss: 25.3167 650/1576 [===========>..................] - ETA: 0s - loss: 25.1797 850/1576 [===============>..............] - ETA: 0s - loss: 25.12781050/1576 [==================>...........] - ETA: 0s - loss: 25.12901250/1576 [======================>.......] - ETA: 0s - loss: 25.16161450/1576 [==========================>...] - ETA: 0s - loss: 25.2796
Epoch 00033: val_loss improved from 22.69839 to 22.69301, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 310us/sample - loss: 25.3271 - val_loss: 22.6930
Epoch 34/50
  50/1576 [..............................] - ETA: 0s - loss: 23.8208 250/1576 [===>..........................] - ETA: 0s - loss: 25.1813 450/1576 [=======>......................] - ETA: 0s - loss: 24.6744 650/1576 [===========>..................] - ETA: 0s - loss: 24.9679 850/1576 [===============>..............] - ETA: 0s - loss: 25.50961050/1576 [==================>...........] - ETA: 0s - loss: 25.53051250/1576 [======================>.......] - ETA: 0s - loss: 25.48771450/1576 [==========================>...] - ETA: 0s - loss: 25.3203
Epoch 00034: val_loss improved from 22.69301 to 22.66607, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 307us/sample - loss: 25.3592 - val_loss: 22.6661
Epoch 35/50
  50/1576 [..............................] - ETA: 0s - loss: 25.9219 250/1576 [===>..........................] - ETA: 0s - loss: 24.7237 450/1576 [=======>......................] - ETA: 0s - loss: 25.4408 650/1576 [===========>..................] - ETA: 0s - loss: 25.1148 850/1576 [===============>..............] - ETA: 0s - loss: 25.40711050/1576 [==================>...........] - ETA: 0s - loss: 25.42081250/1576 [======================>.......] - ETA: 0s - loss: 25.31301450/1576 [==========================>...] - ETA: 0s - loss: 25.2739
Epoch 00035: val_loss did not improve from 22.66607
1576/1576 [==============================] - 0s 288us/sample - loss: 25.2477 - val_loss: 22.6908
Epoch 36/50
  50/1576 [..............................] - ETA: 0s - loss: 28.8418 250/1576 [===>..........................] - ETA: 0s - loss: 25.9195 450/1576 [=======>......................] - ETA: 0s - loss: 25.6571 650/1576 [===========>..................] - ETA: 0s - loss: 25.6948 850/1576 [===============>..............] - ETA: 0s - loss: 25.62721050/1576 [==================>...........] - ETA: 0s - loss: 25.38451250/1576 [======================>.......] - ETA: 0s - loss: 25.15881450/1576 [==========================>...] - ETA: 0s - loss: 25.2100
Epoch 00036: val_loss did not improve from 22.66607

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1576/1576 [==============================] - 0s 289us/sample - loss: 25.2428 - val_loss: 22.7081
Epoch 37/50
  50/1576 [..............................] - ETA: 0s - loss: 27.4576 250/1576 [===>..........................] - ETA: 0s - loss: 25.5415 450/1576 [=======>......................] - ETA: 0s - loss: 25.4583 650/1576 [===========>..................] - ETA: 0s - loss: 25.2783 850/1576 [===============>..............] - ETA: 0s - loss: 25.24441050/1576 [==================>...........] - ETA: 0s - loss: 25.23191250/1576 [======================>.......] - ETA: 0s - loss: 25.21941450/1576 [==========================>...] - ETA: 0s - loss: 25.1972
Epoch 00037: val_loss did not improve from 22.66607
1576/1576 [==============================] - 0s 285us/sample - loss: 25.2465 - val_loss: 22.6893
Epoch 38/50
  50/1576 [..............................] - ETA: 0s - loss: 28.5192 250/1576 [===>..........................] - ETA: 0s - loss: 26.0336 450/1576 [=======>......................] - ETA: 0s - loss: 25.4391 650/1576 [===========>..................] - ETA: 0s - loss: 25.6169 850/1576 [===============>..............] - ETA: 0s - loss: 25.64661050/1576 [==================>...........] - ETA: 0s - loss: 25.51681250/1576 [======================>.......] - ETA: 0s - loss: 25.38031450/1576 [==========================>...] - ETA: 0s - loss: 25.3992
Epoch 00038: val_loss improved from 22.66607 to 22.66604, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt

Epoch 00038: ReduceLROnPlateau reducing learning rate to 1e-05.
1576/1576 [==============================] - 0s 301us/sample - loss: 25.3945 - val_loss: 22.6660
Epoch 39/50
  50/1576 [..............................] - ETA: 0s - loss: 24.0850 250/1576 [===>..........................] - ETA: 0s - loss: 25.7144 450/1576 [=======>......................] - ETA: 0s - loss: 25.1159 650/1576 [===========>..................] - ETA: 0s - loss: 25.4043 850/1576 [===============>..............] - ETA: 0s - loss: 25.73361050/1576 [==================>...........] - ETA: 0s - loss: 25.66961250/1576 [======================>.......] - ETA: 0s - loss: 25.65301450/1576 [==========================>...] - ETA: 0s - loss: 25.4575
Epoch 00039: val_loss improved from 22.66604 to 22.66324, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1576/1576 [==============================] - 0s 306us/sample - loss: 25.4117 - val_loss: 22.6632
Epoch 40/50
  50/1576 [..............................] - ETA: 0s - loss: 25.2583 250/1576 [===>..........................] - ETA: 0s - loss: 25.0029 450/1576 [=======>......................] - ETA: 0s - loss: 24.8534 650/1576 [===========>..................] - ETA: 0s - loss: 25.0780 850/1576 [===============>..............] - ETA: 0s - loss: 25.32881050/1576 [==================>...........] - ETA: 0s - loss: 25.21451250/1576 [======================>.......] - ETA: 0s - loss: 25.35591450/1576 [==========================>...] - ETA: 0s - loss: 25.3500
Epoch 00040: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 287us/sample - loss: 25.3044 - val_loss: 22.6745
Epoch 41/50
  50/1576 [..............................] - ETA: 0s - loss: 24.1377 250/1576 [===>..........................] - ETA: 0s - loss: 24.7753 450/1576 [=======>......................] - ETA: 0s - loss: 24.9281 650/1576 [===========>..................] - ETA: 0s - loss: 25.1648 850/1576 [===============>..............] - ETA: 0s - loss: 25.13021050/1576 [==================>...........] - ETA: 0s - loss: 25.04751250/1576 [======================>.......] - ETA: 0s - loss: 25.07721450/1576 [==========================>...] - ETA: 0s - loss: 25.2325
Epoch 00041: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 289us/sample - loss: 25.3651 - val_loss: 22.6892
Epoch 42/50
  50/1576 [..............................] - ETA: 0s - loss: 28.3354 250/1576 [===>..........................] - ETA: 0s - loss: 25.9527 450/1576 [=======>......................] - ETA: 0s - loss: 25.4170 650/1576 [===========>..................] - ETA: 0s - loss: 25.3463 850/1576 [===============>..............] - ETA: 0s - loss: 25.60471050/1576 [==================>...........] - ETA: 0s - loss: 25.69441250/1576 [======================>.......] - ETA: 0s - loss: 25.62341450/1576 [==========================>...] - ETA: 0s - loss: 25.3882
Epoch 00042: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 296us/sample - loss: 25.4673 - val_loss: 22.6778
Epoch 43/50
  50/1576 [..............................] - ETA: 0s - loss: 24.2622 250/1576 [===>..........................] - ETA: 0s - loss: 24.7474 450/1576 [=======>......................] - ETA: 0s - loss: 25.5284 650/1576 [===========>..................] - ETA: 0s - loss: 25.7661 850/1576 [===============>..............] - ETA: 0s - loss: 25.54591050/1576 [==================>...........] - ETA: 0s - loss: 25.54301250/1576 [======================>.......] - ETA: 0s - loss: 25.47231450/1576 [==========================>...] - ETA: 0s - loss: 25.3459
Epoch 00043: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 289us/sample - loss: 25.3337 - val_loss: 22.6647
Epoch 44/50
  50/1576 [..............................] - ETA: 0s - loss: 26.3721 250/1576 [===>..........................] - ETA: 0s - loss: 25.4642 450/1576 [=======>......................] - ETA: 0s - loss: 25.4772 650/1576 [===========>..................] - ETA: 0s - loss: 25.5640 850/1576 [===============>..............] - ETA: 0s - loss: 25.51211050/1576 [==================>...........] - ETA: 0s - loss: 25.50511250/1576 [======================>.......] - ETA: 0s - loss: 25.43181450/1576 [==========================>...] - ETA: 0s - loss: 25.3415
Epoch 00044: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 291us/sample - loss: 25.3062 - val_loss: 22.6959
Epoch 45/50
  50/1576 [..............................] - ETA: 0s - loss: 25.3591 250/1576 [===>..........................] - ETA: 0s - loss: 24.6447 450/1576 [=======>......................] - ETA: 0s - loss: 25.1344 650/1576 [===========>..................] - ETA: 0s - loss: 24.9627 850/1576 [===============>..............] - ETA: 0s - loss: 25.08691050/1576 [==================>...........] - ETA: 0s - loss: 25.23171250/1576 [======================>.......] - ETA: 0s - loss: 25.18131450/1576 [==========================>...] - ETA: 0s - loss: 25.1917
Epoch 00045: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 292us/sample - loss: 25.1844 - val_loss: 22.6675
Epoch 46/50
  50/1576 [..............................] - ETA: 0s - loss: 24.0760 250/1576 [===>..........................] - ETA: 0s - loss: 24.8927 450/1576 [=======>......................] - ETA: 0s - loss: 25.3601 650/1576 [===========>..................] - ETA: 0s - loss: 25.3752 850/1576 [===============>..............] - ETA: 0s - loss: 25.09021050/1576 [==================>...........] - ETA: 0s - loss: 25.14171250/1576 [======================>.......] - ETA: 0s - loss: 25.06071450/1576 [==========================>...] - ETA: 0s - loss: 25.2175
Epoch 00046: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 296us/sample - loss: 25.3350 - val_loss: 22.6737
Epoch 47/50
  50/1576 [..............................] - ETA: 0s - loss: 25.5335 250/1576 [===>..........................] - ETA: 0s - loss: 24.9388 450/1576 [=======>......................] - ETA: 0s - loss: 24.6946 650/1576 [===========>..................] - ETA: 0s - loss: 24.8412 850/1576 [===============>..............] - ETA: 0s - loss: 24.94501050/1576 [==================>...........] - ETA: 0s - loss: 25.01031250/1576 [======================>.......] - ETA: 0s - loss: 25.09301450/1576 [==========================>...] - ETA: 0s - loss: 25.2218
Epoch 00047: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 292us/sample - loss: 25.2898 - val_loss: 22.6809
Epoch 48/50
  50/1576 [..............................] - ETA: 0s - loss: 25.8900 250/1576 [===>..........................] - ETA: 0s - loss: 25.5282 450/1576 [=======>......................] - ETA: 0s - loss: 25.7680 650/1576 [===========>..................] - ETA: 0s - loss: 25.4077 850/1576 [===============>..............] - ETA: 0s - loss: 25.20951050/1576 [==================>...........] - ETA: 0s - loss: 25.25121250/1576 [======================>.......] - ETA: 0s - loss: 25.34931450/1576 [==========================>...] - ETA: 0s - loss: 25.2275
Epoch 00048: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 286us/sample - loss: 25.3110 - val_loss: 22.6779
Epoch 49/50
  50/1576 [..............................] - ETA: 0s - loss: 23.4670 250/1576 [===>..........................] - ETA: 0s - loss: 24.3539 450/1576 [=======>......................] - ETA: 0s - loss: 24.7411 650/1576 [===========>..................] - ETA: 0s - loss: 24.7459 800/1576 [==============>...............] - ETA: 0s - loss: 24.8925 950/1576 [=================>............] - ETA: 0s - loss: 25.01541150/1576 [====================>.........] - ETA: 0s - loss: 25.21071350/1576 [========================>.....] - ETA: 0s - loss: 25.28941550/1576 [============================>.] - ETA: 0s - loss: 25.2601
Epoch 00049: val_loss did not improve from 22.66324
1576/1576 [==============================] - 0s 311us/sample - loss: 25.2415 - val_loss: 22.6679
Epoch 00049: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 1697 samples, validate on 121 samples
Epoch 1/50
  50/1697 [..............................] - ETA: 37s - loss: 157.8946 200/1697 [==>...........................] - ETA: 9s - loss: 151.9801  400/1697 [======>.......................] - ETA: 4s - loss: 153.6897 600/1697 [=========>....................] - ETA: 2s - loss: 147.8888 800/1697 [=============>................] - ETA: 1s - loss: 135.45171000/1697 [================>.............] - ETA: 1s - loss: 119.09801200/1697 [====================>.........] - ETA: 0s - loss: 108.54381400/1697 [=======================>......] - ETA: 0s - loss: 99.5333 1600/1697 [===========================>..] - ETA: 0s - loss: 92.9288
Epoch 00001: val_loss improved from inf to 70.39981, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 2s 1ms/sample - loss: 89.9574 - val_loss: 70.3998
Epoch 2/50
  50/1697 [..............................] - ETA: 0s - loss: 39.0774 250/1697 [===>..........................] - ETA: 0s - loss: 40.9286 450/1697 [======>.......................] - ETA: 0s - loss: 38.4184 650/1697 [==========>...................] - ETA: 0s - loss: 38.4442 850/1697 [==============>...............] - ETA: 0s - loss: 37.45131050/1697 [=================>............] - ETA: 0s - loss: 36.88751250/1697 [=====================>........] - ETA: 0s - loss: 36.39011450/1697 [========================>.....] - ETA: 0s - loss: 35.77481650/1697 [============================>.] - ETA: 0s - loss: 35.1766
Epoch 00002: val_loss did not improve from 70.39981
1697/1697 [==============================] - 0s 286us/sample - loss: 35.0659 - val_loss: 71.7330
Epoch 3/50
  50/1697 [..............................] - ETA: 0s - loss: 31.8502 250/1697 [===>..........................] - ETA: 0s - loss: 31.5363 450/1697 [======>.......................] - ETA: 0s - loss: 31.1264 650/1697 [==========>...................] - ETA: 0s - loss: 30.6580 850/1697 [==============>...............] - ETA: 0s - loss: 30.55821050/1697 [=================>............] - ETA: 0s - loss: 30.24121250/1697 [=====================>........] - ETA: 0s - loss: 30.22061450/1697 [========================>.....] - ETA: 0s - loss: 30.12651650/1697 [============================>.] - ETA: 0s - loss: 30.0123
Epoch 00003: val_loss improved from 70.39981 to 58.80518, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 295us/sample - loss: 29.8909 - val_loss: 58.8052
Epoch 4/50
  50/1697 [..............................] - ETA: 0s - loss: 29.9767 250/1697 [===>..........................] - ETA: 0s - loss: 28.3022 450/1697 [======>.......................] - ETA: 0s - loss: 28.4986 650/1697 [==========>...................] - ETA: 0s - loss: 28.8410 850/1697 [==============>...............] - ETA: 0s - loss: 28.81491050/1697 [=================>............] - ETA: 0s - loss: 28.62831250/1697 [=====================>........] - ETA: 0s - loss: 28.46261450/1697 [========================>.....] - ETA: 0s - loss: 28.28781650/1697 [============================>.] - ETA: 0s - loss: 28.1252
Epoch 00004: val_loss improved from 58.80518 to 42.72927, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 359us/sample - loss: 28.2228 - val_loss: 42.7293
Epoch 5/50
  50/1697 [..............................] - ETA: 0s - loss: 24.5612 250/1697 [===>..........................] - ETA: 0s - loss: 26.1633 450/1697 [======>.......................] - ETA: 0s - loss: 27.3367 650/1697 [==========>...................] - ETA: 0s - loss: 27.2591 850/1697 [==============>...............] - ETA: 0s - loss: 27.37421050/1697 [=================>............] - ETA: 0s - loss: 27.22911250/1697 [=====================>........] - ETA: 0s - loss: 27.17881450/1697 [========================>.....] - ETA: 0s - loss: 27.16161650/1697 [============================>.] - ETA: 0s - loss: 27.1616
Epoch 00005: val_loss improved from 42.72927 to 33.98775, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 301us/sample - loss: 27.1734 - val_loss: 33.9877
Epoch 6/50
  50/1697 [..............................] - ETA: 0s - loss: 26.3813 250/1697 [===>..........................] - ETA: 0s - loss: 27.0231 450/1697 [======>.......................] - ETA: 0s - loss: 26.6202 650/1697 [==========>...................] - ETA: 0s - loss: 26.4340 850/1697 [==============>...............] - ETA: 0s - loss: 26.31471050/1697 [=================>............] - ETA: 0s - loss: 26.42881250/1697 [=====================>........] - ETA: 0s - loss: 26.57881450/1697 [========================>.....] - ETA: 0s - loss: 26.51461650/1697 [============================>.] - ETA: 0s - loss: 26.6472
Epoch 00006: val_loss improved from 33.98775 to 28.93509, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 305us/sample - loss: 26.6257 - val_loss: 28.9351
Epoch 7/50
  50/1697 [..............................] - ETA: 0s - loss: 26.2372 250/1697 [===>..........................] - ETA: 0s - loss: 26.3576 450/1697 [======>.......................] - ETA: 0s - loss: 26.1221 650/1697 [==========>...................] - ETA: 0s - loss: 26.2607 850/1697 [==============>...............] - ETA: 0s - loss: 26.36341050/1697 [=================>............] - ETA: 0s - loss: 26.21811250/1697 [=====================>........] - ETA: 0s - loss: 26.27551450/1697 [========================>.....] - ETA: 0s - loss: 26.26811650/1697 [============================>.] - ETA: 0s - loss: 26.3530
Epoch 00007: val_loss improved from 28.93509 to 27.84381, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 309us/sample - loss: 26.3262 - val_loss: 27.8438
Epoch 8/50
  50/1697 [..............................] - ETA: 0s - loss: 25.8696 250/1697 [===>..........................] - ETA: 0s - loss: 27.0192 450/1697 [======>.......................] - ETA: 0s - loss: 26.8380 650/1697 [==========>...................] - ETA: 0s - loss: 27.0868 850/1697 [==============>...............] - ETA: 0s - loss: 27.00451050/1697 [=================>............] - ETA: 0s - loss: 26.91891250/1697 [=====================>........] - ETA: 0s - loss: 26.77441450/1697 [========================>.....] - ETA: 0s - loss: 26.98441650/1697 [============================>.] - ETA: 0s - loss: 26.9100
Epoch 00008: val_loss did not improve from 27.84381
1697/1697 [==============================] - 0s 289us/sample - loss: 26.8342 - val_loss: 31.5160
Epoch 9/50
  50/1697 [..............................] - ETA: 0s - loss: 27.5970 250/1697 [===>..........................] - ETA: 0s - loss: 27.3298 450/1697 [======>.......................] - ETA: 0s - loss: 26.8813 650/1697 [==========>...................] - ETA: 0s - loss: 26.8946 850/1697 [==============>...............] - ETA: 0s - loss: 26.57851050/1697 [=================>............] - ETA: 0s - loss: 26.39721250/1697 [=====================>........] - ETA: 0s - loss: 26.37391450/1697 [========================>.....] - ETA: 0s - loss: 26.29291650/1697 [============================>.] - ETA: 0s - loss: 26.3400
Epoch 00009: val_loss improved from 27.84381 to 25.61935, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 307us/sample - loss: 26.3487 - val_loss: 25.6194
Epoch 10/50
  50/1697 [..............................] - ETA: 0s - loss: 25.4926 250/1697 [===>..........................] - ETA: 0s - loss: 25.8138 450/1697 [======>.......................] - ETA: 0s - loss: 25.8120 650/1697 [==========>...................] - ETA: 0s - loss: 26.0680 850/1697 [==============>...............] - ETA: 0s - loss: 26.16891050/1697 [=================>............] - ETA: 0s - loss: 26.04161250/1697 [=====================>........] - ETA: 0s - loss: 25.93921450/1697 [========================>.....] - ETA: 0s - loss: 25.95261650/1697 [============================>.] - ETA: 0s - loss: 25.9750
Epoch 00010: val_loss improved from 25.61935 to 24.48574, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 307us/sample - loss: 25.9748 - val_loss: 24.4857
Epoch 11/50
  50/1697 [..............................] - ETA: 0s - loss: 26.8387 250/1697 [===>..........................] - ETA: 0s - loss: 27.3752 450/1697 [======>.......................] - ETA: 0s - loss: 26.6853 650/1697 [==========>...................] - ETA: 0s - loss: 26.3240 850/1697 [==============>...............] - ETA: 0s - loss: 26.10551050/1697 [=================>............] - ETA: 0s - loss: 26.11161250/1697 [=====================>........] - ETA: 0s - loss: 26.07641450/1697 [========================>.....] - ETA: 0s - loss: 26.11541650/1697 [============================>.] - ETA: 0s - loss: 26.2924
Epoch 00011: val_loss did not improve from 24.48574
1697/1697 [==============================] - 0s 282us/sample - loss: 26.3600 - val_loss: 24.7992
Epoch 12/50
  50/1697 [..............................] - ETA: 0s - loss: 26.4262 250/1697 [===>..........................] - ETA: 0s - loss: 25.9590 450/1697 [======>.......................] - ETA: 0s - loss: 26.0059 650/1697 [==========>...................] - ETA: 0s - loss: 25.8551 850/1697 [==============>...............] - ETA: 0s - loss: 26.09701050/1697 [=================>............] - ETA: 0s - loss: 26.11791250/1697 [=====================>........] - ETA: 0s - loss: 26.07771450/1697 [========================>.....] - ETA: 0s - loss: 26.03871650/1697 [============================>.] - ETA: 0s - loss: 25.9906
Epoch 00012: val_loss improved from 24.48574 to 24.05845, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 299us/sample - loss: 26.0074 - val_loss: 24.0584
Epoch 13/50
  50/1697 [..............................] - ETA: 0s - loss: 25.0721 250/1697 [===>..........................] - ETA: 0s - loss: 25.8679 450/1697 [======>.......................] - ETA: 0s - loss: 26.0653 650/1697 [==========>...................] - ETA: 0s - loss: 26.0600 850/1697 [==============>...............] - ETA: 0s - loss: 26.21611050/1697 [=================>............] - ETA: 0s - loss: 26.02691250/1697 [=====================>........] - ETA: 0s - loss: 26.16451450/1697 [========================>.....] - ETA: 0s - loss: 26.19811650/1697 [============================>.] - ETA: 0s - loss: 26.2334
Epoch 00013: val_loss improved from 24.05845 to 23.46130, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 301us/sample - loss: 26.2034 - val_loss: 23.4613
Epoch 14/50
  50/1697 [..............................] - ETA: 0s - loss: 25.0143 250/1697 [===>..........................] - ETA: 0s - loss: 24.3655 450/1697 [======>.......................] - ETA: 0s - loss: 24.6905 650/1697 [==========>...................] - ETA: 0s - loss: 25.4063 850/1697 [==============>...............] - ETA: 0s - loss: 25.59311050/1697 [=================>............] - ETA: 0s - loss: 25.74601250/1697 [=====================>........] - ETA: 0s - loss: 25.66551450/1697 [========================>.....] - ETA: 0s - loss: 25.81491650/1697 [============================>.] - ETA: 0s - loss: 25.6801
Epoch 00014: val_loss improved from 23.46130 to 23.45536, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 320us/sample - loss: 25.6537 - val_loss: 23.4554
Epoch 15/50
  50/1697 [..............................] - ETA: 0s - loss: 25.2513 250/1697 [===>..........................] - ETA: 0s - loss: 25.4209 450/1697 [======>.......................] - ETA: 0s - loss: 25.3136 650/1697 [==========>...................] - ETA: 0s - loss: 25.4016 850/1697 [==============>...............] - ETA: 0s - loss: 25.42401050/1697 [=================>............] - ETA: 0s - loss: 25.38881250/1697 [=====================>........] - ETA: 0s - loss: 25.46001450/1697 [========================>.....] - ETA: 0s - loss: 25.44151650/1697 [============================>.] - ETA: 0s - loss: 25.5005
Epoch 00015: val_loss did not improve from 23.45536
1697/1697 [==============================] - 0s 290us/sample - loss: 25.5549 - val_loss: 25.1560
Epoch 16/50
  50/1697 [..............................] - ETA: 0s - loss: 25.3097 250/1697 [===>..........................] - ETA: 0s - loss: 25.6568 450/1697 [======>.......................] - ETA: 0s - loss: 25.5550 650/1697 [==========>...................] - ETA: 0s - loss: 25.9122 850/1697 [==============>...............] - ETA: 0s - loss: 26.06491050/1697 [=================>............] - ETA: 0s - loss: 26.21151250/1697 [=====================>........] - ETA: 0s - loss: 26.23681450/1697 [========================>.....] - ETA: 0s - loss: 26.00501650/1697 [============================>.] - ETA: 0s - loss: 25.9364
Epoch 00016: val_loss improved from 23.45536 to 23.11835, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 305us/sample - loss: 25.9521 - val_loss: 23.1183
Epoch 17/50
  50/1697 [..............................] - ETA: 0s - loss: 28.4396 250/1697 [===>..........................] - ETA: 0s - loss: 25.2976 450/1697 [======>.......................] - ETA: 0s - loss: 25.4934 650/1697 [==========>...................] - ETA: 0s - loss: 25.4477 850/1697 [==============>...............] - ETA: 0s - loss: 25.75731050/1697 [=================>............] - ETA: 0s - loss: 25.81131250/1697 [=====================>........] - ETA: 0s - loss: 25.71331450/1697 [========================>.....] - ETA: 0s - loss: 25.75941650/1697 [============================>.] - ETA: 0s - loss: 25.8099
Epoch 00017: val_loss improved from 23.11835 to 23.05691, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 339us/sample - loss: 25.8432 - val_loss: 23.0569
Epoch 18/50
  50/1697 [..............................] - ETA: 0s - loss: 25.1922 250/1697 [===>..........................] - ETA: 0s - loss: 25.4481 450/1697 [======>.......................] - ETA: 0s - loss: 25.1931 650/1697 [==========>...................] - ETA: 0s - loss: 25.4112 850/1697 [==============>...............] - ETA: 0s - loss: 25.25371050/1697 [=================>............] - ETA: 0s - loss: 25.29651250/1697 [=====================>........] - ETA: 0s - loss: 25.30041450/1697 [========================>.....] - ETA: 0s - loss: 25.37231650/1697 [============================>.] - ETA: 0s - loss: 25.3655
Epoch 00018: val_loss did not improve from 23.05691
1697/1697 [==============================] - 0s 281us/sample - loss: 25.3729 - val_loss: 23.4575
Epoch 19/50
  50/1697 [..............................] - ETA: 0s - loss: 25.5728 250/1697 [===>..........................] - ETA: 0s - loss: 25.6579 450/1697 [======>.......................] - ETA: 0s - loss: 25.4609 650/1697 [==========>...................] - ETA: 0s - loss: 25.3760 850/1697 [==============>...............] - ETA: 0s - loss: 25.22361050/1697 [=================>............] - ETA: 0s - loss: 25.21421250/1697 [=====================>........] - ETA: 0s - loss: 25.46101450/1697 [========================>.....] - ETA: 0s - loss: 25.69381650/1697 [============================>.] - ETA: 0s - loss: 25.6298
Epoch 00019: val_loss improved from 23.05691 to 22.67049, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 301us/sample - loss: 25.6808 - val_loss: 22.6705
Epoch 20/50
  50/1697 [..............................] - ETA: 0s - loss: 24.7699 250/1697 [===>..........................] - ETA: 0s - loss: 24.6591 450/1697 [======>.......................] - ETA: 0s - loss: 24.6056 650/1697 [==========>...................] - ETA: 0s - loss: 24.7201 850/1697 [==============>...............] - ETA: 0s - loss: 24.67001050/1697 [=================>............] - ETA: 0s - loss: 24.91761250/1697 [=====================>........] - ETA: 0s - loss: 25.17421450/1697 [========================>.....] - ETA: 0s - loss: 25.46021650/1697 [============================>.] - ETA: 0s - loss: 25.6291
Epoch 00020: val_loss did not improve from 22.67049
1697/1697 [==============================] - 0s 287us/sample - loss: 25.6059 - val_loss: 22.9615
Epoch 21/50
  50/1697 [..............................] - ETA: 0s - loss: 23.9157 250/1697 [===>..........................] - ETA: 0s - loss: 25.0935 450/1697 [======>.......................] - ETA: 0s - loss: 25.3368 650/1697 [==========>...................] - ETA: 0s - loss: 25.1424 850/1697 [==============>...............] - ETA: 0s - loss: 25.13451050/1697 [=================>............] - ETA: 0s - loss: 25.11781250/1697 [=====================>........] - ETA: 0s - loss: 25.23641450/1697 [========================>.....] - ETA: 0s - loss: 25.24531650/1697 [============================>.] - ETA: 0s - loss: 25.3326
Epoch 00021: val_loss did not improve from 22.67049

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1697/1697 [==============================] - 0s 281us/sample - loss: 25.3008 - val_loss: 22.7511
Epoch 22/50
  50/1697 [..............................] - ETA: 0s - loss: 25.4014 250/1697 [===>..........................] - ETA: 0s - loss: 24.9883 450/1697 [======>.......................] - ETA: 0s - loss: 25.3258 650/1697 [==========>...................] - ETA: 0s - loss: 25.6609 850/1697 [==============>...............] - ETA: 0s - loss: 25.62851050/1697 [=================>............] - ETA: 0s - loss: 25.59121250/1697 [=====================>........] - ETA: 0s - loss: 25.68131450/1697 [========================>.....] - ETA: 0s - loss: 25.51951650/1697 [============================>.] - ETA: 0s - loss: 25.3633
Epoch 00022: val_loss did not improve from 22.67049
1697/1697 [==============================] - 0s 282us/sample - loss: 25.3899 - val_loss: 22.7206
Epoch 23/50
  50/1697 [..............................] - ETA: 0s - loss: 26.4207 250/1697 [===>..........................] - ETA: 0s - loss: 24.5691 450/1697 [======>.......................] - ETA: 0s - loss: 25.0200 650/1697 [==========>...................] - ETA: 0s - loss: 24.9193 850/1697 [==============>...............] - ETA: 0s - loss: 25.28461050/1697 [=================>............] - ETA: 0s - loss: 25.22001250/1697 [=====================>........] - ETA: 0s - loss: 25.03751450/1697 [========================>.....] - ETA: 0s - loss: 25.03551650/1697 [============================>.] - ETA: 0s - loss: 25.0014
Epoch 00023: val_loss did not improve from 22.67049

Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1697/1697 [==============================] - 0s 288us/sample - loss: 25.0133 - val_loss: 22.6882
Epoch 24/50
  50/1697 [..............................] - ETA: 0s - loss: 25.6338 250/1697 [===>..........................] - ETA: 0s - loss: 25.1153 450/1697 [======>.......................] - ETA: 0s - loss: 24.9161 650/1697 [==========>...................] - ETA: 0s - loss: 25.0960 850/1697 [==============>...............] - ETA: 0s - loss: 25.19461050/1697 [=================>............] - ETA: 0s - loss: 25.03521250/1697 [=====================>........] - ETA: 0s - loss: 25.12781450/1697 [========================>.....] - ETA: 0s - loss: 25.04971650/1697 [============================>.] - ETA: 0s - loss: 24.9371
Epoch 00024: val_loss did not improve from 22.67049
1697/1697 [==============================] - 0s 288us/sample - loss: 25.0156 - val_loss: 22.7345
Epoch 25/50
  50/1697 [..............................] - ETA: 0s - loss: 25.3835 250/1697 [===>..........................] - ETA: 0s - loss: 24.8779 450/1697 [======>.......................] - ETA: 0s - loss: 24.7484 650/1697 [==========>...................] - ETA: 0s - loss: 24.6710 850/1697 [==============>...............] - ETA: 0s - loss: 24.56711050/1697 [=================>............] - ETA: 0s - loss: 24.60581250/1697 [=====================>........] - ETA: 0s - loss: 24.77891450/1697 [========================>.....] - ETA: 0s - loss: 24.96521650/1697 [============================>.] - ETA: 0s - loss: 25.0160
Epoch 00025: val_loss did not improve from 22.67049

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1697/1697 [==============================] - 0s 285us/sample - loss: 25.0236 - val_loss: 22.7657
Epoch 26/50
  50/1697 [..............................] - ETA: 0s - loss: 25.9272 250/1697 [===>..........................] - ETA: 0s - loss: 25.1536 450/1697 [======>.......................] - ETA: 0s - loss: 25.4514 650/1697 [==========>...................] - ETA: 0s - loss: 25.4018 850/1697 [==============>...............] - ETA: 0s - loss: 25.23591050/1697 [=================>............] - ETA: 0s - loss: 25.20791250/1697 [=====================>........] - ETA: 0s - loss: 25.04381450/1697 [========================>.....] - ETA: 0s - loss: 24.99231650/1697 [============================>.] - ETA: 0s - loss: 25.1818
Epoch 00026: val_loss did not improve from 22.67049
1697/1697 [==============================] - 0s 287us/sample - loss: 25.2380 - val_loss: 22.7108
Epoch 27/50
  50/1697 [..............................] - ETA: 0s - loss: 23.7497 250/1697 [===>..........................] - ETA: 0s - loss: 24.3806 450/1697 [======>.......................] - ETA: 0s - loss: 24.6411 650/1697 [==========>...................] - ETA: 0s - loss: 24.8828 850/1697 [==============>...............] - ETA: 0s - loss: 24.91841050/1697 [=================>............] - ETA: 0s - loss: 24.99371250/1697 [=====================>........] - ETA: 0s - loss: 25.11631450/1697 [========================>.....] - ETA: 0s - loss: 25.03191650/1697 [============================>.] - ETA: 0s - loss: 25.0183
Epoch 00027: val_loss did not improve from 22.67049

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1697/1697 [==============================] - 0s 284us/sample - loss: 25.1222 - val_loss: 22.7158
Epoch 28/50
  50/1697 [..............................] - ETA: 0s - loss: 24.1380 250/1697 [===>..........................] - ETA: 0s - loss: 24.0828 450/1697 [======>.......................] - ETA: 0s - loss: 24.9429 650/1697 [==========>...................] - ETA: 0s - loss: 25.0808 850/1697 [==============>...............] - ETA: 0s - loss: 25.34991050/1697 [=================>............] - ETA: 0s - loss: 25.30781250/1697 [=====================>........] - ETA: 0s - loss: 25.20491450/1697 [========================>.....] - ETA: 0s - loss: 25.06751650/1697 [============================>.] - ETA: 0s - loss: 25.0786
Epoch 00028: val_loss improved from 22.67049 to 22.56541, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 304us/sample - loss: 25.0341 - val_loss: 22.5654
Epoch 29/50
  50/1697 [..............................] - ETA: 0s - loss: 23.6180 250/1697 [===>..........................] - ETA: 0s - loss: 24.8974 450/1697 [======>.......................] - ETA: 0s - loss: 24.7155 650/1697 [==========>...................] - ETA: 0s - loss: 25.0759 850/1697 [==============>...............] - ETA: 0s - loss: 25.09721050/1697 [=================>............] - ETA: 0s - loss: 24.99811250/1697 [=====================>........] - ETA: 0s - loss: 25.00801450/1697 [========================>.....] - ETA: 0s - loss: 24.92841650/1697 [============================>.] - ETA: 0s - loss: 24.9207
Epoch 00029: val_loss improved from 22.56541 to 22.54079, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 306us/sample - loss: 24.9140 - val_loss: 22.5408
Epoch 30/50
  50/1697 [..............................] - ETA: 0s - loss: 24.8407 250/1697 [===>..........................] - ETA: 0s - loss: 24.3990 450/1697 [======>.......................] - ETA: 0s - loss: 24.4771 650/1697 [==========>...................] - ETA: 0s - loss: 24.5726 850/1697 [==============>...............] - ETA: 0s - loss: 24.98621050/1697 [=================>............] - ETA: 0s - loss: 25.09841250/1697 [=====================>........] - ETA: 0s - loss: 25.02291450/1697 [========================>.....] - ETA: 0s - loss: 25.09361650/1697 [============================>.] - ETA: 0s - loss: 25.0350
Epoch 00030: val_loss improved from 22.54079 to 22.52781, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 300us/sample - loss: 25.0400 - val_loss: 22.5278
Epoch 31/50
  50/1697 [..............................] - ETA: 0s - loss: 25.2756 250/1697 [===>..........................] - ETA: 0s - loss: 25.2353 450/1697 [======>.......................] - ETA: 0s - loss: 24.8441 650/1697 [==========>...................] - ETA: 0s - loss: 25.0299 900/1697 [==============>...............] - ETA: 0s - loss: 25.00711100/1697 [==================>...........] - ETA: 0s - loss: 24.84961300/1697 [=====================>........] - ETA: 0s - loss: 25.17681500/1697 [=========================>....] - ETA: 0s - loss: 25.0411
Epoch 00031: val_loss did not improve from 22.52781
1697/1697 [==============================] - 0s 279us/sample - loss: 24.9898 - val_loss: 22.5780
Epoch 32/50
  50/1697 [..............................] - ETA: 0s - loss: 24.4648 250/1697 [===>..........................] - ETA: 0s - loss: 25.1191 450/1697 [======>.......................] - ETA: 0s - loss: 25.0955 650/1697 [==========>...................] - ETA: 0s - loss: 24.9752 850/1697 [==============>...............] - ETA: 0s - loss: 24.93621050/1697 [=================>............] - ETA: 0s - loss: 24.97121250/1697 [=====================>........] - ETA: 0s - loss: 25.00451450/1697 [========================>.....] - ETA: 0s - loss: 24.90241650/1697 [============================>.] - ETA: 0s - loss: 24.9252
Epoch 00032: val_loss did not improve from 22.52781

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1697/1697 [==============================] - 0s 280us/sample - loss: 24.9176 - val_loss: 22.5358
Epoch 33/50
  50/1697 [..............................] - ETA: 0s - loss: 25.9866 250/1697 [===>..........................] - ETA: 0s - loss: 25.3653 450/1697 [======>.......................] - ETA: 0s - loss: 25.0035 650/1697 [==========>...................] - ETA: 0s - loss: 24.9841 850/1697 [==============>...............] - ETA: 0s - loss: 24.88381050/1697 [=================>............] - ETA: 0s - loss: 24.87701250/1697 [=====================>........] - ETA: 0s - loss: 25.01461450/1697 [========================>.....] - ETA: 0s - loss: 25.07671650/1697 [============================>.] - ETA: 0s - loss: 24.9920
Epoch 00033: val_loss did not improve from 22.52781
1697/1697 [==============================] - 0s 282us/sample - loss: 25.0120 - val_loss: 22.5355
Epoch 34/50
  50/1697 [..............................] - ETA: 0s - loss: 22.8362 250/1697 [===>..........................] - ETA: 0s - loss: 24.0956 450/1697 [======>.......................] - ETA: 0s - loss: 24.0448 650/1697 [==========>...................] - ETA: 0s - loss: 24.5352 850/1697 [==============>...............] - ETA: 0s - loss: 24.62861050/1697 [=================>............] - ETA: 0s - loss: 24.69791250/1697 [=====================>........] - ETA: 0s - loss: 24.71551450/1697 [========================>.....] - ETA: 0s - loss: 24.87801650/1697 [============================>.] - ETA: 0s - loss: 24.8916
Epoch 00034: val_loss did not improve from 22.52781

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-05.
1697/1697 [==============================] - 0s 292us/sample - loss: 24.9469 - val_loss: 22.5698
Epoch 35/50
  50/1697 [..............................] - ETA: 0s - loss: 26.7375 250/1697 [===>..........................] - ETA: 0s - loss: 25.0562 450/1697 [======>.......................] - ETA: 0s - loss: 25.0637 650/1697 [==========>...................] - ETA: 0s - loss: 24.7593 850/1697 [==============>...............] - ETA: 0s - loss: 24.87561050/1697 [=================>............] - ETA: 0s - loss: 25.17231250/1697 [=====================>........] - ETA: 0s - loss: 25.30591450/1697 [========================>.....] - ETA: 0s - loss: 25.25381650/1697 [============================>.] - ETA: 0s - loss: 25.0961
Epoch 00035: val_loss improved from 22.52781 to 22.51623, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1697/1697 [==============================] - 1s 312us/sample - loss: 25.0903 - val_loss: 22.5162
Epoch 36/50
  50/1697 [..............................] - ETA: 0s - loss: 24.9054 250/1697 [===>..........................] - ETA: 0s - loss: 24.5067 450/1697 [======>.......................] - ETA: 0s - loss: 24.5895 650/1697 [==========>...................] - ETA: 0s - loss: 24.7444 850/1697 [==============>...............] - ETA: 0s - loss: 24.91221050/1697 [=================>............] - ETA: 0s - loss: 24.92331250/1697 [=====================>........] - ETA: 0s - loss: 24.89541450/1697 [========================>.....] - ETA: 0s - loss: 24.82651650/1697 [============================>.] - ETA: 0s - loss: 24.8355
Epoch 00036: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 288us/sample - loss: 24.8310 - val_loss: 22.5247
Epoch 37/50
  50/1697 [..............................] - ETA: 0s - loss: 26.7638 250/1697 [===>..........................] - ETA: 0s - loss: 25.5535 450/1697 [======>.......................] - ETA: 0s - loss: 25.0843 650/1697 [==========>...................] - ETA: 0s - loss: 25.2365 850/1697 [==============>...............] - ETA: 0s - loss: 25.24791050/1697 [=================>............] - ETA: 0s - loss: 25.07831250/1697 [=====================>........] - ETA: 0s - loss: 25.05731450/1697 [========================>.....] - ETA: 0s - loss: 25.04411650/1697 [============================>.] - ETA: 0s - loss: 25.0262
Epoch 00037: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 290us/sample - loss: 25.0186 - val_loss: 22.5326
Epoch 38/50
  50/1697 [..............................] - ETA: 0s - loss: 25.1417 250/1697 [===>..........................] - ETA: 0s - loss: 24.8449 450/1697 [======>.......................] - ETA: 0s - loss: 25.2048 650/1697 [==========>...................] - ETA: 0s - loss: 25.3339 850/1697 [==============>...............] - ETA: 0s - loss: 25.23791050/1697 [=================>............] - ETA: 0s - loss: 25.18931250/1697 [=====================>........] - ETA: 0s - loss: 25.09391450/1697 [========================>.....] - ETA: 0s - loss: 24.96261650/1697 [============================>.] - ETA: 0s - loss: 24.9409
Epoch 00038: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 287us/sample - loss: 24.9200 - val_loss: 22.5377
Epoch 39/50
  50/1697 [..............................] - ETA: 0s - loss: 24.7188 250/1697 [===>..........................] - ETA: 0s - loss: 25.1619 450/1697 [======>.......................] - ETA: 0s - loss: 24.9610 650/1697 [==========>...................] - ETA: 0s - loss: 24.9161 850/1697 [==============>...............] - ETA: 0s - loss: 25.13291050/1697 [=================>............] - ETA: 0s - loss: 25.26311250/1697 [=====================>........] - ETA: 0s - loss: 25.26721450/1697 [========================>.....] - ETA: 0s - loss: 25.18261650/1697 [============================>.] - ETA: 0s - loss: 25.0828
Epoch 00039: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 291us/sample - loss: 25.0920 - val_loss: 22.5532
Epoch 40/50
  50/1697 [..............................] - ETA: 0s - loss: 24.6352 250/1697 [===>..........................] - ETA: 0s - loss: 24.9181 450/1697 [======>.......................] - ETA: 0s - loss: 25.0523 550/1697 [========>.....................] - ETA: 0s - loss: 25.1571 700/1697 [===========>..................] - ETA: 0s - loss: 25.0939 900/1697 [==============>...............] - ETA: 0s - loss: 24.95621100/1697 [==================>...........] - ETA: 0s - loss: 25.03601300/1697 [=====================>........] - ETA: 0s - loss: 24.96091500/1697 [=========================>....] - ETA: 0s - loss: 24.9488
Epoch 00040: val_loss did not improve from 22.51623
1697/1697 [==============================] - 1s 304us/sample - loss: 24.8748 - val_loss: 22.5277
Epoch 41/50
  50/1697 [..............................] - ETA: 0s - loss: 25.3343 250/1697 [===>..........................] - ETA: 0s - loss: 24.2301 450/1697 [======>.......................] - ETA: 0s - loss: 25.0394 650/1697 [==========>...................] - ETA: 0s - loss: 25.2869 850/1697 [==============>...............] - ETA: 0s - loss: 25.33531050/1697 [=================>............] - ETA: 0s - loss: 25.26761250/1697 [=====================>........] - ETA: 0s - loss: 25.14261450/1697 [========================>.....] - ETA: 0s - loss: 25.04421650/1697 [============================>.] - ETA: 0s - loss: 25.0127
Epoch 00041: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 286us/sample - loss: 24.9829 - val_loss: 22.5414
Epoch 42/50
  50/1697 [..............................] - ETA: 0s - loss: 25.7878 250/1697 [===>..........................] - ETA: 0s - loss: 25.3009 450/1697 [======>.......................] - ETA: 0s - loss: 24.9321 650/1697 [==========>...................] - ETA: 0s - loss: 25.1291 850/1697 [==============>...............] - ETA: 0s - loss: 25.05831050/1697 [=================>............] - ETA: 0s - loss: 25.30371250/1697 [=====================>........] - ETA: 0s - loss: 25.25271450/1697 [========================>.....] - ETA: 0s - loss: 25.08791650/1697 [============================>.] - ETA: 0s - loss: 25.0642
Epoch 00042: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 291us/sample - loss: 25.0868 - val_loss: 22.5506
Epoch 43/50
  50/1697 [..............................] - ETA: 0s - loss: 25.2667 250/1697 [===>..........................] - ETA: 0s - loss: 25.0566 450/1697 [======>.......................] - ETA: 0s - loss: 25.1276 650/1697 [==========>...................] - ETA: 0s - loss: 25.1497 850/1697 [==============>...............] - ETA: 0s - loss: 25.05911050/1697 [=================>............] - ETA: 0s - loss: 24.98071250/1697 [=====================>........] - ETA: 0s - loss: 24.95931450/1697 [========================>.....] - ETA: 0s - loss: 25.06081650/1697 [============================>.] - ETA: 0s - loss: 24.8927
Epoch 00043: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 285us/sample - loss: 24.9782 - val_loss: 22.5368
Epoch 44/50
  50/1697 [..............................] - ETA: 0s - loss: 25.9305 250/1697 [===>..........................] - ETA: 0s - loss: 24.8148 450/1697 [======>.......................] - ETA: 0s - loss: 24.7458 650/1697 [==========>...................] - ETA: 0s - loss: 24.9002 850/1697 [==============>...............] - ETA: 0s - loss: 24.59701050/1697 [=================>............] - ETA: 0s - loss: 24.57331250/1697 [=====================>........] - ETA: 0s - loss: 24.73311450/1697 [========================>.....] - ETA: 0s - loss: 24.76151650/1697 [============================>.] - ETA: 0s - loss: 24.9849
Epoch 00044: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 285us/sample - loss: 24.9744 - val_loss: 22.5359
Epoch 45/50
  50/1697 [..............................] - ETA: 0s - loss: 28.3015 250/1697 [===>..........................] - ETA: 0s - loss: 25.4060 450/1697 [======>.......................] - ETA: 0s - loss: 25.2481 650/1697 [==========>...................] - ETA: 0s - loss: 25.0320 850/1697 [==============>...............] - ETA: 0s - loss: 24.96421050/1697 [=================>............] - ETA: 0s - loss: 24.94361250/1697 [=====================>........] - ETA: 0s - loss: 24.97061450/1697 [========================>.....] - ETA: 0s - loss: 24.90911650/1697 [============================>.] - ETA: 0s - loss: 24.9462
Epoch 00045: val_loss did not improve from 22.51623
1697/1697 [==============================] - 0s 286us/sample - loss: 24.9326 - val_loss: 22.5310
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:209: RuntimeWarning: divide by zero encountered in log
  predictor_matrix[:,:,:,indicesPRED] = numpy.log(predictor_matrix[:,:,:,indicesPRED])
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00045: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3ffa1a1e80> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3ffa5c4278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa5c4e48> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffa5c4240> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa131e48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa1b5cc0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffa5e6cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa5e6c50> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3ffa5ef240> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa611f98> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa61d9e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa626748> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3ffa62efd0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3ffa638320> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3ffa642d30> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa642c18> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa65c828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa65ca90> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa65cc18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa6669e8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b3ffa666b70> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3ffa1a1e80> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3ffa5c4278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa5c4e48> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffa5c4240> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa131e48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa1b5cc0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffa5e6cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa5e6c50> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3ffa5ef240> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa611f98> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa61d9e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa626748> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3ffa62efd0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3ffa638320> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3ffa642d30> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa642c18> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa65c828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa65ca90> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffa65cc18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa6669e8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b3ffa666b70> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3f9f6eac18> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa699f98> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 112.2674
Epoch 00001: val_loss improved from inf to 34.87116, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 10ms/sample - loss: 72.2211 - val_loss: 34.8712
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.5102
Epoch 00002: val_loss did not improve from 34.87116
121/121 [==============================] - 0s 532us/sample - loss: 32.1921 - val_loss: 35.8204
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.5343
Epoch 00003: val_loss improved from 34.87116 to 32.75736, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 718us/sample - loss: 33.9403 - val_loss: 32.7574
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 30.7719
Epoch 00004: val_loss improved from 32.75736 to 26.17786, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 685us/sample - loss: 28.4051 - val_loss: 26.1779
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.9691
Epoch 00005: val_loss did not improve from 26.17786
121/121 [==============================] - 0s 512us/sample - loss: 26.5117 - val_loss: 27.2919
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2024
Epoch 00006: val_loss improved from 26.17786 to 26.15339, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 668us/sample - loss: 26.6353 - val_loss: 26.1534
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1096
Epoch 00007: val_loss did not improve from 26.15339
121/121 [==============================] - 0s 521us/sample - loss: 25.7429 - val_loss: 26.2245
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.9088
Epoch 00008: val_loss improved from 26.15339 to 26.00130, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 695us/sample - loss: 25.6191 - val_loss: 26.0013
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.7053
Epoch 00009: val_loss improved from 26.00130 to 25.65750, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 699us/sample - loss: 25.4739 - val_loss: 25.6575
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7640
Epoch 00010: val_loss improved from 25.65750 to 25.47804, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 734us/sample - loss: 25.1890 - val_loss: 25.4780
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.6521
Epoch 00011: val_loss improved from 25.47804 to 25.32464, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 688us/sample - loss: 25.0371 - val_loss: 25.3246
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.6592
Epoch 00012: val_loss improved from 25.32464 to 25.23414, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 683us/sample - loss: 24.8732 - val_loss: 25.2341
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4556
Epoch 00013: val_loss improved from 25.23414 to 25.08689, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 693us/sample - loss: 24.8220 - val_loss: 25.0869
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6639
Epoch 00014: val_loss improved from 25.08689 to 24.96741, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 719us/sample - loss: 24.5833 - val_loss: 24.9674
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.5199
Epoch 00015: val_loss improved from 24.96741 to 24.91547, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 24.5360 - val_loss: 24.9155
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3266
Epoch 00016: val_loss improved from 24.91547 to 24.73144, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 24.3628 - val_loss: 24.7314
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8076
Epoch 00017: val_loss improved from 24.73144 to 24.62813, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 671us/sample - loss: 24.2926 - val_loss: 24.6281
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.0093
Epoch 00018: val_loss did not improve from 24.62813
121/121 [==============================] - 0s 538us/sample - loss: 24.2638 - val_loss: 24.6284
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.2335
Epoch 00019: val_loss did not improve from 24.62813

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 509us/sample - loss: 24.1826 - val_loss: 24.6454
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1074
Epoch 00020: val_loss improved from 24.62813 to 24.60793, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 757us/sample - loss: 24.1842 - val_loss: 24.6079
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1487
Epoch 00021: val_loss improved from 24.60793 to 24.60081, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 24.1038 - val_loss: 24.6008
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7284
Epoch 00022: val_loss improved from 24.60081 to 24.56496, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 24.0840 - val_loss: 24.5650
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2714
Epoch 00023: val_loss did not improve from 24.56496
121/121 [==============================] - 0s 506us/sample - loss: 24.0708 - val_loss: 24.6049
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8739
Epoch 00024: val_loss did not improve from 24.56496

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 491us/sample - loss: 24.0747 - val_loss: 24.5784
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1278
Epoch 00025: val_loss improved from 24.56496 to 24.55348, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 24.0487 - val_loss: 24.5535
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0013
Epoch 00026: val_loss improved from 24.55348 to 24.51350, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 698us/sample - loss: 24.0333 - val_loss: 24.5135
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5433
Epoch 00027: val_loss improved from 24.51350 to 24.50197, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 702us/sample - loss: 24.0224 - val_loss: 24.5020
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0167
Epoch 00028: val_loss did not improve from 24.50197
121/121 [==============================] - 0s 533us/sample - loss: 24.0166 - val_loss: 24.5077
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6481
Epoch 00029: val_loss did not improve from 24.50197

Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 503us/sample - loss: 24.0182 - val_loss: 24.5143
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1987
Epoch 00030: val_loss improved from 24.50197 to 24.47293, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 851us/sample - loss: 24.0016 - val_loss: 24.4729
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7298
Epoch 00031: val_loss improved from 24.47293 to 24.44496, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 643us/sample - loss: 24.0086 - val_loss: 24.4450
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6240
Epoch 00032: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 482us/sample - loss: 24.0141 - val_loss: 24.4453
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8543
Epoch 00033: val_loss did not improve from 24.44496

Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 491us/sample - loss: 24.0013 - val_loss: 24.4674
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5971
Epoch 00034: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 471us/sample - loss: 23.9945 - val_loss: 24.4761
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0382
Epoch 00035: val_loss did not improve from 24.44496

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 496us/sample - loss: 23.9937 - val_loss: 24.4819
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0679
Epoch 00036: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 488us/sample - loss: 23.9929 - val_loss: 24.4807
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8888
Epoch 00037: val_loss did not improve from 24.44496

Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 499us/sample - loss: 23.9924 - val_loss: 24.4781
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9155
Epoch 00038: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 487us/sample - loss: 23.9921 - val_loss: 24.4747
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2476
Epoch 00039: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 499us/sample - loss: 23.9919 - val_loss: 24.4746
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8484
Epoch 00040: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 460us/sample - loss: 23.9916 - val_loss: 24.4739
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5034
Epoch 00041: val_loss did not improve from 24.44496
121/121 [==============================] - 0s 476us/sample - loss: 23.9917 - val_loss: 24.4700
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00041: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3ffa9baa90> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3ffa9b2940> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa9b2518> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffa9b2710> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffadc60f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa9bd978> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffaddce10> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffaddca58> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3ffade1358> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae122b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae12b00> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae1c860> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3ffae2d4e0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3ffae2d438> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3ffae3be48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae3bd30> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffae53940> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae53ba8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffae53d30> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae5db00> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b3ffae5dc88> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3ffa9baa90> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3ffa9b2940> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa9b2518> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffa9b2710> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffadc60f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa9bd978> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffaddce10> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffaddca58> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3ffade1358> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae122b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae12b00> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae1c860> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3ffae2d4e0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3ffae2d438> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3ffae3be48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae3bd30> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffae53940> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae53ba8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffae53d30> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae5db00> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b3ffae5dc88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3f9f61a198> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffae9b470> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 51.6782
Epoch 00001: val_loss improved from inf to 26.59460, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 7ms/sample - loss: 34.0406 - val_loss: 26.5946
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.6884
Epoch 00002: val_loss did not improve from 26.59460
121/121 [==============================] - 0s 503us/sample - loss: 28.6954 - val_loss: 26.6650
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 29.6592
Epoch 00003: val_loss improved from 26.59460 to 25.15333, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 686us/sample - loss: 26.0410 - val_loss: 25.1533
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6615
Epoch 00004: val_loss did not improve from 25.15333
121/121 [==============================] - 0s 532us/sample - loss: 25.4896 - val_loss: 25.2941
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8027
Epoch 00005: val_loss improved from 25.15333 to 24.48892, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 711us/sample - loss: 24.8504 - val_loss: 24.4889
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0994
Epoch 00006: val_loss improved from 24.48892 to 24.45289, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 24.7272 - val_loss: 24.4529
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7586
Epoch 00007: val_loss improved from 24.45289 to 24.35126, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 681us/sample - loss: 24.6406 - val_loss: 24.3513
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7806
Epoch 00008: val_loss improved from 24.35126 to 24.18263, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 24.3818 - val_loss: 24.1826
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5118
Epoch 00009: val_loss improved from 24.18263 to 23.90795, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 682us/sample - loss: 24.2579 - val_loss: 23.9080
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0163
Epoch 00010: val_loss improved from 23.90795 to 23.81807, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 671us/sample - loss: 24.0857 - val_loss: 23.8181
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4299
Epoch 00011: val_loss improved from 23.81807 to 23.78564, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 670us/sample - loss: 23.9810 - val_loss: 23.7856
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9751
Epoch 00012: val_loss improved from 23.78564 to 23.57497, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 664us/sample - loss: 23.8744 - val_loss: 23.5750
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3365
Epoch 00013: val_loss improved from 23.57497 to 23.54285, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 715us/sample - loss: 23.8630 - val_loss: 23.5429
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4901
Epoch 00014: val_loss improved from 23.54285 to 23.41097, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 696us/sample - loss: 23.7192 - val_loss: 23.4110
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7042
Epoch 00015: val_loss improved from 23.41097 to 23.33434, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 692us/sample - loss: 23.6946 - val_loss: 23.3343
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2301
Epoch 00016: val_loss did not improve from 23.33434
121/121 [==============================] - 0s 523us/sample - loss: 23.5477 - val_loss: 23.3444
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.5037
Epoch 00017: val_loss did not improve from 23.33434

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 515us/sample - loss: 23.4512 - val_loss: 23.3933
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8876
Epoch 00018: val_loss improved from 23.33434 to 23.20678, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 685us/sample - loss: 23.5367 - val_loss: 23.2068
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9256
Epoch 00019: val_loss improved from 23.20678 to 23.11061, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 685us/sample - loss: 23.3270 - val_loss: 23.1106
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0791
Epoch 00020: val_loss improved from 23.11061 to 23.08283, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 682us/sample - loss: 23.3164 - val_loss: 23.0828
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3888
Epoch 00021: val_loss did not improve from 23.08283
121/121 [==============================] - 0s 513us/sample - loss: 23.2710 - val_loss: 23.1333
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1383
Epoch 00022: val_loss improved from 23.08283 to 23.02685, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 666us/sample - loss: 23.2390 - val_loss: 23.0269
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1325
Epoch 00023: val_loss improved from 23.02685 to 22.97959, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 722us/sample - loss: 23.2983 - val_loss: 22.9796
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8341
Epoch 00024: val_loss did not improve from 22.97959
121/121 [==============================] - 0s 540us/sample - loss: 23.2267 - val_loss: 23.0348
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0868
Epoch 00025: val_loss did not improve from 22.97959

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 510us/sample - loss: 23.1729 - val_loss: 23.0293
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9903
Epoch 00026: val_loss did not improve from 22.97959
121/121 [==============================] - 0s 519us/sample - loss: 23.1230 - val_loss: 22.9826
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2493
Epoch 00027: val_loss improved from 22.97959 to 22.96731, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 683us/sample - loss: 23.1177 - val_loss: 22.9673
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9745
Epoch 00028: val_loss improved from 22.96731 to 22.94297, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 687us/sample - loss: 23.1150 - val_loss: 22.9430
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2681
Epoch 00029: val_loss did not improve from 22.94297
121/121 [==============================] - 0s 517us/sample - loss: 23.1058 - val_loss: 22.9639
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5126
Epoch 00030: val_loss did not improve from 22.94297

Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 501us/sample - loss: 23.0911 - val_loss: 22.9884
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8025
Epoch 00031: val_loss did not improve from 22.94297
121/121 [==============================] - 0s 499us/sample - loss: 23.0924 - val_loss: 22.9932
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3974
Epoch 00032: val_loss did not improve from 22.94297

Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 528us/sample - loss: 23.0901 - val_loss: 22.9639
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2341
Epoch 00033: val_loss did not improve from 22.94297
121/121 [==============================] - 0s 527us/sample - loss: 23.0762 - val_loss: 22.9535
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7892
Epoch 00034: val_loss did not improve from 22.94297

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 513us/sample - loss: 23.0749 - val_loss: 22.9449
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5344
Epoch 00035: val_loss improved from 22.94297 to 22.94290, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 687us/sample - loss: 23.0745 - val_loss: 22.9429
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0667
Epoch 00036: val_loss improved from 22.94290 to 22.94086, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 707us/sample - loss: 23.0740 - val_loss: 22.9409
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6590
Epoch 00037: val_loss did not improve from 22.94086
121/121 [==============================] - 0s 556us/sample - loss: 23.0732 - val_loss: 22.9414
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6044
Epoch 00038: val_loss did not improve from 22.94086

Epoch 00038: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 523us/sample - loss: 23.0724 - val_loss: 22.9414
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9499
Epoch 00039: val_loss improved from 22.94086 to 22.93887, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 694us/sample - loss: 23.0720 - val_loss: 22.9389
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.9775
Epoch 00040: val_loss improved from 22.93887 to 22.93613, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 700us/sample - loss: 23.0718 - val_loss: 22.9361
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8545
Epoch 00041: val_loss did not improve from 22.93613
121/121 [==============================] - 0s 562us/sample - loss: 23.0712 - val_loss: 22.9369
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9689
Epoch 00042: val_loss did not improve from 22.93613
121/121 [==============================] - 0s 514us/sample - loss: 23.0707 - val_loss: 22.9378
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7058
Epoch 00043: val_loss did not improve from 22.93613
121/121 [==============================] - 0s 494us/sample - loss: 23.0700 - val_loss: 22.9377
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3783
Epoch 00044: val_loss did not improve from 22.93613
121/121 [==============================] - 0s 490us/sample - loss: 23.0700 - val_loss: 22.9370
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6778
Epoch 00045: val_loss improved from 22.93613 to 22.93383, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 661us/sample - loss: 23.0693 - val_loss: 22.9338
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2275
Epoch 00046: val_loss improved from 22.93383 to 22.93042, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 2ms/sample - loss: 23.0691 - val_loss: 22.9304
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9273
Epoch 00047: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 549us/sample - loss: 23.0688 - val_loss: 22.9309
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3611
Epoch 00048: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 494us/sample - loss: 23.0689 - val_loss: 22.9374
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2729
Epoch 00049: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 476us/sample - loss: 23.0670 - val_loss: 22.9465
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1131
Epoch 00050: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 492us/sample - loss: 23.0670 - val_loss: 22.9552
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2050
Epoch 00051: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 487us/sample - loss: 23.0699 - val_loss: 22.9594
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5155
Epoch 00052: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 496us/sample - loss: 23.0713 - val_loss: 22.9600
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9671
Epoch 00053: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 511us/sample - loss: 23.0704 - val_loss: 22.9588
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9675
Epoch 00054: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 526us/sample - loss: 23.0690 - val_loss: 22.9568
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6036
Epoch 00055: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 492us/sample - loss: 23.0676 - val_loss: 22.9550
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.4143
Epoch 00056: val_loss did not improve from 22.93042
121/121 [==============================] - 0s 503us/sample - loss: 23.0660 - val_loss: 22.9505
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00056: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3ffb1a40f0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3ffb19da90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb19de80> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffb19dda0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb19d9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb1b8898> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffb5d1e80> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb5d1ac8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3ffb5d93c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb607048> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb607b70> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb60f8d0> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3ffb623550> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3ffb6234a8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3ffb62deb8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb62dda0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb6479b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb647c18> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb647da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb653b70> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b3ffb653cf8> False
Model: "model_8"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_5[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 57, 2)    578         conv2d_4[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b3ffb1a40f0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b3ffb19da90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb19de80> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffb19dda0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb19d9b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb1b8898> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b3ffb5d1e80> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb5d1ac8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b3ffb5d93c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb607048> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb607b70> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb60f8d0> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b3ffb623550> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b3ffb6234a8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b3ffb62deb8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb62dda0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb6479b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb647c18> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b3ffb647da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb653b70> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b3ffb653cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffa67a908> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b3ffb68e4e0> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/200
 20/242 [=>............................] - ETA: 34s - loss: 118.5357180/242 [=====================>........] - ETA: 1s - loss: 71.7191  
Epoch 00001: val_loss improved from inf to 31.04509, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 4s 15ms/sample - loss: 61.4663 - val_loss: 31.0451
Epoch 2/200
 20/242 [=>............................] - ETA: 0s - loss: 32.2164240/242 [============================>.] - ETA: 0s - loss: 31.2873
Epoch 00002: val_loss improved from 31.04509 to 26.45586, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 445us/sample - loss: 31.2824 - val_loss: 26.4559
Epoch 3/200
 20/242 [=>............................] - ETA: 0s - loss: 27.7955240/242 [============================>.] - ETA: 0s - loss: 24.8662
Epoch 00003: val_loss improved from 26.45586 to 24.06505, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 453us/sample - loss: 24.8712 - val_loss: 24.0650
Epoch 4/200
 20/242 [=>............................] - ETA: 0s - loss: 23.2300240/242 [============================>.] - ETA: 0s - loss: 23.7969
Epoch 00004: val_loss improved from 24.06505 to 23.47427, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 462us/sample - loss: 23.7652 - val_loss: 23.4743
Epoch 5/200
 20/242 [=>............................] - ETA: 0s - loss: 24.1547240/242 [============================>.] - ETA: 0s - loss: 23.4639
Epoch 00005: val_loss improved from 23.47427 to 23.26687, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 453us/sample - loss: 23.4248 - val_loss: 23.2669
Epoch 6/200
 20/242 [=>............................] - ETA: 0s - loss: 24.6179240/242 [============================>.] - ETA: 0s - loss: 23.2792
Epoch 00006: val_loss improved from 23.26687 to 23.07896, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 469us/sample - loss: 23.2268 - val_loss: 23.0790
Epoch 7/200
 20/242 [=>............................] - ETA: 0s - loss: 23.1948220/242 [==========================>...] - ETA: 0s - loss: 23.2095
Epoch 00007: val_loss improved from 23.07896 to 23.03575, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 497us/sample - loss: 23.1191 - val_loss: 23.0358
Epoch 8/200
 20/242 [=>............................] - ETA: 0s - loss: 23.1790240/242 [============================>.] - ETA: 0s - loss: 22.8296
Epoch 00008: val_loss improved from 23.03575 to 22.87262, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 455us/sample - loss: 22.9641 - val_loss: 22.8726
Epoch 9/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3926
Epoch 00009: val_loss improved from 22.87262 to 22.84878, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 454us/sample - loss: 22.9059 - val_loss: 22.8488
Epoch 10/200
 20/242 [=>............................] - ETA: 0s - loss: 23.0139240/242 [============================>.] - ETA: 0s - loss: 22.8695
Epoch 00010: val_loss improved from 22.84878 to 22.82868, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 465us/sample - loss: 22.8073 - val_loss: 22.8287
Epoch 11/200
 20/242 [=>............................] - ETA: 0s - loss: 21.1610240/242 [============================>.] - ETA: 0s - loss: 22.5425
Epoch 00011: val_loss improved from 22.82868 to 22.71866, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 448us/sample - loss: 22.7741 - val_loss: 22.7187
Epoch 12/200
 20/242 [=>............................] - ETA: 0s - loss: 22.5954240/242 [============================>.] - ETA: 0s - loss: 22.6444
Epoch 00012: val_loss did not improve from 22.71866
242/242 [==============================] - 0s 378us/sample - loss: 22.7228 - val_loss: 22.8078
Epoch 13/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3188240/242 [============================>.] - ETA: 0s - loss: 22.6458
Epoch 00013: val_loss improved from 22.71866 to 22.61738, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 467us/sample - loss: 22.6510 - val_loss: 22.6174
Epoch 14/200
 20/242 [=>............................] - ETA: 0s - loss: 21.2981240/242 [============================>.] - ETA: 0s - loss: 22.5978
Epoch 00014: val_loss improved from 22.61738 to 22.59846, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 473us/sample - loss: 22.5944 - val_loss: 22.5985
Epoch 15/200
 20/242 [=>............................] - ETA: 0s - loss: 21.0154240/242 [============================>.] - ETA: 0s - loss: 22.6000
Epoch 00015: val_loss improved from 22.59846 to 22.59550, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 465us/sample - loss: 22.5923 - val_loss: 22.5955
Epoch 16/200
 20/242 [=>............................] - ETA: 0s - loss: 22.0530240/242 [============================>.] - ETA: 0s - loss: 22.5724
Epoch 00016: val_loss improved from 22.59550 to 22.56178, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 459us/sample - loss: 22.5475 - val_loss: 22.5618
Epoch 17/200
 20/242 [=>............................] - ETA: 0s - loss: 21.4493240/242 [============================>.] - ETA: 0s - loss: 22.4631
Epoch 00017: val_loss did not improve from 22.56178
242/242 [==============================] - 0s 375us/sample - loss: 22.5060 - val_loss: 22.5818
Epoch 18/200
 20/242 [=>............................] - ETA: 0s - loss: 21.6049240/242 [============================>.] - ETA: 0s - loss: 22.5054
Epoch 00018: val_loss improved from 22.56178 to 22.53821, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 465us/sample - loss: 22.5619 - val_loss: 22.5382
Epoch 19/200
 20/242 [=>............................] - ETA: 0s - loss: 21.4947240/242 [============================>.] - ETA: 0s - loss: 22.5147
Epoch 00019: val_loss improved from 22.53821 to 22.53794, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 461us/sample - loss: 22.4772 - val_loss: 22.5379
Epoch 20/200
 20/242 [=>............................] - ETA: 0s - loss: 25.9452100/242 [===========>..................] - ETA: 0s - loss: 22.2808
Epoch 00020: val_loss improved from 22.53794 to 22.52414, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 675us/sample - loss: 22.5597 - val_loss: 22.5241
Epoch 21/200
 20/242 [=>............................] - ETA: 0s - loss: 21.5494
Epoch 00021: val_loss did not improve from 22.52414
242/242 [==============================] - 0s 364us/sample - loss: 22.4596 - val_loss: 22.6515
Epoch 22/200
 20/242 [=>............................] - ETA: 0s - loss: 23.0046240/242 [============================>.] - ETA: 0s - loss: 22.2980
Epoch 00022: val_loss did not improve from 22.52414

Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 379us/sample - loss: 22.4769 - val_loss: 22.5637
Epoch 23/200
 20/242 [=>............................] - ETA: 0s - loss: 21.9353240/242 [============================>.] - ETA: 0s - loss: 22.5018
Epoch 00023: val_loss did not improve from 22.52414
242/242 [==============================] - 0s 383us/sample - loss: 22.4845 - val_loss: 22.5255
Epoch 24/200
 20/242 [=>............................] - ETA: 0s - loss: 19.5828240/242 [============================>.] - ETA: 0s - loss: 22.4396
Epoch 00024: val_loss improved from 22.52414 to 22.50721, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 480us/sample - loss: 22.4516 - val_loss: 22.5072
Epoch 25/200
 20/242 [=>............................] - ETA: 0s - loss: 26.8739240/242 [============================>.] - ETA: 0s - loss: 22.4146
Epoch 00025: val_loss did not improve from 22.50721
242/242 [==============================] - 0s 396us/sample - loss: 22.4396 - val_loss: 22.5500
Epoch 26/200
 20/242 [=>............................] - ETA: 0s - loss: 20.9314240/242 [============================>.] - ETA: 0s - loss: 22.4711
Epoch 00026: val_loss improved from 22.50721 to 22.49506, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2004/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 460us/sample - loss: 22.4410 - val_loss: 22.4951
Epoch 27/200
 20/242 [=>............................] - ETA: 0s - loss: 23.1024240/242 [============================>.] - ETA: 0s - loss: 22.4710
Epoch 00027: val_loss did not improve from 22.49506
242/242 [==============================] - 0s 389us/sample - loss: 22.4370 - val_loss: 22.5429
Epoch 28/200
 20/242 [=>............................] - ETA: 0s - loss: 22.4763240/242 [============================>.] - ETA: 0s - loss: 22.4308
Epoch 00028: val_loss did not improve from 22.49506

Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 379us/sample - loss: 22.4261 - val_loss: 22.5068
Epoch 29/200
 20/242 [=>............................] - ETA: 0s - loss: 25.1526240/242 [============================>.] - ETA: 0s - loss: 22.4284
Epoch 00029: val_loss did not improve from 22.49506
242/242 [==============================] - 0s 391us/sample - loss: 22.4243 - val_loss: 22.5094
Epoch 30/200
 20/242 [=>............................] - ETA: 0s - loss: 21.9679240/242 [============================>.] - ETA: 0s - loss: 22.4213
Epoch 00030: val_loss did not improve from 22.49506

Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 373us/sample - loss: 22.4318 - val_loss: 22.5363
Epoch 31/200
 20/242 [=>............................] - ETA: 0s - loss: 22.4950240/242 [============================>.] - ETA: 0s - loss: 22.4591
Epoch 00031: val_loss did not improve from 22.49506
242/242 [==============================] - 0s 365us/sample - loss: 22.4205 - val_loss: 22.5048
Epoch 32/200
 20/242 [=>............................] - ETA: 0s - loss: 22.5618
Epoch 00032: val_loss did not improve from 22.49506

Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 352us/sample - loss: 22.4166 - val_loss: 22.5133
Epoch 33/200
 20/242 [=>............................] - ETA: 0s - loss: 22.7576240/242 [============================>.] - ETA: 0s - loss: 22.3580
Epoch 00033: val_loss did not improve from 22.49506
242/242 [==============================] - 0s 367us/sample - loss: 22.4150 - val_loss: 22.5107
Epoch 34/200
 20/242 [=>............................] - ETA: 0s - loss: 20.4954
Epoch 00034: val_loss did not improve from 22.49506

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 358us/sample - loss: 22.4154 - val_loss: 22.5031
Epoch 35/200
 20/242 [=>............................] - ETA: 0s - loss: 23.5138220/242 [==========================>...] - ETA: 0s - loss: 22.3432
Epoch 00035: val_loss did not improve from 22.49506
242/242 [==============================] - 0s 389us/sample - loss: 22.4138 - val_loss: 22.5042
Epoch 36/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3578
Epoch 00036: val_loss did not improve from 22.49506

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.
242/242 [==============================] - 0s 359us/sample - loss: 22.4140 - val_loss: 22.5044
Epoch 00036: early stopping
done
