2020-11-09 20:59:26.452127: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 20:59:26.984212: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 20:59:26.984479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556d3606b6d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 20:59:26.984514: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 20:59:27.003539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 20:59:27.120614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:59:27.138347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:59:27.340091: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:59:27.395373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:59:27.495132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:59:27.603579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:59:27.707461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:59:27.836796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:59:27.840211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:59:27.840328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:59:28.034986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:59:28.035059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:59:28.035090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:59:28.040476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:59:28.042971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556d36de6c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 20:59:28.043001: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 20:59:28.047729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:59:28.047820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:59:28.047840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:59:28.047857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:59:28.050866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:59:28.050890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:59:28.050907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:59:28.050923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:59:28.055366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:59:28.055414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:59:28.055426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:59:28.055436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:59:28.058476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:59:28.060463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:59:28.060534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:59:28.060553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:59:28.060575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:59:28.060591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:59:28.060607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:59:28.060622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:59:28.060639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:59:28.066408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:59:28.066450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:59:28.066463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:59:28.066473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:59:28.069520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 159.6742, 152.9396
Mean and standard deviation for "p_sfc" = 984.1400, 62.0586
Mean and standard deviation for "u_tr_p" = 12.7150, 12.3102
Mean and standard deviation for "v_tr_p" = 1.2910, 13.3137
Mean and standard deviation for "Z_p" = 5574.0077, 203.7151
Mean and standard deviation for "IWV" = 13.4042, 7.7252
2020-11-09 20:59:41.298287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:59:41.298447: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:59:41.298471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:59:41.298488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:59:41.298504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:59:41.298519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:59:41.298535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:59:41.298551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:59:41.301620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:59:41.401145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 20:59:41.401260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 20:59:41.401280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 20:59:41.401296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 20:59:41.401313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 20:59:41.401328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 20:59:41.401344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 20:59:41.401360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:59:41.406472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 20:59:41.406529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 20:59:41.406542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 20:59:41.406552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 20:59:41.409605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 20:59:49.914627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 20:59:52.664195: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 20:59:52.679546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 203.2136, 182.5228
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 1698 samples, validate on 121 samples
Epoch 1/50
  50/1698 [..............................] - ETA: 2:47 - loss: 144.1106 100/1698 [>.............................] - ETA: 1:22 - loss: 150.6454 300/1698 [====>.........................] - ETA: 24s - loss: 152.3384  500/1698 [=======>......................] - ETA: 12s - loss: 150.1402 700/1698 [===========>..................] - ETA: 7s - loss: 143.7542  900/1698 [==============>...............] - ETA: 4s - loss: 128.91161100/1698 [==================>...........] - ETA: 2s - loss: 118.40821300/1698 [=====================>........] - ETA: 1s - loss: 109.36351500/1698 [=========================>....] - ETA: 0s - loss: 101.9188
Epoch 00001: val_loss improved from inf to 50.29253, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 6s 4ms/sample - loss: 95.8553 - val_loss: 50.2925
Epoch 2/50
  50/1698 [..............................] - ETA: 0s - loss: 44.3503 250/1698 [===>..........................] - ETA: 0s - loss: 44.4919 450/1698 [======>.......................] - ETA: 0s - loss: 43.4558 650/1698 [==========>...................] - ETA: 0s - loss: 42.5176 850/1698 [==============>...............] - ETA: 0s - loss: 41.55371050/1698 [=================>............] - ETA: 0s - loss: 40.89801250/1698 [=====================>........] - ETA: 0s - loss: 39.97161450/1698 [========================>.....] - ETA: 0s - loss: 39.19401650/1698 [============================>.] - ETA: 0s - loss: 38.5228
Epoch 00002: val_loss did not improve from 50.29253
1698/1698 [==============================] - 0s 292us/sample - loss: 38.3507 - val_loss: 66.8116
Epoch 3/50
  50/1698 [..............................] - ETA: 0s - loss: 35.6825 250/1698 [===>..........................] - ETA: 0s - loss: 32.6120 450/1698 [======>.......................] - ETA: 0s - loss: 32.1275 650/1698 [==========>...................] - ETA: 0s - loss: 31.6936 850/1698 [==============>...............] - ETA: 0s - loss: 31.21091050/1698 [=================>............] - ETA: 0s - loss: 31.08921250/1698 [=====================>........] - ETA: 0s - loss: 30.94981450/1698 [========================>.....] - ETA: 0s - loss: 30.52381650/1698 [============================>.] - ETA: 0s - loss: 30.5773
Epoch 00003: val_loss did not improve from 50.29253

Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1698/1698 [==============================] - 0s 289us/sample - loss: 30.5201 - val_loss: 60.4926
Epoch 4/50
  50/1698 [..............................] - ETA: 0s - loss: 29.2077 250/1698 [===>..........................] - ETA: 0s - loss: 28.1701 450/1698 [======>.......................] - ETA: 0s - loss: 28.4265 650/1698 [==========>...................] - ETA: 0s - loss: 28.4158 850/1698 [==============>...............] - ETA: 0s - loss: 28.34421050/1698 [=================>............] - ETA: 0s - loss: 28.50761250/1698 [=====================>........] - ETA: 0s - loss: 28.65311450/1698 [========================>.....] - ETA: 0s - loss: 28.60561650/1698 [============================>.] - ETA: 0s - loss: 28.5664
Epoch 00004: val_loss did not improve from 50.29253
1698/1698 [==============================] - 0s 289us/sample - loss: 28.5436 - val_loss: 50.4286
Epoch 5/50
  50/1698 [..............................] - ETA: 0s - loss: 27.2973 250/1698 [===>..........................] - ETA: 0s - loss: 27.4169 450/1698 [======>.......................] - ETA: 0s - loss: 27.6528 650/1698 [==========>...................] - ETA: 0s - loss: 27.9965 850/1698 [==============>...............] - ETA: 0s - loss: 28.03761050/1698 [=================>............] - ETA: 0s - loss: 27.92781250/1698 [=====================>........] - ETA: 0s - loss: 27.99311400/1698 [=======================>......] - ETA: 0s - loss: 28.02531550/1698 [==========================>...] - ETA: 0s - loss: 28.0328
Epoch 00005: val_loss improved from 50.29253 to 42.95272, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 344us/sample - loss: 27.9555 - val_loss: 42.9527
Epoch 6/50
  50/1698 [..............................] - ETA: 0s - loss: 26.9720 250/1698 [===>..........................] - ETA: 0s - loss: 27.2707 450/1698 [======>.......................] - ETA: 0s - loss: 27.3082 650/1698 [==========>...................] - ETA: 0s - loss: 27.4978 850/1698 [==============>...............] - ETA: 0s - loss: 27.70641050/1698 [=================>............] - ETA: 0s - loss: 27.84881250/1698 [=====================>........] - ETA: 0s - loss: 27.89651450/1698 [========================>.....] - ETA: 0s - loss: 27.86571650/1698 [============================>.] - ETA: 0s - loss: 27.9393
Epoch 00006: val_loss improved from 42.95272 to 33.17713, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 302us/sample - loss: 27.9257 - val_loss: 33.1771
Epoch 7/50
  50/1698 [..............................] - ETA: 0s - loss: 26.2549 250/1698 [===>..........................] - ETA: 0s - loss: 26.5341 450/1698 [======>.......................] - ETA: 0s - loss: 27.2409 650/1698 [==========>...................] - ETA: 0s - loss: 26.9653 850/1698 [==============>...............] - ETA: 0s - loss: 27.41031050/1698 [=================>............] - ETA: 0s - loss: 27.57461250/1698 [=====================>........] - ETA: 0s - loss: 27.54831450/1698 [========================>.....] - ETA: 0s - loss: 27.61451650/1698 [============================>.] - ETA: 0s - loss: 27.3635
Epoch 00007: val_loss improved from 33.17713 to 31.85386, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 302us/sample - loss: 27.4446 - val_loss: 31.8539
Epoch 8/50
  50/1698 [..............................] - ETA: 0s - loss: 25.9503 250/1698 [===>..........................] - ETA: 0s - loss: 26.3041 450/1698 [======>.......................] - ETA: 0s - loss: 26.4080 650/1698 [==========>...................] - ETA: 0s - loss: 26.4543 850/1698 [==============>...............] - ETA: 0s - loss: 26.66781050/1698 [=================>............] - ETA: 0s - loss: 26.87531250/1698 [=====================>........] - ETA: 0s - loss: 26.87771450/1698 [========================>.....] - ETA: 0s - loss: 27.05161650/1698 [============================>.] - ETA: 0s - loss: 27.0909
Epoch 00008: val_loss improved from 31.85386 to 27.55882, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 312us/sample - loss: 27.1196 - val_loss: 27.5588
Epoch 9/50
  50/1698 [..............................] - ETA: 0s - loss: 26.7482 250/1698 [===>..........................] - ETA: 0s - loss: 26.7468 450/1698 [======>.......................] - ETA: 0s - loss: 27.6238 650/1698 [==========>...................] - ETA: 0s - loss: 27.7036 850/1698 [==============>...............] - ETA: 0s - loss: 27.78261050/1698 [=================>............] - ETA: 0s - loss: 27.59081250/1698 [=====================>........] - ETA: 0s - loss: 27.66511450/1698 [========================>.....] - ETA: 0s - loss: 27.68371650/1698 [============================>.] - ETA: 0s - loss: 27.6554
Epoch 00009: val_loss did not improve from 27.55882
1698/1698 [==============================] - 0s 285us/sample - loss: 27.6966 - val_loss: 28.7583
Epoch 10/50
  50/1698 [..............................] - ETA: 0s - loss: 26.2065 250/1698 [===>..........................] - ETA: 0s - loss: 27.2950 450/1698 [======>.......................] - ETA: 0s - loss: 27.4974 650/1698 [==========>...................] - ETA: 0s - loss: 27.3368 850/1698 [==============>...............] - ETA: 0s - loss: 27.25581050/1698 [=================>............] - ETA: 0s - loss: 27.28621250/1698 [=====================>........] - ETA: 0s - loss: 27.08021450/1698 [========================>.....] - ETA: 0s - loss: 26.98411650/1698 [============================>.] - ETA: 0s - loss: 26.9068
Epoch 00010: val_loss improved from 27.55882 to 25.87605, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 305us/sample - loss: 26.8633 - val_loss: 25.8761
Epoch 11/50
  50/1698 [..............................] - ETA: 0s - loss: 26.8467 250/1698 [===>..........................] - ETA: 0s - loss: 26.0927 450/1698 [======>.......................] - ETA: 0s - loss: 26.3159 650/1698 [==========>...................] - ETA: 0s - loss: 26.8249 850/1698 [==============>...............] - ETA: 0s - loss: 26.73581050/1698 [=================>............] - ETA: 0s - loss: 26.99021250/1698 [=====================>........] - ETA: 0s - loss: 26.98841450/1698 [========================>.....] - ETA: 0s - loss: 26.95351650/1698 [============================>.] - ETA: 0s - loss: 26.8798
Epoch 00011: val_loss did not improve from 25.87605
1698/1698 [==============================] - 0s 292us/sample - loss: 26.8056 - val_loss: 26.0109
Epoch 12/50
  50/1698 [..............................] - ETA: 0s - loss: 27.6876 250/1698 [===>..........................] - ETA: 0s - loss: 27.2675 450/1698 [======>.......................] - ETA: 0s - loss: 27.2196 650/1698 [==========>...................] - ETA: 0s - loss: 26.7541 850/1698 [==============>...............] - ETA: 0s - loss: 27.16251050/1698 [=================>............] - ETA: 0s - loss: 27.04711250/1698 [=====================>........] - ETA: 0s - loss: 26.92071450/1698 [========================>.....] - ETA: 0s - loss: 26.77791650/1698 [============================>.] - ETA: 0s - loss: 26.6954
Epoch 00012: val_loss improved from 25.87605 to 24.65695, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 306us/sample - loss: 26.6687 - val_loss: 24.6570
Epoch 13/50
  50/1698 [..............................] - ETA: 0s - loss: 27.3508 250/1698 [===>..........................] - ETA: 0s - loss: 26.0978 450/1698 [======>.......................] - ETA: 0s - loss: 26.2319 650/1698 [==========>...................] - ETA: 0s - loss: 26.3068 850/1698 [==============>...............] - ETA: 0s - loss: 26.40811050/1698 [=================>............] - ETA: 0s - loss: 26.46361250/1698 [=====================>........] - ETA: 0s - loss: 26.47801450/1698 [========================>.....] - ETA: 0s - loss: 26.41621650/1698 [============================>.] - ETA: 0s - loss: 26.6214
Epoch 00013: val_loss did not improve from 24.65695
1698/1698 [==============================] - 0s 291us/sample - loss: 26.5919 - val_loss: 24.9059
Epoch 14/50
  50/1698 [..............................] - ETA: 0s - loss: 26.3897 250/1698 [===>..........................] - ETA: 0s - loss: 26.9821 450/1698 [======>.......................] - ETA: 0s - loss: 26.7989 650/1698 [==========>...................] - ETA: 0s - loss: 26.5741 850/1698 [==============>...............] - ETA: 0s - loss: 26.54951050/1698 [=================>............] - ETA: 0s - loss: 26.48161250/1698 [=====================>........] - ETA: 0s - loss: 26.31631450/1698 [========================>.....] - ETA: 0s - loss: 26.27341650/1698 [============================>.] - ETA: 0s - loss: 26.2863
Epoch 00014: val_loss did not improve from 24.65695

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1698/1698 [==============================] - 0s 289us/sample - loss: 26.2905 - val_loss: 24.7955
Epoch 15/50
  50/1698 [..............................] - ETA: 0s - loss: 25.9659 250/1698 [===>..........................] - ETA: 0s - loss: 27.0028 450/1698 [======>.......................] - ETA: 0s - loss: 26.4703 650/1698 [==========>...................] - ETA: 0s - loss: 26.2528 850/1698 [==============>...............] - ETA: 0s - loss: 26.33251050/1698 [=================>............] - ETA: 0s - loss: 26.27401250/1698 [=====================>........] - ETA: 0s - loss: 26.39661450/1698 [========================>.....] - ETA: 0s - loss: 26.34871650/1698 [============================>.] - ETA: 0s - loss: 26.2579
Epoch 00015: val_loss improved from 24.65695 to 24.32319, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 307us/sample - loss: 26.2427 - val_loss: 24.3232
Epoch 16/50
  50/1698 [..............................] - ETA: 0s - loss: 27.2803 250/1698 [===>..........................] - ETA: 0s - loss: 26.4419 450/1698 [======>.......................] - ETA: 0s - loss: 26.1020 650/1698 [==========>...................] - ETA: 0s - loss: 26.1363 850/1698 [==============>...............] - ETA: 0s - loss: 26.35681050/1698 [=================>............] - ETA: 0s - loss: 26.22871250/1698 [=====================>........] - ETA: 0s - loss: 26.27071450/1698 [========================>.....] - ETA: 0s - loss: 26.20721650/1698 [============================>.] - ETA: 0s - loss: 26.1242
Epoch 00016: val_loss improved from 24.32319 to 24.08879, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 303us/sample - loss: 26.1263 - val_loss: 24.0888
Epoch 17/50
  50/1698 [..............................] - ETA: 0s - loss: 24.8586 250/1698 [===>..........................] - ETA: 0s - loss: 25.9720 450/1698 [======>.......................] - ETA: 0s - loss: 26.0074 650/1698 [==========>...................] - ETA: 0s - loss: 26.2012 850/1698 [==============>...............] - ETA: 0s - loss: 26.01091050/1698 [=================>............] - ETA: 0s - loss: 26.14311250/1698 [=====================>........] - ETA: 0s - loss: 26.27541450/1698 [========================>.....] - ETA: 0s - loss: 26.20721650/1698 [============================>.] - ETA: 0s - loss: 26.1953
Epoch 00017: val_loss did not improve from 24.08879
1698/1698 [==============================] - 0s 288us/sample - loss: 26.2109 - val_loss: 24.1698
Epoch 18/50
  50/1698 [..............................] - ETA: 0s - loss: 26.5607 250/1698 [===>..........................] - ETA: 0s - loss: 26.2280 450/1698 [======>.......................] - ETA: 0s - loss: 25.8150 650/1698 [==========>...................] - ETA: 0s - loss: 25.6486 850/1698 [==============>...............] - ETA: 0s - loss: 26.10571050/1698 [=================>............] - ETA: 0s - loss: 26.16711250/1698 [=====================>........] - ETA: 0s - loss: 26.19571450/1698 [========================>.....] - ETA: 0s - loss: 26.13671650/1698 [============================>.] - ETA: 0s - loss: 26.1530
Epoch 00018: val_loss improved from 24.08879 to 23.92110, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 304us/sample - loss: 26.1352 - val_loss: 23.9211
Epoch 19/50
  50/1698 [..............................] - ETA: 0s - loss: 25.3100 250/1698 [===>..........................] - ETA: 0s - loss: 25.8259 450/1698 [======>.......................] - ETA: 0s - loss: 26.3481 650/1698 [==========>...................] - ETA: 0s - loss: 26.3872 850/1698 [==============>...............] - ETA: 0s - loss: 26.51511050/1698 [=================>............] - ETA: 0s - loss: 26.39161250/1698 [=====================>........] - ETA: 0s - loss: 26.18941450/1698 [========================>.....] - ETA: 0s - loss: 26.13701650/1698 [============================>.] - ETA: 0s - loss: 26.1477
Epoch 00019: val_loss improved from 23.92110 to 23.89731, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 304us/sample - loss: 26.2149 - val_loss: 23.8973
Epoch 20/50
  50/1698 [..............................] - ETA: 0s - loss: 25.5593 250/1698 [===>..........................] - ETA: 0s - loss: 26.1708 450/1698 [======>.......................] - ETA: 0s - loss: 26.1442 650/1698 [==========>...................] - ETA: 0s - loss: 26.1572 850/1698 [==============>...............] - ETA: 0s - loss: 26.33461050/1698 [=================>............] - ETA: 0s - loss: 26.27601250/1698 [=====================>........] - ETA: 0s - loss: 26.29431450/1698 [========================>.....] - ETA: 0s - loss: 26.11701650/1698 [============================>.] - ETA: 0s - loss: 26.0014
Epoch 00020: val_loss did not improve from 23.89731
1698/1698 [==============================] - 0s 283us/sample - loss: 26.0115 - val_loss: 24.0278
Epoch 21/50
  50/1698 [..............................] - ETA: 0s - loss: 29.9351 250/1698 [===>..........................] - ETA: 0s - loss: 26.7221 450/1698 [======>.......................] - ETA: 0s - loss: 26.2881 650/1698 [==========>...................] - ETA: 0s - loss: 26.1773 850/1698 [==============>...............] - ETA: 0s - loss: 26.19041050/1698 [=================>............] - ETA: 0s - loss: 26.33961250/1698 [=====================>........] - ETA: 0s - loss: 26.30641450/1698 [========================>.....] - ETA: 0s - loss: 26.36601650/1698 [============================>.] - ETA: 0s - loss: 26.2883
Epoch 00021: val_loss improved from 23.89731 to 23.86549, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 302us/sample - loss: 26.2319 - val_loss: 23.8655
Epoch 22/50
  50/1698 [..............................] - ETA: 0s - loss: 24.8276 250/1698 [===>..........................] - ETA: 0s - loss: 26.8357 450/1698 [======>.......................] - ETA: 0s - loss: 26.3374 650/1698 [==========>...................] - ETA: 0s - loss: 26.1947 850/1698 [==============>...............] - ETA: 0s - loss: 26.20621050/1698 [=================>............] - ETA: 0s - loss: 26.20181250/1698 [=====================>........] - ETA: 0s - loss: 26.04631450/1698 [========================>.....] - ETA: 0s - loss: 26.00181650/1698 [============================>.] - ETA: 0s - loss: 26.1183
Epoch 00022: val_loss improved from 23.86549 to 23.71437, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 307us/sample - loss: 26.1184 - val_loss: 23.7144
Epoch 23/50
  50/1698 [..............................] - ETA: 0s - loss: 27.6444 250/1698 [===>..........................] - ETA: 0s - loss: 25.8069 450/1698 [======>.......................] - ETA: 0s - loss: 26.4210 650/1698 [==========>...................] - ETA: 0s - loss: 26.2097 850/1698 [==============>...............] - ETA: 0s - loss: 26.07451050/1698 [=================>............] - ETA: 0s - loss: 26.11421250/1698 [=====================>........] - ETA: 0s - loss: 25.93981450/1698 [========================>.....] - ETA: 0s - loss: 25.91991650/1698 [============================>.] - ETA: 0s - loss: 26.0039
Epoch 00023: val_loss did not improve from 23.71437
1698/1698 [==============================] - 0s 291us/sample - loss: 26.0304 - val_loss: 23.8604
Epoch 24/50
  50/1698 [..............................] - ETA: 0s - loss: 25.6611 250/1698 [===>..........................] - ETA: 0s - loss: 26.0968 450/1698 [======>.......................] - ETA: 0s - loss: 26.3011 650/1698 [==========>...................] - ETA: 0s - loss: 25.9931 850/1698 [==============>...............] - ETA: 0s - loss: 25.90901050/1698 [=================>............] - ETA: 0s - loss: 25.89401250/1698 [=====================>........] - ETA: 0s - loss: 25.84541450/1698 [========================>.....] - ETA: 0s - loss: 25.99461650/1698 [============================>.] - ETA: 0s - loss: 26.0023
Epoch 00024: val_loss did not improve from 23.71437

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1698/1698 [==============================] - 0s 281us/sample - loss: 25.9754 - val_loss: 23.7358
Epoch 25/50
  50/1698 [..............................] - ETA: 0s - loss: 25.1826 250/1698 [===>..........................] - ETA: 0s - loss: 26.2069 450/1698 [======>.......................] - ETA: 0s - loss: 26.0570 650/1698 [==========>...................] - ETA: 0s - loss: 25.9779 850/1698 [==============>...............] - ETA: 0s - loss: 25.94061050/1698 [=================>............] - ETA: 0s - loss: 25.99531250/1698 [=====================>........] - ETA: 0s - loss: 25.92831450/1698 [========================>.....] - ETA: 0s - loss: 26.01231650/1698 [============================>.] - ETA: 0s - loss: 25.9857
Epoch 00025: val_loss improved from 23.71437 to 23.52179, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 300us/sample - loss: 26.0305 - val_loss: 23.5218
Epoch 26/50
  50/1698 [..............................] - ETA: 0s - loss: 24.5289 250/1698 [===>..........................] - ETA: 0s - loss: 25.5158 450/1698 [======>.......................] - ETA: 0s - loss: 25.9343 650/1698 [==========>...................] - ETA: 0s - loss: 25.7463 850/1698 [==============>...............] - ETA: 0s - loss: 25.89001050/1698 [=================>............] - ETA: 0s - loss: 25.85051250/1698 [=====================>........] - ETA: 0s - loss: 25.93921450/1698 [========================>.....] - ETA: 0s - loss: 25.96961650/1698 [============================>.] - ETA: 0s - loss: 26.0184
Epoch 00026: val_loss did not improve from 23.52179
1698/1698 [==============================] - 0s 276us/sample - loss: 26.0568 - val_loss: 23.5841
Epoch 27/50
  50/1698 [..............................] - ETA: 0s - loss: 25.4388 250/1698 [===>..........................] - ETA: 0s - loss: 25.8742 450/1698 [======>.......................] - ETA: 0s - loss: 26.1887 650/1698 [==========>...................] - ETA: 0s - loss: 26.0892 750/1698 [============>.................] - ETA: 0s - loss: 25.9714 900/1698 [==============>...............] - ETA: 0s - loss: 26.17301100/1698 [==================>...........] - ETA: 0s - loss: 26.03291300/1698 [=====================>........] - ETA: 0s - loss: 25.95611500/1698 [=========================>....] - ETA: 0s - loss: 25.8861
Epoch 00027: val_loss did not improve from 23.52179

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1698/1698 [==============================] - 1s 301us/sample - loss: 25.9839 - val_loss: 23.5594
Epoch 28/50
  50/1698 [..............................] - ETA: 0s - loss: 25.3387 250/1698 [===>..........................] - ETA: 0s - loss: 25.9675 450/1698 [======>.......................] - ETA: 0s - loss: 26.0851 650/1698 [==========>...................] - ETA: 0s - loss: 26.1113 850/1698 [==============>...............] - ETA: 0s - loss: 26.03071050/1698 [=================>............] - ETA: 0s - loss: 26.01091250/1698 [=====================>........] - ETA: 0s - loss: 25.92551450/1698 [========================>.....] - ETA: 0s - loss: 25.92551650/1698 [============================>.] - ETA: 0s - loss: 26.0433
Epoch 00028: val_loss improved from 23.52179 to 23.50288, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 322us/sample - loss: 26.0052 - val_loss: 23.5029
Epoch 29/50
  50/1698 [..............................] - ETA: 0s - loss: 26.8083 250/1698 [===>..........................] - ETA: 0s - loss: 26.0563 450/1698 [======>.......................] - ETA: 0s - loss: 26.3094 650/1698 [==========>...................] - ETA: 0s - loss: 25.9118 850/1698 [==============>...............] - ETA: 0s - loss: 25.78571050/1698 [=================>............] - ETA: 0s - loss: 25.76381250/1698 [=====================>........] - ETA: 0s - loss: 25.75241450/1698 [========================>.....] - ETA: 0s - loss: 25.95401650/1698 [============================>.] - ETA: 0s - loss: 25.8494
Epoch 00029: val_loss improved from 23.50288 to 23.48435, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 308us/sample - loss: 25.8704 - val_loss: 23.4843
Epoch 30/50
  50/1698 [..............................] - ETA: 0s - loss: 25.6346 250/1698 [===>..........................] - ETA: 0s - loss: 26.0939 450/1698 [======>.......................] - ETA: 0s - loss: 26.0663 650/1698 [==========>...................] - ETA: 0s - loss: 26.2542 850/1698 [==============>...............] - ETA: 0s - loss: 26.17841050/1698 [=================>............] - ETA: 0s - loss: 25.95061250/1698 [=====================>........] - ETA: 0s - loss: 25.94581450/1698 [========================>.....] - ETA: 0s - loss: 26.03121650/1698 [============================>.] - ETA: 0s - loss: 25.9608
Epoch 00030: val_loss did not improve from 23.48435
1698/1698 [==============================] - 0s 292us/sample - loss: 25.9231 - val_loss: 23.4946
Epoch 31/50
  50/1698 [..............................] - ETA: 0s - loss: 25.5198 250/1698 [===>..........................] - ETA: 0s - loss: 25.5004 450/1698 [======>.......................] - ETA: 0s - loss: 25.6019 650/1698 [==========>...................] - ETA: 0s - loss: 26.1240 850/1698 [==============>...............] - ETA: 0s - loss: 25.99901050/1698 [=================>............] - ETA: 0s - loss: 25.88231250/1698 [=====================>........] - ETA: 0s - loss: 25.86481450/1698 [========================>.....] - ETA: 0s - loss: 25.91131650/1698 [============================>.] - ETA: 0s - loss: 25.9701
Epoch 00031: val_loss improved from 23.48435 to 23.47349, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 310us/sample - loss: 25.9314 - val_loss: 23.4735
Epoch 32/50
  50/1698 [..............................] - ETA: 0s - loss: 28.6000 250/1698 [===>..........................] - ETA: 0s - loss: 27.2854 450/1698 [======>.......................] - ETA: 0s - loss: 26.6902 650/1698 [==========>...................] - ETA: 0s - loss: 26.3361 850/1698 [==============>...............] - ETA: 0s - loss: 26.33461050/1698 [=================>............] - ETA: 0s - loss: 26.11711250/1698 [=====================>........] - ETA: 0s - loss: 26.21871450/1698 [========================>.....] - ETA: 0s - loss: 26.07821650/1698 [============================>.] - ETA: 0s - loss: 26.0916
Epoch 00032: val_loss did not improve from 23.47349
1698/1698 [==============================] - 0s 286us/sample - loss: 26.0490 - val_loss: 23.4969
Epoch 33/50
  50/1698 [..............................] - ETA: 0s - loss: 25.8216 250/1698 [===>..........................] - ETA: 0s - loss: 26.8846 450/1698 [======>.......................] - ETA: 0s - loss: 26.8270 650/1698 [==========>...................] - ETA: 0s - loss: 26.4546 850/1698 [==============>...............] - ETA: 0s - loss: 26.17101050/1698 [=================>............] - ETA: 0s - loss: 26.31371250/1698 [=====================>........] - ETA: 0s - loss: 26.28141450/1698 [========================>.....] - ETA: 0s - loss: 26.09081650/1698 [============================>.] - ETA: 0s - loss: 25.9677
Epoch 00033: val_loss did not improve from 23.47349

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1698/1698 [==============================] - 0s 286us/sample - loss: 25.9905 - val_loss: 23.4757
Epoch 34/50
  50/1698 [..............................] - ETA: 0s - loss: 25.5711 250/1698 [===>..........................] - ETA: 0s - loss: 25.8021 450/1698 [======>.......................] - ETA: 0s - loss: 25.4597 650/1698 [==========>...................] - ETA: 0s - loss: 25.3719 850/1698 [==============>...............] - ETA: 0s - loss: 25.45281050/1698 [=================>............] - ETA: 0s - loss: 25.80621250/1698 [=====================>........] - ETA: 0s - loss: 25.91791450/1698 [========================>.....] - ETA: 0s - loss: 25.89461650/1698 [============================>.] - ETA: 0s - loss: 25.9033
Epoch 00034: val_loss improved from 23.47349 to 23.46539, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 297us/sample - loss: 25.8900 - val_loss: 23.4654
Epoch 35/50
  50/1698 [..............................] - ETA: 0s - loss: 25.7033 250/1698 [===>..........................] - ETA: 0s - loss: 25.3861 450/1698 [======>.......................] - ETA: 0s - loss: 25.7771 650/1698 [==========>...................] - ETA: 0s - loss: 26.0230 850/1698 [==============>...............] - ETA: 0s - loss: 26.19761050/1698 [=================>............] - ETA: 0s - loss: 26.13321250/1698 [=====================>........] - ETA: 0s - loss: 25.95501450/1698 [========================>.....] - ETA: 0s - loss: 25.96361650/1698 [============================>.] - ETA: 0s - loss: 25.9812
Epoch 00035: val_loss improved from 23.46539 to 23.45860, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 304us/sample - loss: 25.9745 - val_loss: 23.4586
Epoch 36/50
  50/1698 [..............................] - ETA: 0s - loss: 24.8431 250/1698 [===>..........................] - ETA: 0s - loss: 25.5077 450/1698 [======>.......................] - ETA: 0s - loss: 25.6521 650/1698 [==========>...................] - ETA: 0s - loss: 25.8264 850/1698 [==============>...............] - ETA: 0s - loss: 25.56441050/1698 [=================>............] - ETA: 0s - loss: 25.69921250/1698 [=====================>........] - ETA: 0s - loss: 25.73111450/1698 [========================>.....] - ETA: 0s - loss: 25.78621650/1698 [============================>.] - ETA: 0s - loss: 25.8253
Epoch 00036: val_loss did not improve from 23.45860
1698/1698 [==============================] - 0s 284us/sample - loss: 25.8290 - val_loss: 23.4586
Epoch 37/50
  50/1698 [..............................] - ETA: 0s - loss: 26.1733 250/1698 [===>..........................] - ETA: 0s - loss: 26.2051 450/1698 [======>.......................] - ETA: 0s - loss: 25.9021 650/1698 [==========>...................] - ETA: 0s - loss: 25.7131 850/1698 [==============>...............] - ETA: 0s - loss: 25.70131050/1698 [=================>............] - ETA: 0s - loss: 25.93201250/1698 [=====================>........] - ETA: 0s - loss: 26.19851450/1698 [========================>.....] - ETA: 0s - loss: 26.07831650/1698 [============================>.] - ETA: 0s - loss: 26.0133
Epoch 00037: val_loss improved from 23.45860 to 23.45751, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 302us/sample - loss: 25.9593 - val_loss: 23.4575
Epoch 38/50
  50/1698 [..............................] - ETA: 0s - loss: 24.8950 250/1698 [===>..........................] - ETA: 0s - loss: 25.6389 450/1698 [======>.......................] - ETA: 0s - loss: 26.3390 650/1698 [==========>...................] - ETA: 0s - loss: 26.0434 850/1698 [==============>...............] - ETA: 0s - loss: 25.97401050/1698 [=================>............] - ETA: 0s - loss: 26.03681250/1698 [=====================>........] - ETA: 0s - loss: 25.96961450/1698 [========================>.....] - ETA: 0s - loss: 25.95661650/1698 [============================>.] - ETA: 0s - loss: 25.9419
Epoch 00038: val_loss improved from 23.45751 to 23.45691, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 308us/sample - loss: 25.9464 - val_loss: 23.4569
Epoch 39/50
  50/1698 [..............................] - ETA: 0s - loss: 24.9815 250/1698 [===>..........................] - ETA: 0s - loss: 26.1620 450/1698 [======>.......................] - ETA: 0s - loss: 26.0768 650/1698 [==========>...................] - ETA: 0s - loss: 26.1697 850/1698 [==============>...............] - ETA: 0s - loss: 26.12531050/1698 [=================>............] - ETA: 0s - loss: 26.02481250/1698 [=====================>........] - ETA: 0s - loss: 25.93891450/1698 [========================>.....] - ETA: 0s - loss: 25.83451650/1698 [============================>.] - ETA: 0s - loss: 25.9230
Epoch 00039: val_loss improved from 23.45691 to 23.45611, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 302us/sample - loss: 25.9096 - val_loss: 23.4561
Epoch 40/50
  50/1698 [..............................] - ETA: 0s - loss: 25.9399 250/1698 [===>..........................] - ETA: 0s - loss: 25.6020 450/1698 [======>.......................] - ETA: 0s - loss: 25.8697 650/1698 [==========>...................] - ETA: 0s - loss: 26.1201 850/1698 [==============>...............] - ETA: 0s - loss: 25.93241050/1698 [=================>............] - ETA: 0s - loss: 26.06211250/1698 [=====================>........] - ETA: 0s - loss: 25.95341450/1698 [========================>.....] - ETA: 0s - loss: 25.82551650/1698 [============================>.] - ETA: 0s - loss: 25.7622
Epoch 00040: val_loss improved from 23.45611 to 23.44808, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 307us/sample - loss: 25.7915 - val_loss: 23.4481
Epoch 41/50
  50/1698 [..............................] - ETA: 0s - loss: 27.0601 250/1698 [===>..........................] - ETA: 0s - loss: 26.1938 450/1698 [======>.......................] - ETA: 0s - loss: 26.0310 650/1698 [==========>...................] - ETA: 0s - loss: 25.7128 850/1698 [==============>...............] - ETA: 0s - loss: 25.66731050/1698 [=================>............] - ETA: 0s - loss: 25.76711250/1698 [=====================>........] - ETA: 0s - loss: 25.85651450/1698 [========================>.....] - ETA: 0s - loss: 25.84951650/1698 [============================>.] - ETA: 0s - loss: 25.8556
Epoch 00041: val_loss did not improve from 23.44808
1698/1698 [==============================] - 0s 284us/sample - loss: 25.8649 - val_loss: 23.4497
Epoch 42/50
  50/1698 [..............................] - ETA: 0s - loss: 27.0461 250/1698 [===>..........................] - ETA: 0s - loss: 25.9491 450/1698 [======>.......................] - ETA: 0s - loss: 25.7254 650/1698 [==========>...................] - ETA: 0s - loss: 25.6977 850/1698 [==============>...............] - ETA: 0s - loss: 25.78251050/1698 [=================>............] - ETA: 0s - loss: 25.98081250/1698 [=====================>........] - ETA: 0s - loss: 25.93361450/1698 [========================>.....] - ETA: 0s - loss: 25.93981650/1698 [============================>.] - ETA: 0s - loss: 25.9316
Epoch 00042: val_loss improved from 23.44808 to 23.44763, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 308us/sample - loss: 25.9166 - val_loss: 23.4476
Epoch 43/50
  50/1698 [..............................] - ETA: 0s - loss: 26.7726 250/1698 [===>..........................] - ETA: 0s - loss: 26.3017 450/1698 [======>.......................] - ETA: 0s - loss: 26.0904 650/1698 [==========>...................] - ETA: 0s - loss: 25.8648 850/1698 [==============>...............] - ETA: 0s - loss: 25.94731050/1698 [=================>............] - ETA: 0s - loss: 26.24491250/1698 [=====================>........] - ETA: 0s - loss: 26.01401450/1698 [========================>.....] - ETA: 0s - loss: 26.01221650/1698 [============================>.] - ETA: 0s - loss: 25.9062
Epoch 00043: val_loss improved from 23.44763 to 23.44499, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 0s 293us/sample - loss: 25.8509 - val_loss: 23.4450
Epoch 44/50
  50/1698 [..............................] - ETA: 0s - loss: 25.2008 250/1698 [===>..........................] - ETA: 0s - loss: 26.4296 450/1698 [======>.......................] - ETA: 0s - loss: 26.0806 650/1698 [==========>...................] - ETA: 0s - loss: 25.8503 850/1698 [==============>...............] - ETA: 0s - loss: 25.54591050/1698 [=================>............] - ETA: 0s - loss: 25.65331250/1698 [=====================>........] - ETA: 0s - loss: 25.67101450/1698 [========================>.....] - ETA: 0s - loss: 25.80071650/1698 [============================>.] - ETA: 0s - loss: 25.9222
Epoch 00044: val_loss improved from 23.44499 to 23.44329, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 306us/sample - loss: 25.8939 - val_loss: 23.4433
Epoch 45/50
  50/1698 [..............................] - ETA: 0s - loss: 24.8810 250/1698 [===>..........................] - ETA: 0s - loss: 25.8806 450/1698 [======>.......................] - ETA: 0s - loss: 25.9273 650/1698 [==========>...................] - ETA: 0s - loss: 26.1322 850/1698 [==============>...............] - ETA: 0s - loss: 26.15211050/1698 [=================>............] - ETA: 0s - loss: 26.13641250/1698 [=====================>........] - ETA: 0s - loss: 26.06951450/1698 [========================>.....] - ETA: 0s - loss: 25.98331650/1698 [============================>.] - ETA: 0s - loss: 25.8774
Epoch 00045: val_loss did not improve from 23.44329
1698/1698 [==============================] - 0s 282us/sample - loss: 25.8372 - val_loss: 23.4433
Epoch 46/50
  50/1698 [..............................] - ETA: 0s - loss: 27.9799 250/1698 [===>..........................] - ETA: 0s - loss: 26.3083 450/1698 [======>.......................] - ETA: 0s - loss: 26.5556 650/1698 [==========>...................] - ETA: 0s - loss: 26.2317 850/1698 [==============>...............] - ETA: 0s - loss: 26.16911050/1698 [=================>............] - ETA: 0s - loss: 26.13901250/1698 [=====================>........] - ETA: 0s - loss: 25.95411450/1698 [========================>.....] - ETA: 0s - loss: 25.91021650/1698 [============================>.] - ETA: 0s - loss: 25.9370
Epoch 00046: val_loss improved from 23.44329 to 23.44063, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 305us/sample - loss: 25.8887 - val_loss: 23.4406
Epoch 47/50
  50/1698 [..............................] - ETA: 0s - loss: 25.3336 250/1698 [===>..........................] - ETA: 0s - loss: 25.3573 450/1698 [======>.......................] - ETA: 0s - loss: 26.0483 650/1698 [==========>...................] - ETA: 0s - loss: 25.9900 850/1698 [==============>...............] - ETA: 0s - loss: 25.99541050/1698 [=================>............] - ETA: 0s - loss: 25.99401250/1698 [=====================>........] - ETA: 0s - loss: 25.86361450/1698 [========================>.....] - ETA: 0s - loss: 25.83551650/1698 [============================>.] - ETA: 0s - loss: 25.8037
Epoch 00047: val_loss improved from 23.44063 to 23.43812, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
1698/1698 [==============================] - 1s 309us/sample - loss: 25.8370 - val_loss: 23.4381
Epoch 48/50
  50/1698 [..............................] - ETA: 0s - loss: 24.4680 250/1698 [===>..........................] - ETA: 0s - loss: 25.8357 450/1698 [======>.......................] - ETA: 0s - loss: 25.5613 650/1698 [==========>...................] - ETA: 0s - loss: 25.6483 850/1698 [==============>...............] - ETA: 0s - loss: 25.76071050/1698 [=================>............] - ETA: 0s - loss: 25.94501250/1698 [=====================>........] - ETA: 0s - loss: 26.07351450/1698 [========================>.....] - ETA: 0s - loss: 25.98771650/1698 [============================>.] - ETA: 0s - loss: 25.9017
Epoch 00048: val_loss did not improve from 23.43812
1698/1698 [==============================] - 0s 285us/sample - loss: 25.9002 - val_loss: 23.4383
Epoch 49/50
  50/1698 [..............................] - ETA: 0s - loss: 25.2595 200/1698 [==>...........................] - ETA: 0s - loss: 26.8968 400/1698 [======>.......................] - ETA: 0s - loss: 26.5739 600/1698 [=========>....................] - ETA: 0s - loss: 26.3349 800/1698 [=============>................] - ETA: 0s - loss: 26.11401000/1698 [================>.............] - ETA: 0s - loss: 26.06911200/1698 [====================>.........] - ETA: 0s - loss: 26.21461400/1698 [=======================>......] - ETA: 0s - loss: 26.13711600/1698 [===========================>..] - ETA: 0s - loss: 26.0094
Epoch 00049: val_loss did not improve from 23.43812

Epoch 00049: ReduceLROnPlateau reducing learning rate to 1e-05.
1698/1698 [==============================] - 1s 308us/sample - loss: 25.9829 - val_loss: 23.4446
Epoch 50/50
  50/1698 [..............................] - ETA: 0s - loss: 24.6259 250/1698 [===>..........................] - ETA: 0s - loss: 25.8385 450/1698 [======>.......................] - ETA: 0s - loss: 25.9621 650/1698 [==========>...................] - ETA: 0s - loss: 26.3552 850/1698 [==============>...............] - ETA: 0s - loss: 26.45161050/1698 [=================>............] - ETA: 0s - loss: 26.39991250/1698 [=====================>........] - ETA: 0s - loss: 26.22481450/1698 [========================>.....] - ETA: 0s - loss: 26.14151650/1698 [============================>.] - ETA: 0s - loss: 26.0932
Epoch 00050: val_loss did not improve from 23.43812
1698/1698 [==============================] - 0s 281us/sample - loss: 26.0368 - val_loss: 23.4383
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 1698 samples, validate on 121 samples
Epoch 1/50
  50/1698 [..............................] - ETA: 3:00 - loss: 143.4550 200/1698 [==>...........................] - ETA: 41s - loss: 154.9519  400/1698 [======>.......................] - ETA: 18s - loss: 153.7096 600/1698 [=========>....................] - ETA: 10s - loss: 151.5919 800/1698 [=============>................] - ETA: 6s - loss: 145.3975 1000/1698 [================>.............] - ETA: 4s - loss: 132.22661200/1698 [====================>.........] - ETA: 2s - loss: 120.03801400/1698 [=======================>......] - ETA: 1s - loss: 110.22041600/1698 [===========================>..] - ETA: 0s - loss: 102.4143
Epoch 00001: val_loss improved from inf to 65.50379, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 6s 4ms/sample - loss: 99.0432 - val_loss: 65.5038
Epoch 2/50
  50/1698 [..............................] - ETA: 0s - loss: 45.2715 250/1698 [===>..........................] - ETA: 0s - loss: 42.7787 450/1698 [======>.......................] - ETA: 0s - loss: 41.4047 650/1698 [==========>...................] - ETA: 0s - loss: 40.6812 850/1698 [==============>...............] - ETA: 0s - loss: 39.87851050/1698 [=================>............] - ETA: 0s - loss: 39.40341250/1698 [=====================>........] - ETA: 0s - loss: 38.67271450/1698 [========================>.....] - ETA: 0s - loss: 38.31861650/1698 [============================>.] - ETA: 0s - loss: 37.7995
Epoch 00002: val_loss did not improve from 65.50379
1698/1698 [==============================] - 0s 283us/sample - loss: 37.7334 - val_loss: 77.0522
Epoch 3/50
  50/1698 [..............................] - ETA: 0s - loss: 33.2829 250/1698 [===>..........................] - ETA: 0s - loss: 33.2985 450/1698 [======>.......................] - ETA: 0s - loss: 33.2465 650/1698 [==========>...................] - ETA: 0s - loss: 32.8388 850/1698 [==============>...............] - ETA: 0s - loss: 32.13291050/1698 [=================>............] - ETA: 0s - loss: 32.07551250/1698 [=====================>........] - ETA: 0s - loss: 31.92811450/1698 [========================>.....] - ETA: 0s - loss: 31.69851650/1698 [============================>.] - ETA: 0s - loss: 31.4436
Epoch 00003: val_loss improved from 65.50379 to 61.70542, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 343us/sample - loss: 31.4444 - val_loss: 61.7054
Epoch 4/50
  50/1698 [..............................] - ETA: 0s - loss: 28.2446 250/1698 [===>..........................] - ETA: 0s - loss: 29.6421 450/1698 [======>.......................] - ETA: 0s - loss: 29.8058 650/1698 [==========>...................] - ETA: 0s - loss: 29.5748 850/1698 [==============>...............] - ETA: 0s - loss: 30.04881050/1698 [=================>............] - ETA: 0s - loss: 30.03921250/1698 [=====================>........] - ETA: 0s - loss: 29.91601450/1698 [========================>.....] - ETA: 0s - loss: 29.74591650/1698 [============================>.] - ETA: 0s - loss: 29.5623
Epoch 00004: val_loss improved from 61.70542 to 51.06059, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 299us/sample - loss: 29.5134 - val_loss: 51.0606
Epoch 5/50
  50/1698 [..............................] - ETA: 0s - loss: 25.1700 250/1698 [===>..........................] - ETA: 0s - loss: 28.1912 450/1698 [======>.......................] - ETA: 0s - loss: 28.7580 650/1698 [==========>...................] - ETA: 0s - loss: 28.6446 850/1698 [==============>...............] - ETA: 0s - loss: 28.63251050/1698 [=================>............] - ETA: 0s - loss: 28.52021250/1698 [=====================>........] - ETA: 0s - loss: 28.56551450/1698 [========================>.....] - ETA: 0s - loss: 28.71301650/1698 [============================>.] - ETA: 0s - loss: 28.8084
Epoch 00005: val_loss improved from 51.06059 to 40.38482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 310us/sample - loss: 28.7459 - val_loss: 40.3848
Epoch 6/50
  50/1698 [..............................] - ETA: 0s - loss: 30.1344 250/1698 [===>..........................] - ETA: 0s - loss: 28.3133 450/1698 [======>.......................] - ETA: 0s - loss: 28.0413 650/1698 [==========>...................] - ETA: 0s - loss: 27.8998 850/1698 [==============>...............] - ETA: 0s - loss: 27.70651050/1698 [=================>............] - ETA: 0s - loss: 27.62801250/1698 [=====================>........] - ETA: 0s - loss: 27.77211450/1698 [========================>.....] - ETA: 0s - loss: 27.96011650/1698 [============================>.] - ETA: 0s - loss: 27.8525
Epoch 00006: val_loss improved from 40.38482 to 34.18723, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 301us/sample - loss: 27.8160 - val_loss: 34.1872
Epoch 7/50
  50/1698 [..............................] - ETA: 0s - loss: 28.3796 250/1698 [===>..........................] - ETA: 0s - loss: 29.1881 450/1698 [======>.......................] - ETA: 0s - loss: 28.3011 650/1698 [==========>...................] - ETA: 0s - loss: 28.3478 850/1698 [==============>...............] - ETA: 0s - loss: 28.10641050/1698 [=================>............] - ETA: 0s - loss: 27.89911250/1698 [=====================>........] - ETA: 0s - loss: 27.97501450/1698 [========================>.....] - ETA: 0s - loss: 28.00711650/1698 [============================>.] - ETA: 0s - loss: 27.8620
Epoch 00007: val_loss improved from 34.18723 to 29.10271, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 305us/sample - loss: 27.8225 - val_loss: 29.1027
Epoch 8/50
  50/1698 [..............................] - ETA: 0s - loss: 26.8979 250/1698 [===>..........................] - ETA: 0s - loss: 27.9667 450/1698 [======>.......................] - ETA: 0s - loss: 27.5859 650/1698 [==========>...................] - ETA: 0s - loss: 27.4096 850/1698 [==============>...............] - ETA: 0s - loss: 27.09761050/1698 [=================>............] - ETA: 0s - loss: 27.22431250/1698 [=====================>........] - ETA: 0s - loss: 27.22481450/1698 [========================>.....] - ETA: 0s - loss: 27.25841650/1698 [============================>.] - ETA: 0s - loss: 27.2696
Epoch 00008: val_loss improved from 29.10271 to 25.89538, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 317us/sample - loss: 27.2180 - val_loss: 25.8954
Epoch 9/50
  50/1698 [..............................] - ETA: 0s - loss: 26.9356 250/1698 [===>..........................] - ETA: 0s - loss: 26.8683 450/1698 [======>.......................] - ETA: 0s - loss: 26.5687 650/1698 [==========>...................] - ETA: 0s - loss: 26.6720 850/1698 [==============>...............] - ETA: 0s - loss: 26.85071050/1698 [=================>............] - ETA: 0s - loss: 26.97781250/1698 [=====================>........] - ETA: 0s - loss: 26.92021450/1698 [========================>.....] - ETA: 0s - loss: 27.07921650/1698 [============================>.] - ETA: 0s - loss: 26.8701
Epoch 00009: val_loss did not improve from 25.89538
1698/1698 [==============================] - 0s 285us/sample - loss: 26.8675 - val_loss: 26.1248
Epoch 10/50
  50/1698 [..............................] - ETA: 0s - loss: 26.3937 250/1698 [===>..........................] - ETA: 0s - loss: 26.9850 450/1698 [======>.......................] - ETA: 0s - loss: 27.0536 650/1698 [==========>...................] - ETA: 0s - loss: 27.3107 850/1698 [==============>...............] - ETA: 0s - loss: 26.77691050/1698 [=================>............] - ETA: 0s - loss: 26.87651250/1698 [=====================>........] - ETA: 0s - loss: 26.84221450/1698 [========================>.....] - ETA: 0s - loss: 26.80501650/1698 [============================>.] - ETA: 0s - loss: 26.9048
Epoch 00010: val_loss improved from 25.89538 to 25.78642, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 308us/sample - loss: 26.8903 - val_loss: 25.7864
Epoch 11/50
  50/1698 [..............................] - ETA: 0s - loss: 25.9907 250/1698 [===>..........................] - ETA: 0s - loss: 26.6723 450/1698 [======>.......................] - ETA: 0s - loss: 26.5916 650/1698 [==========>...................] - ETA: 0s - loss: 26.7833 850/1698 [==============>...............] - ETA: 0s - loss: 26.96581050/1698 [=================>............] - ETA: 0s - loss: 26.85601250/1698 [=====================>........] - ETA: 0s - loss: 26.91261450/1698 [========================>.....] - ETA: 0s - loss: 26.88711650/1698 [============================>.] - ETA: 0s - loss: 26.8197
Epoch 00011: val_loss did not improve from 25.78642
1698/1698 [==============================] - 0s 289us/sample - loss: 26.8050 - val_loss: 25.8803
Epoch 12/50
  50/1698 [..............................] - ETA: 0s - loss: 27.7393 250/1698 [===>..........................] - ETA: 0s - loss: 26.2489 450/1698 [======>.......................] - ETA: 0s - loss: 26.8366 650/1698 [==========>...................] - ETA: 0s - loss: 27.1320 850/1698 [==============>...............] - ETA: 0s - loss: 26.92761050/1698 [=================>............] - ETA: 0s - loss: 26.99381250/1698 [=====================>........] - ETA: 0s - loss: 26.98081450/1698 [========================>.....] - ETA: 0s - loss: 26.85141650/1698 [============================>.] - ETA: 0s - loss: 26.8359
Epoch 00012: val_loss improved from 25.78642 to 24.35896, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 311us/sample - loss: 26.7887 - val_loss: 24.3590
Epoch 13/50
  50/1698 [..............................] - ETA: 0s - loss: 27.2000 250/1698 [===>..........................] - ETA: 0s - loss: 26.8447 450/1698 [======>.......................] - ETA: 0s - loss: 26.1052 650/1698 [==========>...................] - ETA: 0s - loss: 26.3763 850/1698 [==============>...............] - ETA: 0s - loss: 26.51351050/1698 [=================>............] - ETA: 0s - loss: 26.74661250/1698 [=====================>........] - ETA: 0s - loss: 26.92711450/1698 [========================>.....] - ETA: 0s - loss: 26.93371650/1698 [============================>.] - ETA: 0s - loss: 26.9251
Epoch 00013: val_loss improved from 24.35896 to 24.21727, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 305us/sample - loss: 26.9606 - val_loss: 24.2173
Epoch 14/50
  50/1698 [..............................] - ETA: 0s - loss: 26.9794 250/1698 [===>..........................] - ETA: 0s - loss: 27.0392 450/1698 [======>.......................] - ETA: 0s - loss: 26.1672 650/1698 [==========>...................] - ETA: 0s - loss: 26.2640 850/1698 [==============>...............] - ETA: 0s - loss: 26.10221050/1698 [=================>............] - ETA: 0s - loss: 26.69921250/1698 [=====================>........] - ETA: 0s - loss: 26.89901450/1698 [========================>.....] - ETA: 0s - loss: 26.98451650/1698 [============================>.] - ETA: 0s - loss: 26.8810
Epoch 00014: val_loss improved from 24.21727 to 23.51500, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 304us/sample - loss: 26.8476 - val_loss: 23.5150
Epoch 15/50
  50/1698 [..............................] - ETA: 0s - loss: 24.0861 250/1698 [===>..........................] - ETA: 0s - loss: 25.9816 450/1698 [======>.......................] - ETA: 0s - loss: 26.0527 650/1698 [==========>...................] - ETA: 0s - loss: 26.0348 850/1698 [==============>...............] - ETA: 0s - loss: 26.12821050/1698 [=================>............] - ETA: 0s - loss: 25.98851250/1698 [=====================>........] - ETA: 0s - loss: 26.20341450/1698 [========================>.....] - ETA: 0s - loss: 26.30941650/1698 [============================>.] - ETA: 0s - loss: 26.2684
Epoch 00015: val_loss did not improve from 23.51500
1698/1698 [==============================] - 0s 283us/sample - loss: 26.2259 - val_loss: 25.8250
Epoch 16/50
  50/1698 [..............................] - ETA: 0s - loss: 30.0062 250/1698 [===>..........................] - ETA: 0s - loss: 26.6045 450/1698 [======>.......................] - ETA: 0s - loss: 26.1433 650/1698 [==========>...................] - ETA: 0s - loss: 26.2925 850/1698 [==============>...............] - ETA: 0s - loss: 26.33351050/1698 [=================>............] - ETA: 0s - loss: 26.31831250/1698 [=====================>........] - ETA: 0s - loss: 26.20781450/1698 [========================>.....] - ETA: 0s - loss: 26.26201650/1698 [============================>.] - ETA: 0s - loss: 26.2092
Epoch 00016: val_loss did not improve from 23.51500

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1698/1698 [==============================] - 0s 285us/sample - loss: 26.1529 - val_loss: 24.3144
Epoch 17/50
  50/1698 [..............................] - ETA: 0s - loss: 26.9138 250/1698 [===>..........................] - ETA: 0s - loss: 26.8456 450/1698 [======>.......................] - ETA: 0s - loss: 26.5118 650/1698 [==========>...................] - ETA: 0s - loss: 26.2364 850/1698 [==============>...............] - ETA: 0s - loss: 26.12121050/1698 [=================>............] - ETA: 0s - loss: 26.44941250/1698 [=====================>........] - ETA: 0s - loss: 26.50981450/1698 [========================>.....] - ETA: 0s - loss: 26.31631650/1698 [============================>.] - ETA: 0s - loss: 26.2301
Epoch 00017: val_loss improved from 23.51500 to 23.48587, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 303us/sample - loss: 26.1906 - val_loss: 23.4859
Epoch 18/50
  50/1698 [..............................] - ETA: 0s - loss: 28.2585 250/1698 [===>..........................] - ETA: 0s - loss: 26.0697 450/1698 [======>.......................] - ETA: 0s - loss: 25.9686 650/1698 [==========>...................] - ETA: 0s - loss: 25.8378 850/1698 [==============>...............] - ETA: 0s - loss: 26.17441050/1698 [=================>............] - ETA: 0s - loss: 26.33421250/1698 [=====================>........] - ETA: 0s - loss: 26.36541450/1698 [========================>.....] - ETA: 0s - loss: 26.36831650/1698 [============================>.] - ETA: 0s - loss: 26.1889
Epoch 00018: val_loss improved from 23.48587 to 23.08283, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 305us/sample - loss: 26.1290 - val_loss: 23.0828
Epoch 19/50
  50/1698 [..............................] - ETA: 0s - loss: 28.6501 250/1698 [===>..........................] - ETA: 0s - loss: 26.1675 450/1698 [======>.......................] - ETA: 0s - loss: 26.1877 650/1698 [==========>...................] - ETA: 0s - loss: 26.1494 850/1698 [==============>...............] - ETA: 0s - loss: 26.40991050/1698 [=================>............] - ETA: 0s - loss: 26.20531250/1698 [=====================>........] - ETA: 0s - loss: 26.26331450/1698 [========================>.....] - ETA: 0s - loss: 26.24251650/1698 [============================>.] - ETA: 0s - loss: 26.1237
Epoch 00019: val_loss improved from 23.08283 to 22.90177, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 309us/sample - loss: 26.0932 - val_loss: 22.9018
Epoch 20/50
  50/1698 [..............................] - ETA: 0s - loss: 25.6106 250/1698 [===>..........................] - ETA: 0s - loss: 27.1789 450/1698 [======>.......................] - ETA: 0s - loss: 26.4207 650/1698 [==========>...................] - ETA: 0s - loss: 26.6624 850/1698 [==============>...............] - ETA: 0s - loss: 26.52031050/1698 [=================>............] - ETA: 0s - loss: 26.18051250/1698 [=====================>........] - ETA: 0s - loss: 25.99591450/1698 [========================>.....] - ETA: 0s - loss: 26.00161650/1698 [============================>.] - ETA: 0s - loss: 26.0399
Epoch 00020: val_loss did not improve from 22.90177
1698/1698 [==============================] - 0s 282us/sample - loss: 26.1065 - val_loss: 23.1476
Epoch 21/50
  50/1698 [..............................] - ETA: 0s - loss: 25.3714 250/1698 [===>..........................] - ETA: 0s - loss: 26.5051 450/1698 [======>.......................] - ETA: 0s - loss: 26.1521 650/1698 [==========>...................] - ETA: 0s - loss: 25.7150 850/1698 [==============>...............] - ETA: 0s - loss: 25.85541050/1698 [=================>............] - ETA: 0s - loss: 25.83131250/1698 [=====================>........] - ETA: 0s - loss: 25.79661450/1698 [========================>.....] - ETA: 0s - loss: 25.91681650/1698 [============================>.] - ETA: 0s - loss: 25.8190
Epoch 00021: val_loss improved from 22.90177 to 22.88201, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 305us/sample - loss: 25.8392 - val_loss: 22.8820
Epoch 22/50
  50/1698 [..............................] - ETA: 0s - loss: 24.7027 250/1698 [===>..........................] - ETA: 0s - loss: 25.5580 450/1698 [======>.......................] - ETA: 0s - loss: 25.5872 650/1698 [==========>...................] - ETA: 0s - loss: 25.3530 850/1698 [==============>...............] - ETA: 0s - loss: 25.47201050/1698 [=================>............] - ETA: 0s - loss: 25.57311250/1698 [=====================>........] - ETA: 0s - loss: 25.62921450/1698 [========================>.....] - ETA: 0s - loss: 25.75711650/1698 [============================>.] - ETA: 0s - loss: 25.7768
Epoch 00022: val_loss improved from 22.88201 to 22.79454, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 303us/sample - loss: 25.8148 - val_loss: 22.7945
Epoch 23/50
  50/1698 [..............................] - ETA: 0s - loss: 24.5113 250/1698 [===>..........................] - ETA: 0s - loss: 26.4746 450/1698 [======>.......................] - ETA: 0s - loss: 26.3878 650/1698 [==========>...................] - ETA: 0s - loss: 26.0309 850/1698 [==============>...............] - ETA: 0s - loss: 26.33511050/1698 [=================>............] - ETA: 0s - loss: 26.07691250/1698 [=====================>........] - ETA: 0s - loss: 26.03231450/1698 [========================>.....] - ETA: 0s - loss: 25.95051650/1698 [============================>.] - ETA: 0s - loss: 25.9385
Epoch 00023: val_loss did not improve from 22.79454
1698/1698 [==============================] - 0s 286us/sample - loss: 25.9217 - val_loss: 22.8499
Epoch 24/50
  50/1698 [..............................] - ETA: 0s - loss: 24.9543 250/1698 [===>..........................] - ETA: 0s - loss: 25.9943 450/1698 [======>.......................] - ETA: 0s - loss: 25.8361 650/1698 [==========>...................] - ETA: 0s - loss: 26.0884 850/1698 [==============>...............] - ETA: 0s - loss: 26.00621050/1698 [=================>............] - ETA: 0s - loss: 25.99311250/1698 [=====================>........] - ETA: 0s - loss: 26.03581450/1698 [========================>.....] - ETA: 0s - loss: 25.93251650/1698 [============================>.] - ETA: 0s - loss: 26.0008
Epoch 00024: val_loss did not improve from 22.79454

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1698/1698 [==============================] - 0s 273us/sample - loss: 26.0025 - val_loss: 22.9085
Epoch 25/50
  50/1698 [..............................] - ETA: 0s - loss: 26.2513 250/1698 [===>..........................] - ETA: 0s - loss: 26.1824 450/1698 [======>.......................] - ETA: 0s - loss: 25.8929 650/1698 [==========>...................] - ETA: 0s - loss: 25.8087 750/1698 [============>.................] - ETA: 0s - loss: 25.6610 900/1698 [==============>...............] - ETA: 0s - loss: 25.49671100/1698 [==================>...........] - ETA: 0s - loss: 25.69421300/1698 [=====================>........] - ETA: 0s - loss: 25.64721500/1698 [=========================>....] - ETA: 0s - loss: 25.6889
Epoch 00025: val_loss improved from 22.79454 to 22.71846, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 326us/sample - loss: 25.6727 - val_loss: 22.7185
Epoch 26/50
  50/1698 [..............................] - ETA: 0s - loss: 25.0680 250/1698 [===>..........................] - ETA: 0s - loss: 25.7995 450/1698 [======>.......................] - ETA: 0s - loss: 25.4336 650/1698 [==========>...................] - ETA: 0s - loss: 25.5237 850/1698 [==============>...............] - ETA: 0s - loss: 25.56211050/1698 [=================>............] - ETA: 0s - loss: 25.30411250/1698 [=====================>........] - ETA: 0s - loss: 25.34831450/1698 [========================>.....] - ETA: 0s - loss: 25.46461650/1698 [============================>.] - ETA: 0s - loss: 25.6531
Epoch 00026: val_loss improved from 22.71846 to 22.70998, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 313us/sample - loss: 25.6799 - val_loss: 22.7100
Epoch 27/50
  50/1698 [..............................] - ETA: 0s - loss: 25.2505 250/1698 [===>..........................] - ETA: 0s - loss: 25.5094 450/1698 [======>.......................] - ETA: 0s - loss: 25.9531 650/1698 [==========>...................] - ETA: 0s - loss: 25.8592 850/1698 [==============>...............] - ETA: 0s - loss: 25.89741050/1698 [=================>............] - ETA: 0s - loss: 25.87131250/1698 [=====================>........] - ETA: 0s - loss: 25.73011450/1698 [========================>.....] - ETA: 0s - loss: 25.61851650/1698 [============================>.] - ETA: 0s - loss: 25.6480
Epoch 00027: val_loss did not improve from 22.70998
1698/1698 [==============================] - 1s 297us/sample - loss: 25.6085 - val_loss: 22.7729
Epoch 28/50
  50/1698 [..............................] - ETA: 0s - loss: 25.3680 250/1698 [===>..........................] - ETA: 0s - loss: 25.7281 450/1698 [======>.......................] - ETA: 0s - loss: 25.8436 650/1698 [==========>...................] - ETA: 0s - loss: 26.1227 850/1698 [==============>...............] - ETA: 0s - loss: 26.04371050/1698 [=================>............] - ETA: 0s - loss: 26.06111250/1698 [=====================>........] - ETA: 0s - loss: 25.99741450/1698 [========================>.....] - ETA: 0s - loss: 25.72311650/1698 [============================>.] - ETA: 0s - loss: 25.6033
Epoch 00028: val_loss did not improve from 22.70998

Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1698/1698 [==============================] - 0s 290us/sample - loss: 25.6015 - val_loss: 23.5007
Epoch 29/50
  50/1698 [..............................] - ETA: 0s - loss: 24.3157 250/1698 [===>..........................] - ETA: 0s - loss: 25.6186 450/1698 [======>.......................] - ETA: 0s - loss: 25.3532 650/1698 [==========>...................] - ETA: 0s - loss: 25.0628 850/1698 [==============>...............] - ETA: 0s - loss: 25.45231050/1698 [=================>............] - ETA: 0s - loss: 25.37081250/1698 [=====================>........] - ETA: 0s - loss: 25.47881450/1698 [========================>.....] - ETA: 0s - loss: 25.53501650/1698 [============================>.] - ETA: 0s - loss: 25.6315
Epoch 00029: val_loss improved from 22.70998 to 22.68233, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
1698/1698 [==============================] - 1s 303us/sample - loss: 25.6831 - val_loss: 22.6823
Epoch 30/50
  50/1698 [..............................] - ETA: 0s - loss: 26.8683 250/1698 [===>..........................] - ETA: 0s - loss: 25.9206 450/1698 [======>.......................] - ETA: 0s - loss: 25.6968 600/1698 [=========>....................] - ETA: 0s - loss: 25.9778 800/1698 [=============>................] - ETA: 0s - loss: 25.74951000/1698 [================>.............] - ETA: 0s - loss: 25.70521200/1698 [====================>.........] - ETA: 0s - loss: 25.83691400/1698 [=======================>......] - ETA: 0s - loss: 25.86781600/1698 [===========================>..] - ETA: 0s - loss: 25.7386
Epoch 00030: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 291us/sample - loss: 25.6340 - val_loss: 22.7519
Epoch 31/50
  50/1698 [..............................] - ETA: 0s - loss: 26.9993 250/1698 [===>..........................] - ETA: 0s - loss: 25.6064 450/1698 [======>.......................] - ETA: 0s - loss: 25.6497 650/1698 [==========>...................] - ETA: 0s - loss: 25.5775 850/1698 [==============>...............] - ETA: 0s - loss: 25.63801050/1698 [=================>............] - ETA: 0s - loss: 25.78651250/1698 [=====================>........] - ETA: 0s - loss: 25.68291450/1698 [========================>.....] - ETA: 0s - loss: 25.69521650/1698 [============================>.] - ETA: 0s - loss: 25.6616
Epoch 00031: val_loss did not improve from 22.68233

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1698/1698 [==============================] - 0s 286us/sample - loss: 25.6294 - val_loss: 22.6835
Epoch 32/50
  50/1698 [..............................] - ETA: 0s - loss: 25.6035 250/1698 [===>..........................] - ETA: 0s - loss: 25.7115 450/1698 [======>.......................] - ETA: 0s - loss: 26.0021 650/1698 [==========>...................] - ETA: 0s - loss: 25.5045 850/1698 [==============>...............] - ETA: 0s - loss: 25.28031050/1698 [=================>............] - ETA: 0s - loss: 25.37661250/1698 [=====================>........] - ETA: 0s - loss: 25.44191450/1698 [========================>.....] - ETA: 0s - loss: 25.49691650/1698 [============================>.] - ETA: 0s - loss: 25.5624
Epoch 00032: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 284us/sample - loss: 25.5478 - val_loss: 22.7120
Epoch 33/50
  50/1698 [..............................] - ETA: 0s - loss: 23.7940 250/1698 [===>..........................] - ETA: 0s - loss: 24.9188 450/1698 [======>.......................] - ETA: 0s - loss: 24.9648 650/1698 [==========>...................] - ETA: 0s - loss: 25.3163 850/1698 [==============>...............] - ETA: 0s - loss: 25.46591050/1698 [=================>............] - ETA: 0s - loss: 25.47941250/1698 [=====================>........] - ETA: 0s - loss: 25.55031450/1698 [========================>.....] - ETA: 0s - loss: 25.52681650/1698 [============================>.] - ETA: 0s - loss: 25.6133
Epoch 00033: val_loss did not improve from 22.68233

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1698/1698 [==============================] - 0s 285us/sample - loss: 25.6121 - val_loss: 22.6982
Epoch 34/50
  50/1698 [..............................] - ETA: 0s - loss: 25.4545 250/1698 [===>..........................] - ETA: 0s - loss: 24.8591 450/1698 [======>.......................] - ETA: 0s - loss: 25.1393 650/1698 [==========>...................] - ETA: 0s - loss: 26.2056 850/1698 [==============>...............] - ETA: 0s - loss: 25.90831050/1698 [=================>............] - ETA: 0s - loss: 25.79521250/1698 [=====================>........] - ETA: 0s - loss: 25.82461450/1698 [========================>.....] - ETA: 0s - loss: 25.74811650/1698 [============================>.] - ETA: 0s - loss: 25.6543
Epoch 00034: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 282us/sample - loss: 25.6524 - val_loss: 22.7057
Epoch 35/50
  50/1698 [..............................] - ETA: 0s - loss: 25.5896 250/1698 [===>..........................] - ETA: 0s - loss: 26.0016 450/1698 [======>.......................] - ETA: 0s - loss: 25.7648 650/1698 [==========>...................] - ETA: 0s - loss: 25.6080 850/1698 [==============>...............] - ETA: 0s - loss: 25.77871050/1698 [=================>............] - ETA: 0s - loss: 25.69871250/1698 [=====================>........] - ETA: 0s - loss: 25.64641450/1698 [========================>.....] - ETA: 0s - loss: 25.62291650/1698 [============================>.] - ETA: 0s - loss: 25.5943
Epoch 00035: val_loss did not improve from 22.68233

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-05.
1698/1698 [==============================] - 0s 286us/sample - loss: 25.6584 - val_loss: 22.7214
Epoch 36/50
  50/1698 [..............................] - ETA: 0s - loss: 25.3681 250/1698 [===>..........................] - ETA: 0s - loss: 25.2847 450/1698 [======>.......................] - ETA: 0s - loss: 25.6777 650/1698 [==========>...................] - ETA: 0s - loss: 25.9427 850/1698 [==============>...............] - ETA: 0s - loss: 25.99161050/1698 [=================>............] - ETA: 0s - loss: 25.90701250/1698 [=====================>........] - ETA: 0s - loss: 25.73961450/1698 [========================>.....] - ETA: 0s - loss: 25.70961650/1698 [============================>.] - ETA: 0s - loss: 25.6259
Epoch 00036: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 285us/sample - loss: 25.5888 - val_loss: 22.7041
Epoch 37/50
  50/1698 [..............................] - ETA: 0s - loss: 25.9976 250/1698 [===>..........................] - ETA: 0s - loss: 25.8502 450/1698 [======>.......................] - ETA: 0s - loss: 25.6592 650/1698 [==========>...................] - ETA: 0s - loss: 25.5123 850/1698 [==============>...............] - ETA: 0s - loss: 25.59271050/1698 [=================>............] - ETA: 0s - loss: 25.42381250/1698 [=====================>........] - ETA: 0s - loss: 25.53301450/1698 [========================>.....] - ETA: 0s - loss: 25.52581650/1698 [============================>.] - ETA: 0s - loss: 25.6443
Epoch 00037: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 291us/sample - loss: 25.6450 - val_loss: 22.7190
Epoch 38/50
  50/1698 [..............................] - ETA: 0s - loss: 27.0346 250/1698 [===>..........................] - ETA: 0s - loss: 25.7670 450/1698 [======>.......................] - ETA: 0s - loss: 25.6985 650/1698 [==========>...................] - ETA: 0s - loss: 25.4481 850/1698 [==============>...............] - ETA: 0s - loss: 25.36631050/1698 [=================>............] - ETA: 0s - loss: 25.50241250/1698 [=====================>........] - ETA: 0s - loss: 25.49691450/1698 [========================>.....] - ETA: 0s - loss: 25.44731650/1698 [============================>.] - ETA: 0s - loss: 25.4905
Epoch 00038: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 285us/sample - loss: 25.4599 - val_loss: 22.6867
Epoch 39/50
  50/1698 [..............................] - ETA: 0s - loss: 25.9340 250/1698 [===>..........................] - ETA: 0s - loss: 25.6804 450/1698 [======>.......................] - ETA: 0s - loss: 25.5371 650/1698 [==========>...................] - ETA: 0s - loss: 25.4589 850/1698 [==============>...............] - ETA: 0s - loss: 25.44641050/1698 [=================>............] - ETA: 0s - loss: 25.53991250/1698 [=====================>........] - ETA: 0s - loss: 25.59741450/1698 [========================>.....] - ETA: 0s - loss: 25.51541650/1698 [============================>.] - ETA: 0s - loss: 25.5249
Epoch 00039: val_loss did not improve from 22.68233
1698/1698 [==============================] - 0s 286us/sample - loss: 25.5892 - val_loss: 22.7094
Epoch 00039: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 1819 samples, validate on 121 samples
Epoch 1/50
  50/1819 [..............................] - ETA: 52s - loss: 161.3468 200/1819 [==>...........................] - ETA: 12s - loss: 158.0564 400/1819 [=====>........................] - ETA: 5s - loss: 156.6031  600/1819 [========>.....................] - ETA: 3s - loss: 151.3850 800/1819 [============>.................] - ETA: 2s - loss: 145.19001000/1819 [===============>..............] - ETA: 1s - loss: 132.22821200/1819 [==================>...........] - ETA: 0s - loss: 120.55421400/1819 [======================>.......] - ETA: 0s - loss: 110.55581600/1819 [=========================>....] - ETA: 0s - loss: 103.08331800/1819 [============================>.] - ETA: 0s - loss: 96.6125 
Epoch 00001: val_loss improved from inf to 52.11225, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 3s 1ms/sample - loss: 96.0542 - val_loss: 52.1123
Epoch 2/50
  50/1819 [..............................] - ETA: 0s - loss: 42.7450 250/1819 [===>..........................] - ETA: 0s - loss: 41.2929 450/1819 [======>.......................] - ETA: 0s - loss: 39.7339 650/1819 [=========>....................] - ETA: 0s - loss: 39.2699 850/1819 [=============>................] - ETA: 0s - loss: 38.64321050/1819 [================>.............] - ETA: 0s - loss: 37.93391250/1819 [===================>..........] - ETA: 0s - loss: 37.41491450/1819 [======================>.......] - ETA: 0s - loss: 36.83921650/1819 [==========================>...] - ETA: 0s - loss: 36.2897
Epoch 00002: val_loss did not improve from 52.11225
1819/1819 [==============================] - 1s 284us/sample - loss: 35.8661 - val_loss: 65.8562
Epoch 3/50
  50/1819 [..............................] - ETA: 0s - loss: 32.3905 250/1819 [===>..........................] - ETA: 0s - loss: 32.2499 450/1819 [======>.......................] - ETA: 0s - loss: 31.5607 650/1819 [=========>....................] - ETA: 0s - loss: 30.9897 850/1819 [=============>................] - ETA: 0s - loss: 30.82011050/1819 [================>.............] - ETA: 0s - loss: 30.71181250/1819 [===================>..........] - ETA: 0s - loss: 30.53341450/1819 [======================>.......] - ETA: 0s - loss: 30.34811650/1819 [==========================>...] - ETA: 0s - loss: 30.2217
Epoch 00003: val_loss did not improve from 52.11225

Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
1819/1819 [==============================] - 1s 282us/sample - loss: 30.0098 - val_loss: 60.9354
Epoch 4/50
  50/1819 [..............................] - ETA: 0s - loss: 29.8758 250/1819 [===>..........................] - ETA: 0s - loss: 29.7738 450/1819 [======>.......................] - ETA: 0s - loss: 29.5359 650/1819 [=========>....................] - ETA: 0s - loss: 29.4026 850/1819 [=============>................] - ETA: 0s - loss: 29.58821050/1819 [================>.............] - ETA: 0s - loss: 29.30421200/1819 [==================>...........] - ETA: 0s - loss: 29.11351400/1819 [======================>.......] - ETA: 0s - loss: 29.01071600/1819 [=========================>....] - ETA: 0s - loss: 28.75151800/1819 [============================>.] - ETA: 0s - loss: 28.5454
Epoch 00004: val_loss improved from 52.11225 to 45.28710, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 315us/sample - loss: 28.5168 - val_loss: 45.2871
Epoch 5/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6962 250/1819 [===>..........................] - ETA: 0s - loss: 27.5732 450/1819 [======>.......................] - ETA: 0s - loss: 28.0641 650/1819 [=========>....................] - ETA: 0s - loss: 27.9329 850/1819 [=============>................] - ETA: 0s - loss: 27.87631050/1819 [================>.............] - ETA: 0s - loss: 27.87171250/1819 [===================>..........] - ETA: 0s - loss: 27.90121450/1819 [======================>.......] - ETA: 0s - loss: 27.73161650/1819 [==========================>...] - ETA: 0s - loss: 27.9139
Epoch 00005: val_loss improved from 45.28710 to 34.24141, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 302us/sample - loss: 27.9576 - val_loss: 34.2414
Epoch 6/50
  50/1819 [..............................] - ETA: 0s - loss: 27.1269 250/1819 [===>..........................] - ETA: 0s - loss: 27.8502 450/1819 [======>.......................] - ETA: 0s - loss: 27.8951 650/1819 [=========>....................] - ETA: 0s - loss: 27.5452 850/1819 [=============>................] - ETA: 0s - loss: 27.83181050/1819 [================>.............] - ETA: 0s - loss: 27.76471250/1819 [===================>..........] - ETA: 0s - loss: 27.61631450/1819 [======================>.......] - ETA: 0s - loss: 27.73391650/1819 [==========================>...] - ETA: 0s - loss: 27.5965
Epoch 00006: val_loss did not improve from 34.24141
1819/1819 [==============================] - 1s 286us/sample - loss: 27.4397 - val_loss: 34.8168
Epoch 7/50
  50/1819 [..............................] - ETA: 0s - loss: 29.1402 250/1819 [===>..........................] - ETA: 0s - loss: 27.4699 300/1819 [===>..........................] - ETA: 0s - loss: 27.2748 500/1819 [=======>......................] - ETA: 0s - loss: 27.4182 700/1819 [==========>...................] - ETA: 0s - loss: 27.3876 900/1819 [=============>................] - ETA: 0s - loss: 27.32351100/1819 [=================>............] - ETA: 0s - loss: 27.01731300/1819 [====================>.........] - ETA: 0s - loss: 27.26861500/1819 [=======================>......] - ETA: 0s - loss: 27.33391700/1819 [===========================>..] - ETA: 0s - loss: 27.3031
Epoch 00007: val_loss improved from 34.24141 to 28.82933, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 339us/sample - loss: 27.3062 - val_loss: 28.8293
Epoch 8/50
  50/1819 [..............................] - ETA: 0s - loss: 26.6653 250/1819 [===>..........................] - ETA: 0s - loss: 26.1968 450/1819 [======>.......................] - ETA: 0s - loss: 26.6071 650/1819 [=========>....................] - ETA: 0s - loss: 26.7609 850/1819 [=============>................] - ETA: 0s - loss: 26.93121050/1819 [================>.............] - ETA: 0s - loss: 26.98561250/1819 [===================>..........] - ETA: 0s - loss: 27.05531450/1819 [======================>.......] - ETA: 0s - loss: 26.98561650/1819 [==========================>...] - ETA: 0s - loss: 26.9719
Epoch 00008: val_loss improved from 28.82933 to 26.11102, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 304us/sample - loss: 26.9303 - val_loss: 26.1110
Epoch 9/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4810 250/1819 [===>..........................] - ETA: 0s - loss: 26.2006 450/1819 [======>.......................] - ETA: 0s - loss: 27.0008 650/1819 [=========>....................] - ETA: 0s - loss: 27.1922 850/1819 [=============>................] - ETA: 0s - loss: 27.31901050/1819 [================>.............] - ETA: 0s - loss: 27.19881250/1819 [===================>..........] - ETA: 0s - loss: 27.12941450/1819 [======================>.......] - ETA: 0s - loss: 27.10401650/1819 [==========================>...] - ETA: 0s - loss: 27.03611800/1819 [============================>.] - ETA: 0s - loss: 26.8979
Epoch 00009: val_loss improved from 26.11102 to 25.44271, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 312us/sample - loss: 26.8996 - val_loss: 25.4427
Epoch 10/50
  50/1819 [..............................] - ETA: 0s - loss: 25.9814 250/1819 [===>..........................] - ETA: 0s - loss: 27.6397 450/1819 [======>.......................] - ETA: 0s - loss: 26.8211 650/1819 [=========>....................] - ETA: 0s - loss: 26.8087 850/1819 [=============>................] - ETA: 0s - loss: 26.68231050/1819 [================>.............] - ETA: 0s - loss: 26.39931250/1819 [===================>..........] - ETA: 0s - loss: 26.52701450/1819 [======================>.......] - ETA: 0s - loss: 26.67951650/1819 [==========================>...] - ETA: 0s - loss: 26.8245
Epoch 00010: val_loss did not improve from 25.44271
1819/1819 [==============================] - 1s 278us/sample - loss: 26.8226 - val_loss: 27.0742
Epoch 11/50
  50/1819 [..............................] - ETA: 0s - loss: 30.5363 250/1819 [===>..........................] - ETA: 0s - loss: 26.9408 450/1819 [======>.......................] - ETA: 0s - loss: 26.5942 650/1819 [=========>....................] - ETA: 0s - loss: 26.5622 850/1819 [=============>................] - ETA: 0s - loss: 26.41831050/1819 [================>.............] - ETA: 0s - loss: 26.53741250/1819 [===================>..........] - ETA: 0s - loss: 26.50951450/1819 [======================>.......] - ETA: 0s - loss: 26.48071650/1819 [==========================>...] - ETA: 0s - loss: 26.4243
Epoch 00011: val_loss improved from 25.44271 to 24.40468, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 295us/sample - loss: 26.4651 - val_loss: 24.4047
Epoch 12/50
  50/1819 [..............................] - ETA: 0s - loss: 26.0851 250/1819 [===>..........................] - ETA: 0s - loss: 25.6781 450/1819 [======>.......................] - ETA: 0s - loss: 26.2871 650/1819 [=========>....................] - ETA: 0s - loss: 26.3687 850/1819 [=============>................] - ETA: 0s - loss: 26.44861000/1819 [===============>..............] - ETA: 0s - loss: 26.35111200/1819 [==================>...........] - ETA: 0s - loss: 26.48841400/1819 [======================>.......] - ETA: 0s - loss: 26.39111600/1819 [=========================>....] - ETA: 0s - loss: 26.52801800/1819 [============================>.] - ETA: 0s - loss: 26.4481
Epoch 00012: val_loss improved from 24.40468 to 23.78036, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 306us/sample - loss: 26.4535 - val_loss: 23.7804
Epoch 13/50
  50/1819 [..............................] - ETA: 0s - loss: 25.7079 250/1819 [===>..........................] - ETA: 0s - loss: 25.4055 450/1819 [======>.......................] - ETA: 0s - loss: 25.5682 650/1819 [=========>....................] - ETA: 0s - loss: 25.8227 850/1819 [=============>................] - ETA: 0s - loss: 25.84031050/1819 [================>.............] - ETA: 0s - loss: 25.91861250/1819 [===================>..........] - ETA: 0s - loss: 25.92201450/1819 [======================>.......] - ETA: 0s - loss: 26.15671650/1819 [==========================>...] - ETA: 0s - loss: 26.1261
Epoch 00013: val_loss did not improve from 23.78036
1819/1819 [==============================] - 0s 271us/sample - loss: 26.1221 - val_loss: 24.0516
Epoch 14/50
  50/1819 [..............................] - ETA: 0s - loss: 25.5799 250/1819 [===>..........................] - ETA: 0s - loss: 25.3800 450/1819 [======>.......................] - ETA: 0s - loss: 25.6895 650/1819 [=========>....................] - ETA: 0s - loss: 26.1179 850/1819 [=============>................] - ETA: 0s - loss: 26.17921050/1819 [================>.............] - ETA: 0s - loss: 26.06401250/1819 [===================>..........] - ETA: 0s - loss: 26.16781450/1819 [======================>.......] - ETA: 0s - loss: 26.33451650/1819 [==========================>...] - ETA: 0s - loss: 26.3649
Epoch 00014: val_loss did not improve from 23.78036

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
1819/1819 [==============================] - 1s 275us/sample - loss: 26.2504 - val_loss: 23.9831
Epoch 15/50
  50/1819 [..............................] - ETA: 0s - loss: 23.5860 250/1819 [===>..........................] - ETA: 0s - loss: 26.9651 450/1819 [======>.......................] - ETA: 0s - loss: 26.6509 650/1819 [=========>....................] - ETA: 0s - loss: 26.2492 850/1819 [=============>................] - ETA: 0s - loss: 26.07471050/1819 [================>.............] - ETA: 0s - loss: 26.06061250/1819 [===================>..........] - ETA: 0s - loss: 26.04811450/1819 [======================>.......] - ETA: 0s - loss: 26.07531650/1819 [==========================>...] - ETA: 0s - loss: 26.1123
Epoch 00015: val_loss improved from 23.78036 to 23.46066, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 294us/sample - loss: 26.1402 - val_loss: 23.4607
Epoch 16/50
  50/1819 [..............................] - ETA: 0s - loss: 24.2537 250/1819 [===>..........................] - ETA: 0s - loss: 25.2059 450/1819 [======>.......................] - ETA: 0s - loss: 25.2476 650/1819 [=========>....................] - ETA: 0s - loss: 25.4822 850/1819 [=============>................] - ETA: 0s - loss: 25.71591050/1819 [================>.............] - ETA: 0s - loss: 25.88941250/1819 [===================>..........] - ETA: 0s - loss: 26.08701450/1819 [======================>.......] - ETA: 0s - loss: 26.07691650/1819 [==========================>...] - ETA: 0s - loss: 25.9833
Epoch 00016: val_loss did not improve from 23.46066
1819/1819 [==============================] - 0s 272us/sample - loss: 25.9062 - val_loss: 24.0810
Epoch 17/50
  50/1819 [..............................] - ETA: 0s - loss: 24.7394 250/1819 [===>..........................] - ETA: 0s - loss: 26.6492 450/1819 [======>.......................] - ETA: 0s - loss: 26.7056 650/1819 [=========>....................] - ETA: 0s - loss: 26.3136 850/1819 [=============>................] - ETA: 0s - loss: 26.23481050/1819 [================>.............] - ETA: 0s - loss: 26.20951250/1819 [===================>..........] - ETA: 0s - loss: 26.12461450/1819 [======================>.......] - ETA: 0s - loss: 26.08771650/1819 [==========================>...] - ETA: 0s - loss: 25.9601
Epoch 00017: val_loss did not improve from 23.46066

Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
1819/1819 [==============================] - 1s 276us/sample - loss: 26.0237 - val_loss: 23.5426
Epoch 18/50
  50/1819 [..............................] - ETA: 0s - loss: 26.1113 250/1819 [===>..........................] - ETA: 0s - loss: 26.1609 450/1819 [======>.......................] - ETA: 0s - loss: 26.1343 600/1819 [========>.....................] - ETA: 0s - loss: 25.8801 700/1819 [==========>...................] - ETA: 0s - loss: 25.8675 900/1819 [=============>................] - ETA: 0s - loss: 25.78871100/1819 [=================>............] - ETA: 0s - loss: 25.96391300/1819 [====================>.........] - ETA: 0s - loss: 25.74571500/1819 [=======================>......] - ETA: 0s - loss: 25.83191700/1819 [===========================>..] - ETA: 0s - loss: 25.9302
Epoch 00018: val_loss improved from 23.46066 to 23.28538, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 315us/sample - loss: 25.9422 - val_loss: 23.2854
Epoch 19/50
  50/1819 [..............................] - ETA: 0s - loss: 27.4630 250/1819 [===>..........................] - ETA: 0s - loss: 26.1075 450/1819 [======>.......................] - ETA: 0s - loss: 25.8747 650/1819 [=========>....................] - ETA: 0s - loss: 25.8554 850/1819 [=============>................] - ETA: 0s - loss: 26.10221050/1819 [================>.............] - ETA: 0s - loss: 26.02461250/1819 [===================>..........] - ETA: 0s - loss: 25.90631500/1819 [=======================>......] - ETA: 0s - loss: 25.90281700/1819 [===========================>..] - ETA: 0s - loss: 25.9031
Epoch 00019: val_loss improved from 23.28538 to 23.27660, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 285us/sample - loss: 25.9708 - val_loss: 23.2766
Epoch 20/50
  50/1819 [..............................] - ETA: 0s - loss: 25.0148 250/1819 [===>..........................] - ETA: 0s - loss: 25.8781 450/1819 [======>.......................] - ETA: 0s - loss: 26.1770 650/1819 [=========>....................] - ETA: 0s - loss: 26.2670 850/1819 [=============>................] - ETA: 0s - loss: 26.09691050/1819 [================>.............] - ETA: 0s - loss: 25.94311250/1819 [===================>..........] - ETA: 0s - loss: 26.06581450/1819 [======================>.......] - ETA: 0s - loss: 25.99791650/1819 [==========================>...] - ETA: 0s - loss: 25.9168
Epoch 00020: val_loss did not improve from 23.27660
1819/1819 [==============================] - 0s 271us/sample - loss: 25.8732 - val_loss: 23.3120
Epoch 21/50
  50/1819 [..............................] - ETA: 0s - loss: 23.3686 300/1819 [===>..........................] - ETA: 0s - loss: 25.2694 500/1819 [=======>......................] - ETA: 0s - loss: 25.1482 700/1819 [==========>...................] - ETA: 0s - loss: 25.4383 900/1819 [=============>................] - ETA: 0s - loss: 25.78281100/1819 [=================>............] - ETA: 0s - loss: 25.95661300/1819 [====================>.........] - ETA: 0s - loss: 25.92361500/1819 [=======================>......] - ETA: 0s - loss: 25.84241700/1819 [===========================>..] - ETA: 0s - loss: 25.7950
Epoch 00021: val_loss did not improve from 23.27660

Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
1819/1819 [==============================] - 0s 272us/sample - loss: 25.7874 - val_loss: 23.3525
Epoch 22/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6456 250/1819 [===>..........................] - ETA: 0s - loss: 25.3777 450/1819 [======>.......................] - ETA: 0s - loss: 25.2921 650/1819 [=========>....................] - ETA: 0s - loss: 25.7525 850/1819 [=============>................] - ETA: 0s - loss: 25.82281050/1819 [================>.............] - ETA: 0s - loss: 25.84771250/1819 [===================>..........] - ETA: 0s - loss: 25.97161450/1819 [======================>.......] - ETA: 0s - loss: 25.85341650/1819 [==========================>...] - ETA: 0s - loss: 25.8272
Epoch 00022: val_loss improved from 23.27660 to 23.25646, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 299us/sample - loss: 25.7598 - val_loss: 23.2565
Epoch 23/50
  50/1819 [..............................] - ETA: 0s - loss: 25.0448 250/1819 [===>..........................] - ETA: 0s - loss: 25.6150 450/1819 [======>.......................] - ETA: 0s - loss: 25.7978 650/1819 [=========>....................] - ETA: 0s - loss: 25.8654 850/1819 [=============>................] - ETA: 0s - loss: 25.77561050/1819 [================>.............] - ETA: 0s - loss: 25.71051250/1819 [===================>..........] - ETA: 0s - loss: 25.92151450/1819 [======================>.......] - ETA: 0s - loss: 25.93491650/1819 [==========================>...] - ETA: 0s - loss: 25.9865
Epoch 00023: val_loss improved from 23.25646 to 23.21927, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 309us/sample - loss: 25.8688 - val_loss: 23.2193
Epoch 24/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6402 250/1819 [===>..........................] - ETA: 0s - loss: 25.5115 450/1819 [======>.......................] - ETA: 0s - loss: 25.5534 650/1819 [=========>....................] - ETA: 0s - loss: 25.5569 850/1819 [=============>................] - ETA: 0s - loss: 25.56811050/1819 [================>.............] - ETA: 0s - loss: 25.82051250/1819 [===================>..........] - ETA: 0s - loss: 25.73721450/1819 [======================>.......] - ETA: 0s - loss: 25.74711650/1819 [==========================>...] - ETA: 0s - loss: 25.7202
Epoch 00024: val_loss did not improve from 23.21927
1819/1819 [==============================] - 1s 290us/sample - loss: 25.7302 - val_loss: 23.2745
Epoch 25/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4379 250/1819 [===>..........................] - ETA: 0s - loss: 25.9393 450/1819 [======>.......................] - ETA: 0s - loss: 25.8148 650/1819 [=========>....................] - ETA: 0s - loss: 25.5698 850/1819 [=============>................] - ETA: 0s - loss: 25.82231050/1819 [================>.............] - ETA: 0s - loss: 25.75491250/1819 [===================>..........] - ETA: 0s - loss: 25.65261450/1819 [======================>.......] - ETA: 0s - loss: 25.61831650/1819 [==========================>...] - ETA: 0s - loss: 25.6965
Epoch 00025: val_loss did not improve from 23.21927

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
1819/1819 [==============================] - 1s 289us/sample - loss: 25.7382 - val_loss: 23.2232
Epoch 26/50
  50/1819 [..............................] - ETA: 0s - loss: 26.5269 250/1819 [===>..........................] - ETA: 0s - loss: 25.9898 400/1819 [=====>........................] - ETA: 0s - loss: 25.5881 600/1819 [========>.....................] - ETA: 0s - loss: 25.8003 800/1819 [============>.................] - ETA: 0s - loss: 25.79851000/1819 [===============>..............] - ETA: 0s - loss: 25.80301200/1819 [==================>...........] - ETA: 0s - loss: 25.70611400/1819 [======================>.......] - ETA: 0s - loss: 25.62341600/1819 [=========================>....] - ETA: 0s - loss: 25.70661800/1819 [============================>.] - ETA: 0s - loss: 25.8050
Epoch 00026: val_loss did not improve from 23.21927
1819/1819 [==============================] - 1s 300us/sample - loss: 25.8061 - val_loss: 23.2269
Epoch 27/50
  50/1819 [..............................] - ETA: 0s - loss: 24.5587 250/1819 [===>..........................] - ETA: 0s - loss: 26.1228 450/1819 [======>.......................] - ETA: 0s - loss: 25.5027 650/1819 [=========>....................] - ETA: 0s - loss: 25.4335 850/1819 [=============>................] - ETA: 0s - loss: 25.37251050/1819 [================>.............] - ETA: 0s - loss: 25.43751250/1819 [===================>..........] - ETA: 0s - loss: 25.43791450/1819 [======================>.......] - ETA: 0s - loss: 25.59531650/1819 [==========================>...] - ETA: 0s - loss: 25.6707
Epoch 00027: val_loss did not improve from 23.21927

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-05.
1819/1819 [==============================] - 1s 288us/sample - loss: 25.7359 - val_loss: 23.2445
Epoch 28/50
  50/1819 [..............................] - ETA: 0s - loss: 25.2341 250/1819 [===>..........................] - ETA: 0s - loss: 26.2121 450/1819 [======>.......................] - ETA: 0s - loss: 25.8567 650/1819 [=========>....................] - ETA: 0s - loss: 25.7272 850/1819 [=============>................] - ETA: 0s - loss: 25.84401050/1819 [================>.............] - ETA: 0s - loss: 25.86871250/1819 [===================>..........] - ETA: 0s - loss: 25.82511450/1819 [======================>.......] - ETA: 0s - loss: 25.89131650/1819 [==========================>...] - ETA: 0s - loss: 25.9816
Epoch 00028: val_loss did not improve from 23.21927
1819/1819 [==============================] - 1s 283us/sample - loss: 25.9223 - val_loss: 23.2287
Epoch 29/50
  50/1819 [..............................] - ETA: 0s - loss: 27.1029 250/1819 [===>..........................] - ETA: 0s - loss: 26.7165 450/1819 [======>.......................] - ETA: 0s - loss: 25.9622 650/1819 [=========>....................] - ETA: 0s - loss: 25.9423 850/1819 [=============>................] - ETA: 0s - loss: 25.83921050/1819 [================>.............] - ETA: 0s - loss: 25.86731250/1819 [===================>..........] - ETA: 0s - loss: 25.74111450/1819 [======================>.......] - ETA: 0s - loss: 25.66141650/1819 [==========================>...] - ETA: 0s - loss: 25.7753
Epoch 00029: val_loss did not improve from 23.21927
1819/1819 [==============================] - 1s 288us/sample - loss: 25.8611 - val_loss: 23.2736
Epoch 30/50
  50/1819 [..............................] - ETA: 0s - loss: 24.9331 250/1819 [===>..........................] - ETA: 0s - loss: 24.8458 450/1819 [======>.......................] - ETA: 0s - loss: 25.0613 650/1819 [=========>....................] - ETA: 0s - loss: 25.4397 850/1819 [=============>................] - ETA: 0s - loss: 25.59961050/1819 [================>.............] - ETA: 0s - loss: 25.71971250/1819 [===================>..........] - ETA: 0s - loss: 25.69331450/1819 [======================>.......] - ETA: 0s - loss: 25.70951650/1819 [==========================>...] - ETA: 0s - loss: 25.6668
Epoch 00030: val_loss did not improve from 23.21927
1819/1819 [==============================] - 1s 290us/sample - loss: 25.7623 - val_loss: 23.2203
Epoch 31/50
  50/1819 [..............................] - ETA: 0s - loss: 26.0983 250/1819 [===>..........................] - ETA: 0s - loss: 26.9212 450/1819 [======>.......................] - ETA: 0s - loss: 26.4534 650/1819 [=========>....................] - ETA: 0s - loss: 26.3836 850/1819 [=============>................] - ETA: 0s - loss: 26.29011050/1819 [================>.............] - ETA: 0s - loss: 26.14641250/1819 [===================>..........] - ETA: 0s - loss: 26.00261450/1819 [======================>.......] - ETA: 0s - loss: 25.88651650/1819 [==========================>...] - ETA: 0s - loss: 25.83371800/1819 [============================>.] - ETA: 0s - loss: 25.8162
Epoch 00031: val_loss improved from 23.21927 to 23.21750, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 316us/sample - loss: 25.8109 - val_loss: 23.2175
Epoch 32/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4218 250/1819 [===>..........................] - ETA: 0s - loss: 25.5292 450/1819 [======>.......................] - ETA: 0s - loss: 25.5763 650/1819 [=========>....................] - ETA: 0s - loss: 25.7058 850/1819 [=============>................] - ETA: 0s - loss: 25.93791050/1819 [================>.............] - ETA: 0s - loss: 25.82761250/1819 [===================>..........] - ETA: 0s - loss: 25.81641450/1819 [======================>.......] - ETA: 0s - loss: 25.85921650/1819 [==========================>...] - ETA: 0s - loss: 25.7871
Epoch 00032: val_loss improved from 23.21750 to 23.21582, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 316us/sample - loss: 25.7314 - val_loss: 23.2158
Epoch 33/50
  50/1819 [..............................] - ETA: 0s - loss: 25.6139 250/1819 [===>..........................] - ETA: 0s - loss: 26.0533 450/1819 [======>.......................] - ETA: 0s - loss: 26.0891 650/1819 [=========>....................] - ETA: 0s - loss: 26.1707 850/1819 [=============>................] - ETA: 0s - loss: 26.00991050/1819 [================>.............] - ETA: 0s - loss: 26.18221250/1819 [===================>..........] - ETA: 0s - loss: 25.92701450/1819 [======================>.......] - ETA: 0s - loss: 25.88521650/1819 [==========================>...] - ETA: 0s - loss: 25.8477
Epoch 00033: val_loss did not improve from 23.21582
1819/1819 [==============================] - 1s 292us/sample - loss: 25.7968 - val_loss: 23.2175
Epoch 34/50
  50/1819 [..............................] - ETA: 0s - loss: 27.0760 250/1819 [===>..........................] - ETA: 0s - loss: 26.1931 450/1819 [======>.......................] - ETA: 0s - loss: 25.8715 650/1819 [=========>....................] - ETA: 0s - loss: 25.3940 850/1819 [=============>................] - ETA: 0s - loss: 25.51561050/1819 [================>.............] - ETA: 0s - loss: 25.65771200/1819 [==================>...........] - ETA: 0s - loss: 25.61091400/1819 [======================>.......] - ETA: 0s - loss: 25.72781600/1819 [=========================>....] - ETA: 0s - loss: 25.87471800/1819 [============================>.] - ETA: 0s - loss: 25.7897
Epoch 00034: val_loss did not improve from 23.21582
1819/1819 [==============================] - 1s 300us/sample - loss: 25.7593 - val_loss: 23.2316
Epoch 35/50
  50/1819 [..............................] - ETA: 0s - loss: 26.5316 250/1819 [===>..........................] - ETA: 0s - loss: 25.6895 450/1819 [======>.......................] - ETA: 0s - loss: 25.7621 650/1819 [=========>....................] - ETA: 0s - loss: 25.9765 850/1819 [=============>................] - ETA: 0s - loss: 25.86261050/1819 [================>.............] - ETA: 0s - loss: 26.03461250/1819 [===================>..........] - ETA: 0s - loss: 25.92301450/1819 [======================>.......] - ETA: 0s - loss: 25.88091650/1819 [==========================>...] - ETA: 0s - loss: 25.8226
Epoch 00035: val_loss did not improve from 23.21582
1819/1819 [==============================] - 1s 290us/sample - loss: 25.8251 - val_loss: 23.2216
Epoch 36/50
  50/1819 [..............................] - ETA: 0s - loss: 25.1022 250/1819 [===>..........................] - ETA: 0s - loss: 25.2013 450/1819 [======>.......................] - ETA: 0s - loss: 25.5316 650/1819 [=========>....................] - ETA: 0s - loss: 25.4195 850/1819 [=============>................] - ETA: 0s - loss: 25.69641050/1819 [================>.............] - ETA: 0s - loss: 25.81561250/1819 [===================>..........] - ETA: 0s - loss: 25.77271450/1819 [======================>.......] - ETA: 0s - loss: 25.86171650/1819 [==========================>...] - ETA: 0s - loss: 25.7810
Epoch 00036: val_loss improved from 23.21582 to 23.21548, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 332us/sample - loss: 25.7350 - val_loss: 23.2155
Epoch 37/50
  50/1819 [..............................] - ETA: 0s - loss: 25.0508 250/1819 [===>..........................] - ETA: 0s - loss: 25.6890 450/1819 [======>.......................] - ETA: 0s - loss: 25.6764 550/1819 [========>.....................] - ETA: 0s - loss: 25.6893 750/1819 [===========>..................] - ETA: 0s - loss: 25.9223 950/1819 [==============>...............] - ETA: 0s - loss: 25.67241150/1819 [=================>............] - ETA: 0s - loss: 25.69701350/1819 [=====================>........] - ETA: 0s - loss: 25.67821550/1819 [========================>.....] - ETA: 0s - loss: 25.70451750/1819 [===========================>..] - ETA: 0s - loss: 25.6618
Epoch 00037: val_loss improved from 23.21548 to 23.21045, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 318us/sample - loss: 25.6775 - val_loss: 23.2104
Epoch 38/50
  50/1819 [..............................] - ETA: 0s - loss: 24.5648 250/1819 [===>..........................] - ETA: 0s - loss: 25.6162 450/1819 [======>.......................] - ETA: 0s - loss: 25.4056 650/1819 [=========>....................] - ETA: 0s - loss: 25.5406 850/1819 [=============>................] - ETA: 0s - loss: 25.65861050/1819 [================>.............] - ETA: 0s - loss: 25.64641250/1819 [===================>..........] - ETA: 0s - loss: 25.75441450/1819 [======================>.......] - ETA: 0s - loss: 25.77041650/1819 [==========================>...] - ETA: 0s - loss: 25.7399
Epoch 00038: val_loss improved from 23.21045 to 23.20895, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 314us/sample - loss: 25.6894 - val_loss: 23.2090
Epoch 39/50
  50/1819 [..............................] - ETA: 0s - loss: 26.5700 250/1819 [===>..........................] - ETA: 0s - loss: 25.7057 450/1819 [======>.......................] - ETA: 0s - loss: 25.4616 650/1819 [=========>....................] - ETA: 0s - loss: 25.5267 850/1819 [=============>................] - ETA: 0s - loss: 25.65191050/1819 [================>.............] - ETA: 0s - loss: 25.68501250/1819 [===================>..........] - ETA: 0s - loss: 25.82371450/1819 [======================>.......] - ETA: 0s - loss: 25.78141650/1819 [==========================>...] - ETA: 0s - loss: 25.9081
Epoch 00039: val_loss improved from 23.20895 to 23.19730, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 309us/sample - loss: 25.8471 - val_loss: 23.1973
Epoch 40/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3258 250/1819 [===>..........................] - ETA: 0s - loss: 25.4379 450/1819 [======>.......................] - ETA: 0s - loss: 26.7287 650/1819 [=========>....................] - ETA: 0s - loss: 26.5934 850/1819 [=============>................] - ETA: 0s - loss: 26.22021050/1819 [================>.............] - ETA: 0s - loss: 26.20571250/1819 [===================>..........] - ETA: 0s - loss: 25.97041450/1819 [======================>.......] - ETA: 0s - loss: 25.84571650/1819 [==========================>...] - ETA: 0s - loss: 25.7718
Epoch 00040: val_loss did not improve from 23.19730
1819/1819 [==============================] - 1s 291us/sample - loss: 25.7763 - val_loss: 23.2124
Epoch 41/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3984 250/1819 [===>..........................] - ETA: 0s - loss: 26.0659 450/1819 [======>.......................] - ETA: 0s - loss: 25.9938 650/1819 [=========>....................] - ETA: 0s - loss: 25.9926 850/1819 [=============>................] - ETA: 0s - loss: 25.94941050/1819 [================>.............] - ETA: 0s - loss: 25.90221250/1819 [===================>..........] - ETA: 0s - loss: 25.93371450/1819 [======================>.......] - ETA: 0s - loss: 25.78421650/1819 [==========================>...] - ETA: 0s - loss: 25.7392
Epoch 00041: val_loss did not improve from 23.19730
1819/1819 [==============================] - 1s 286us/sample - loss: 25.7495 - val_loss: 23.2150
Epoch 42/50
  50/1819 [..............................] - ETA: 0s - loss: 25.3884 250/1819 [===>..........................] - ETA: 0s - loss: 24.4799 450/1819 [======>.......................] - ETA: 0s - loss: 25.2066 650/1819 [=========>....................] - ETA: 0s - loss: 25.8456 850/1819 [=============>................] - ETA: 0s - loss: 25.65251050/1819 [================>.............] - ETA: 0s - loss: 25.78021250/1819 [===================>..........] - ETA: 0s - loss: 25.73241450/1819 [======================>.......] - ETA: 0s - loss: 25.62831600/1819 [=========================>....] - ETA: 0s - loss: 25.78801800/1819 [============================>.] - ETA: 0s - loss: 25.8608
Epoch 00042: val_loss did not improve from 23.19730
1819/1819 [==============================] - 1s 297us/sample - loss: 25.8435 - val_loss: 23.2058
Epoch 43/50
  50/1819 [..............................] - ETA: 0s - loss: 23.7635 250/1819 [===>..........................] - ETA: 0s - loss: 25.6385 450/1819 [======>.......................] - ETA: 0s - loss: 25.9287 650/1819 [=========>....................] - ETA: 0s - loss: 25.7131 850/1819 [=============>................] - ETA: 0s - loss: 25.72091050/1819 [================>.............] - ETA: 0s - loss: 25.54331250/1819 [===================>..........] - ETA: 0s - loss: 25.73011450/1819 [======================>.......] - ETA: 0s - loss: 25.67841650/1819 [==========================>...] - ETA: 0s - loss: 25.8112
Epoch 00043: val_loss improved from 23.19730 to 23.19187, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 313us/sample - loss: 25.7871 - val_loss: 23.1919
Epoch 44/50
  50/1819 [..............................] - ETA: 0s - loss: 26.2445 250/1819 [===>..........................] - ETA: 0s - loss: 25.8338 450/1819 [======>.......................] - ETA: 0s - loss: 26.2013 650/1819 [=========>....................] - ETA: 0s - loss: 25.8503 850/1819 [=============>................] - ETA: 0s - loss: 25.65761050/1819 [================>.............] - ETA: 0s - loss: 25.66161250/1819 [===================>..........] - ETA: 0s - loss: 25.63651450/1819 [======================>.......] - ETA: 0s - loss: 25.75631650/1819 [==========================>...] - ETA: 0s - loss: 25.7521
Epoch 00044: val_loss did not improve from 23.19187
1819/1819 [==============================] - 1s 291us/sample - loss: 25.7080 - val_loss: 23.2127
Epoch 45/50
  50/1819 [..............................] - ETA: 0s - loss: 27.3858 250/1819 [===>..........................] - ETA: 0s - loss: 25.9468 450/1819 [======>.......................] - ETA: 0s - loss: 26.1254 650/1819 [=========>....................] - ETA: 0s - loss: 25.6611 850/1819 [=============>................] - ETA: 0s - loss: 25.55911050/1819 [================>.............] - ETA: 0s - loss: 25.61211250/1819 [===================>..........] - ETA: 0s - loss: 25.91551450/1819 [======================>.......] - ETA: 0s - loss: 25.84961650/1819 [==========================>...] - ETA: 0s - loss: 25.7792
Epoch 00045: val_loss did not improve from 23.19187
1819/1819 [==============================] - 1s 296us/sample - loss: 25.7916 - val_loss: 23.1934
Epoch 46/50
  50/1819 [..............................] - ETA: 0s - loss: 26.0971 250/1819 [===>..........................] - ETA: 0s - loss: 25.8330 450/1819 [======>.......................] - ETA: 0s - loss: 25.8819 650/1819 [=========>....................] - ETA: 0s - loss: 26.0838 850/1819 [=============>................] - ETA: 0s - loss: 25.88471050/1819 [================>.............] - ETA: 0s - loss: 25.97861250/1819 [===================>..........] - ETA: 0s - loss: 25.97411450/1819 [======================>.......] - ETA: 0s - loss: 25.80211650/1819 [==========================>...] - ETA: 0s - loss: 25.7813
Epoch 00046: val_loss improved from 23.19187 to 23.18132, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 313us/sample - loss: 25.6905 - val_loss: 23.1813
Epoch 47/50
  50/1819 [..............................] - ETA: 0s - loss: 24.8356 250/1819 [===>..........................] - ETA: 0s - loss: 25.1053 450/1819 [======>.......................] - ETA: 0s - loss: 25.7226 650/1819 [=========>....................] - ETA: 0s - loss: 25.6602 850/1819 [=============>................] - ETA: 0s - loss: 25.79161050/1819 [================>.............] - ETA: 0s - loss: 25.63261250/1819 [===================>..........] - ETA: 0s - loss: 25.73381450/1819 [======================>.......] - ETA: 0s - loss: 25.78831650/1819 [==========================>...] - ETA: 0s - loss: 25.7246
Epoch 00047: val_loss did not improve from 23.18132
1819/1819 [==============================] - 1s 301us/sample - loss: 25.8164 - val_loss: 23.2026
Epoch 48/50
  50/1819 [..............................] - ETA: 0s - loss: 25.4883 250/1819 [===>..........................] - ETA: 0s - loss: 25.7479 450/1819 [======>.......................] - ETA: 0s - loss: 25.7791 650/1819 [=========>....................] - ETA: 0s - loss: 25.7965 850/1819 [=============>................] - ETA: 0s - loss: 25.83531050/1819 [================>.............] - ETA: 0s - loss: 25.88261250/1819 [===================>..........] - ETA: 0s - loss: 25.87731450/1819 [======================>.......] - ETA: 0s - loss: 25.87971650/1819 [==========================>...] - ETA: 0s - loss: 25.6804
Epoch 00048: val_loss did not improve from 23.18132
1819/1819 [==============================] - 1s 308us/sample - loss: 25.7929 - val_loss: 23.1976
Epoch 49/50
  50/1819 [..............................] - ETA: 0s - loss: 25.0174 250/1819 [===>..........................] - ETA: 0s - loss: 25.0283 450/1819 [======>.......................] - ETA: 0s - loss: 25.6485 650/1819 [=========>....................] - ETA: 0s - loss: 25.6932 850/1819 [=============>................] - ETA: 0s - loss: 25.89611050/1819 [================>.............] - ETA: 0s - loss: 25.82911250/1819 [===================>..........] - ETA: 0s - loss: 25.91161450/1819 [======================>.......] - ETA: 0s - loss: 25.71241650/1819 [==========================>...] - ETA: 0s - loss: 25.7369
Epoch 00049: val_loss improved from 23.18132 to 23.17607, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
1819/1819 [==============================] - 1s 316us/sample - loss: 25.7733 - val_loss: 23.1761
Epoch 50/50
  50/1819 [..............................] - ETA: 0s - loss: 27.2328 250/1819 [===>..........................] - ETA: 0s - loss: 25.8938 450/1819 [======>.......................] - ETA: 0s - loss: 26.2583 650/1819 [=========>....................] - ETA: 0s - loss: 26.0470 850/1819 [=============>................] - ETA: 0s - loss: 25.96641050/1819 [================>.............] - ETA: 0s - loss: 25.92121250/1819 [===================>..........] - ETA: 0s - loss: 25.85121450/1819 [======================>.......] - ETA: 0s - loss: 25.81391650/1819 [==========================>...] - ETA: 0s - loss: 25.7876
Epoch 00050: val_loss did not improve from 23.17607
1819/1819 [==============================] - 1s 298us/sample - loss: 25.7165 - val_loss: 23.2157
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b258f3760b8> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b258f3f8240> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f3f8e48> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258f3f8eb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f816048> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f815390> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258f829d68> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f829cc0> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b258f8332b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f856c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f861a58> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8677b8> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b258f87c438> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b258f87c390> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b258f887da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f887c88> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f8a0898> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8a0b00> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f8a0c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8aaa58> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b258f8aabe0> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b258f3760b8> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b258f3f8240> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f3f8e48> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258f3f8eb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f816048> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f815390> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258f829d68> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f829cc0> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b258f8332b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f856c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f861a58> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8677b8> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b258f87c438> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b258f87c390> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b258f887da0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f887c88> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f8a0898> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8a0b00> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258f8a0c88> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8aaa58> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b258f8aabe0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258e8e1518> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8defd0> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 169.0201
Epoch 00001: val_loss improved from inf to 99.47841, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 11ms/sample - loss: 132.1594 - val_loss: 99.4784
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 98.6899
Epoch 00002: val_loss improved from 99.47841 to 61.59626, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 745us/sample - loss: 80.0298 - val_loss: 61.5963
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 58.8457
Epoch 00003: val_loss improved from 61.59626 to 45.66078, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 771us/sample - loss: 51.8881 - val_loss: 45.6608
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.1359
Epoch 00004: val_loss improved from 45.66078 to 40.67324, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 766us/sample - loss: 41.1248 - val_loss: 40.6732
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 36.0018
Epoch 00005: val_loss improved from 40.67324 to 33.67163, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 747us/sample - loss: 36.3391 - val_loss: 33.6716
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 29.6143
Epoch 00006: val_loss improved from 33.67163 to 27.68623, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 728us/sample - loss: 29.7418 - val_loss: 27.6862
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.9865
Epoch 00007: val_loss improved from 27.68623 to 26.85318, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 738us/sample - loss: 26.9281 - val_loss: 26.8532
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8312
Epoch 00008: val_loss did not improve from 26.85318
121/121 [==============================] - 0s 518us/sample - loss: 26.5209 - val_loss: 26.8933
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.5707
Epoch 00009: val_loss improved from 26.85318 to 26.01544, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 742us/sample - loss: 26.0535 - val_loss: 26.0154
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.4156
Epoch 00010: val_loss improved from 26.01544 to 25.87503, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 794us/sample - loss: 25.5917 - val_loss: 25.8750
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7272
Epoch 00011: val_loss improved from 25.87503 to 25.32275, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 815us/sample - loss: 25.1606 - val_loss: 25.3228
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.5628
Epoch 00012: val_loss improved from 25.32275 to 25.21326, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 779us/sample - loss: 24.8443 - val_loss: 25.2133
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.8486
Epoch 00013: val_loss improved from 25.21326 to 24.86358, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 24.6329 - val_loss: 24.8636
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4981
Epoch 00014: val_loss improved from 24.86358 to 24.74307, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 24.3778 - val_loss: 24.7431
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.4803
Epoch 00015: val_loss improved from 24.74307 to 24.56847, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 749us/sample - loss: 24.1659 - val_loss: 24.5685
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6676
Epoch 00016: val_loss improved from 24.56847 to 24.44317, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 789us/sample - loss: 23.9625 - val_loss: 24.4432
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6156
Epoch 00017: val_loss improved from 24.44317 to 24.21833, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 780us/sample - loss: 23.8450 - val_loss: 24.2183
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3344
Epoch 00018: val_loss did not improve from 24.21833
121/121 [==============================] - 0s 544us/sample - loss: 23.7480 - val_loss: 24.3192
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2681
Epoch 00019: val_loss improved from 24.21833 to 24.06579, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 683us/sample - loss: 23.7888 - val_loss: 24.0658
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6972
Epoch 00020: val_loss did not improve from 24.06579
121/121 [==============================] - 0s 552us/sample - loss: 23.5670 - val_loss: 24.0804
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6138
Epoch 00021: val_loss improved from 24.06579 to 23.93493, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 718us/sample - loss: 23.5570 - val_loss: 23.9349
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6731
Epoch 00022: val_loss improved from 23.93493 to 23.85337, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 753us/sample - loss: 23.4874 - val_loss: 23.8534
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0746
Epoch 00023: val_loss did not improve from 23.85337
121/121 [==============================] - 0s 588us/sample - loss: 23.3860 - val_loss: 24.0498
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8607
Epoch 00024: val_loss improved from 23.85337 to 23.77038, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 724us/sample - loss: 23.3396 - val_loss: 23.7704
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4189
Epoch 00025: val_loss did not improve from 23.77038
121/121 [==============================] - 0s 543us/sample - loss: 23.4688 - val_loss: 23.9522
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7891
Epoch 00026: val_loss did not improve from 23.77038

Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 533us/sample - loss: 23.3251 - val_loss: 23.8197
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1353
Epoch 00027: val_loss improved from 23.77038 to 23.74106, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 674us/sample - loss: 23.2019 - val_loss: 23.7411
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8576
Epoch 00028: val_loss did not improve from 23.74106
121/121 [==============================] - 0s 495us/sample - loss: 23.2011 - val_loss: 23.7969
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5793
Epoch 00029: val_loss did not improve from 23.74106

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 529us/sample - loss: 23.2271 - val_loss: 23.9598
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6815
Epoch 00030: val_loss did not improve from 23.74106
121/121 [==============================] - 0s 532us/sample - loss: 23.2058 - val_loss: 23.8075
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9065
Epoch 00031: val_loss improved from 23.74106 to 23.70411, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 2ms/sample - loss: 23.1678 - val_loss: 23.7041
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.9147
Epoch 00032: val_loss improved from 23.70411 to 23.68852, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 744us/sample - loss: 23.1866 - val_loss: 23.6885
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7766
Epoch 00033: val_loss did not improve from 23.68852
121/121 [==============================] - 0s 573us/sample - loss: 23.1905 - val_loss: 23.7268
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.7326
Epoch 00034: val_loss did not improve from 23.68852

Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 563us/sample - loss: 23.1643 - val_loss: 23.7328
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2054
Epoch 00035: val_loss did not improve from 23.68852
121/121 [==============================] - 0s 572us/sample - loss: 23.1527 - val_loss: 23.7311
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1092
Epoch 00036: val_loss did not improve from 23.68852

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 581us/sample - loss: 23.1508 - val_loss: 23.7302
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3214
Epoch 00037: val_loss did not improve from 23.68852
121/121 [==============================] - 0s 553us/sample - loss: 23.1498 - val_loss: 23.7215
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0482
Epoch 00038: val_loss did not improve from 23.68852

Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 529us/sample - loss: 23.1518 - val_loss: 23.7100
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5979
Epoch 00039: val_loss did not improve from 23.68852
121/121 [==============================] - 0s 523us/sample - loss: 23.1527 - val_loss: 23.7082
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.8710
Epoch 00040: val_loss did not improve from 23.68852

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 536us/sample - loss: 23.1528 - val_loss: 23.7060
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.7148
Epoch 00041: val_loss did not improve from 23.68852
121/121 [==============================] - 0s 528us/sample - loss: 23.1530 - val_loss: 23.7040
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4411
Epoch 00042: val_loss did not improve from 23.68852
121/121 [==============================] - 0s 544us/sample - loss: 23.1537 - val_loss: 23.7010
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:209: RuntimeWarning: divide by zero encountered in log
  predictor_matrix[:,:,:,indicesPRED] = numpy.log(predictor_matrix[:,:,:,indicesPRED])
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00042: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b258fb95668> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b258fc1d2b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fc1d4e0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258fc1deb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258fc26e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fc26748> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258fe46b70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258fe46ac8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b258fe4c0b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fe72e10> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fe78860> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fe825c0> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b258fe8ee48> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b258fe96198> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b258fea1f28> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fea1a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258febb6a0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258febb908> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258febba90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fec4860> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b258fec49e8> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b258fb95668> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b258fc1d2b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fc1d4e0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258fc1deb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258fc26e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fc26748> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b258fe46b70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258fe46ac8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b258fe4c0b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fe72e10> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fe78860> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fe825c0> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b258fe8ee48> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b258fe96198> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b258fea1f28> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fea1a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258febb6a0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258febb908> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b258febba90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258fec4860> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b258fec49e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f0a7780> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258ff0a3c8> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 150.6766
Epoch 00001: val_loss improved from inf to 56.46819, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 7ms/sample - loss: 114.5041 - val_loss: 56.4682
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 57.8244
Epoch 00002: val_loss improved from 56.46819 to 35.39850, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 779us/sample - loss: 46.2021 - val_loss: 35.3985
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 36.2966
Epoch 00003: val_loss did not improve from 35.39850
121/121 [==============================] - 0s 568us/sample - loss: 38.1296 - val_loss: 36.9313
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.5143
Epoch 00004: val_loss improved from 35.39850 to 26.86398, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 694us/sample - loss: 35.4980 - val_loss: 26.8640
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 29.9439
Epoch 00005: val_loss improved from 26.86398 to 26.82880, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 711us/sample - loss: 25.9269 - val_loss: 26.8288
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 28.5447
Epoch 00006: val_loss improved from 26.82880 to 25.51269, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 707us/sample - loss: 26.8669 - val_loss: 25.5127
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3821
Epoch 00007: val_loss improved from 25.51269 to 24.90440, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 25.1272 - val_loss: 24.9044
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2087
Epoch 00008: val_loss improved from 24.90440 to 24.46715, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 808us/sample - loss: 25.1129 - val_loss: 24.4671
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3581
Epoch 00009: val_loss improved from 24.46715 to 24.33269, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 2ms/sample - loss: 24.7179 - val_loss: 24.3327
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.0488
Epoch 00010: val_loss improved from 24.33269 to 24.31658, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 24.4773 - val_loss: 24.3166
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.5713
Epoch 00011: val_loss improved from 24.31658 to 24.22006, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 731us/sample - loss: 24.4588 - val_loss: 24.2201
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.0368
Epoch 00012: val_loss improved from 24.22006 to 23.96374, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 744us/sample - loss: 24.2670 - val_loss: 23.9637
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5135
Epoch 00013: val_loss did not improve from 23.96374
121/121 [==============================] - 0s 583us/sample - loss: 24.1987 - val_loss: 24.0353
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.1937
Epoch 00014: val_loss improved from 23.96374 to 23.86267, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 694us/sample - loss: 24.1082 - val_loss: 23.8627
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1665
Epoch 00015: val_loss improved from 23.86267 to 23.75646, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 702us/sample - loss: 24.0382 - val_loss: 23.7565
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7974
Epoch 00016: val_loss improved from 23.75646 to 23.74458, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 701us/sample - loss: 23.9798 - val_loss: 23.7446
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7423
Epoch 00017: val_loss improved from 23.74458 to 23.64781, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 704us/sample - loss: 23.9183 - val_loss: 23.6478
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6825
Epoch 00018: val_loss improved from 23.64781 to 23.61775, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 23.9095 - val_loss: 23.6177
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0646
Epoch 00019: val_loss did not improve from 23.61775
121/121 [==============================] - 0s 555us/sample - loss: 23.8541 - val_loss: 23.6523
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1808
Epoch 00020: val_loss improved from 23.61775 to 23.51514, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 683us/sample - loss: 23.8254 - val_loss: 23.5151
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9818
Epoch 00021: val_loss improved from 23.51514 to 23.49792, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 673us/sample - loss: 23.8125 - val_loss: 23.4979
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.0727
Epoch 00022: val_loss did not improve from 23.49792
121/121 [==============================] - 0s 531us/sample - loss: 23.7482 - val_loss: 23.5246
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.7666
Epoch 00023: val_loss improved from 23.49792 to 23.42775, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 681us/sample - loss: 23.6890 - val_loss: 23.4278
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3806
Epoch 00024: val_loss improved from 23.42775 to 23.36234, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 23.6558 - val_loss: 23.3623
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0271
Epoch 00025: val_loss improved from 23.36234 to 23.31133, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 761us/sample - loss: 23.6324 - val_loss: 23.3113
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.5237
Epoch 00026: val_loss improved from 23.31133 to 23.29572, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 747us/sample - loss: 23.5614 - val_loss: 23.2957
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6923
Epoch 00027: val_loss improved from 23.29572 to 23.26417, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 732us/sample - loss: 23.4993 - val_loss: 23.2642
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9321
Epoch 00028: val_loss did not improve from 23.26417
121/121 [==============================] - 0s 561us/sample - loss: 23.4872 - val_loss: 23.2942
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3464
Epoch 00029: val_loss improved from 23.26417 to 23.18388, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 734us/sample - loss: 23.4260 - val_loss: 23.1839
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3185
Epoch 00030: val_loss did not improve from 23.18388
121/121 [==============================] - 0s 568us/sample - loss: 23.5936 - val_loss: 23.3669
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2793
Epoch 00031: val_loss did not improve from 23.18388

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 544us/sample - loss: 23.5759 - val_loss: 23.2259
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1572
Epoch 00032: val_loss improved from 23.18388 to 23.08053, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 816us/sample - loss: 23.4057 - val_loss: 23.0805
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0889
Epoch 00033: val_loss did not improve from 23.08053
121/121 [==============================] - 0s 567us/sample - loss: 23.3428 - val_loss: 23.0817
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.3863
Epoch 00034: val_loss did not improve from 23.08053

Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 528us/sample - loss: 23.3149 - val_loss: 23.2399
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3436
Epoch 00035: val_loss did not improve from 23.08053
121/121 [==============================] - 0s 513us/sample - loss: 23.3491 - val_loss: 23.1670
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.8871
Epoch 00036: val_loss improved from 23.08053 to 23.03719, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 654us/sample - loss: 23.2562 - val_loss: 23.0372
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0705
Epoch 00037: val_loss improved from 23.03719 to 23.02104, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 710us/sample - loss: 23.2782 - val_loss: 23.0210
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6345
Epoch 00038: val_loss did not improve from 23.02104
121/121 [==============================] - 0s 561us/sample - loss: 23.2772 - val_loss: 23.0517
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3816
Epoch 00039: val_loss did not improve from 23.02104

Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 533us/sample - loss: 23.2554 - val_loss: 23.0680
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8949
Epoch 00040: val_loss did not improve from 23.02104
121/121 [==============================] - 0s 567us/sample - loss: 23.2431 - val_loss: 23.0786
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0315
Epoch 00041: val_loss did not improve from 23.02104

Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 535us/sample - loss: 23.2430 - val_loss: 23.0775
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.1837
Epoch 00042: val_loss did not improve from 23.02104
121/121 [==============================] - 0s 520us/sample - loss: 23.2411 - val_loss: 23.0743
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4407
Epoch 00043: val_loss did not improve from 23.02104

Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 552us/sample - loss: 23.2390 - val_loss: 23.0597
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1031
Epoch 00044: val_loss did not improve from 23.02104
121/121 [==============================] - 0s 533us/sample - loss: 23.2348 - val_loss: 23.0538
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8751
Epoch 00045: val_loss did not improve from 23.02104

Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 583us/sample - loss: 23.2335 - val_loss: 23.0468
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.6641
Epoch 00046: val_loss did not improve from 23.02104
121/121 [==============================] - 0s 584us/sample - loss: 23.2335 - val_loss: 23.0424
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8219
Epoch 00047: val_loss did not improve from 23.02104
121/121 [==============================] - 0s 560us/sample - loss: 23.2333 - val_loss: 23.0389
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00047: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b261480c940> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b2614819860> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b2614819c50> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b2614819e10> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b261481eda0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b261481e668> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b2614833c50> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b2614833ba8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b261483b198> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b2614861e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148698d0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b2614872630> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b261487ceb8> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b2614884208> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b261488ef98> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b261488eac8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b26148aa710> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148aa978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b26148aab00> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148b48d0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b26148b4a58> False
Model: "model_8"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_5[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 57, 2)    578         conv2d_4[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b261480c940> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b2614819860> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b2614819c50> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b2614819e10> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b261481eda0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b261481e668> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b2614833c50> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b2614833ba8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b261483b198> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b2614861e80> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148698d0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b2614872630> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b261487ceb8> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b2614884208> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b261488ef98> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b261488eac8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b26148aa710> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148aa978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b26148aab00> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148b48d0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b26148b4a58> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b258f8bd978> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b26148e8a58> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/200
 20/242 [=>............................] - ETA: 35s - loss: 114.6446200/242 [=======================>......] - ETA: 0s - loss: 61.8230  
Epoch 00001: val_loss improved from inf to 34.07867, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 4s 15ms/sample - loss: 57.0472 - val_loss: 34.0787
Epoch 2/200
 20/242 [=>............................] - ETA: 0s - loss: 36.0912200/242 [=======================>......] - ETA: 0s - loss: 34.6345
Epoch 00002: val_loss improved from 34.07867 to 25.95646, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 526us/sample - loss: 33.1747 - val_loss: 25.9565
Epoch 3/200
 20/242 [=>............................] - ETA: 0s - loss: 28.7678220/242 [==========================>...] - ETA: 0s - loss: 26.9705
Epoch 00003: val_loss improved from 25.95646 to 25.69943, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 509us/sample - loss: 27.0929 - val_loss: 25.6994
Epoch 4/200
 20/242 [=>............................] - ETA: 0s - loss: 26.2163220/242 [==========================>...] - ETA: 0s - loss: 25.8676
Epoch 00004: val_loss improved from 25.69943 to 25.34461, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 535us/sample - loss: 25.6779 - val_loss: 25.3446
Epoch 5/200
 20/242 [=>............................] - ETA: 0s - loss: 26.0582220/242 [==========================>...] - ETA: 0s - loss: 25.3348
Epoch 00005: val_loss improved from 25.34461 to 25.02768, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 525us/sample - loss: 25.2777 - val_loss: 25.0277
Epoch 6/200
 20/242 [=>............................] - ETA: 0s - loss: 23.6169200/242 [=======================>......] - ETA: 0s - loss: 24.9631
Epoch 00006: val_loss improved from 25.02768 to 24.56185, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 514us/sample - loss: 24.9298 - val_loss: 24.5618
Epoch 7/200
 20/242 [=>............................] - ETA: 0s - loss: 23.6584200/242 [=======================>......] - ETA: 0s - loss: 24.2847
Epoch 00007: val_loss improved from 24.56185 to 24.11808, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 548us/sample - loss: 24.2626 - val_loss: 24.1181
Epoch 8/200
 20/242 [=>............................] - ETA: 0s - loss: 24.1843200/242 [=======================>......] - ETA: 0s - loss: 24.1689
Epoch 00008: val_loss improved from 24.11808 to 23.82327, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 517us/sample - loss: 23.9912 - val_loss: 23.8233
Epoch 9/200
 20/242 [=>............................] - ETA: 0s - loss: 24.2394200/242 [=======================>......] - ETA: 0s - loss: 23.7729
Epoch 00009: val_loss improved from 23.82327 to 23.74577, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 539us/sample - loss: 23.7678 - val_loss: 23.7458
Epoch 10/200
 20/242 [=>............................] - ETA: 0s - loss: 22.6063200/242 [=======================>......] - ETA: 0s - loss: 23.7107
Epoch 00010: val_loss improved from 23.74577 to 23.64347, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 520us/sample - loss: 23.6885 - val_loss: 23.6435
Epoch 11/200
 20/242 [=>............................] - ETA: 0s - loss: 22.0275200/242 [=======================>......] - ETA: 0s - loss: 23.5013
Epoch 00011: val_loss improved from 23.64347 to 23.54233, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 557us/sample - loss: 23.5797 - val_loss: 23.5423
Epoch 12/200
 20/242 [=>............................] - ETA: 0s - loss: 23.3186220/242 [==========================>...] - ETA: 0s - loss: 23.5891
Epoch 00012: val_loss improved from 23.54233 to 23.46648, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 522us/sample - loss: 23.5543 - val_loss: 23.4665
Epoch 13/200
 20/242 [=>............................] - ETA: 0s - loss: 24.1000200/242 [=======================>......] - ETA: 0s - loss: 23.7021
Epoch 00013: val_loss did not improve from 23.46648
242/242 [==============================] - 0s 425us/sample - loss: 23.4027 - val_loss: 23.4932
Epoch 14/200
 20/242 [=>............................] - ETA: 0s - loss: 21.6677200/242 [=======================>......] - ETA: 0s - loss: 23.5950
Epoch 00014: val_loss improved from 23.46648 to 23.40138, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 566us/sample - loss: 23.4369 - val_loss: 23.4014
Epoch 15/200
 20/242 [=>............................] - ETA: 0s - loss: 24.3360220/242 [==========================>...] - ETA: 0s - loss: 23.0709
Epoch 00015: val_loss improved from 23.40138 to 23.28621, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 504us/sample - loss: 23.2959 - val_loss: 23.2862
Epoch 16/200
 20/242 [=>............................] - ETA: 0s - loss: 23.4099220/242 [==========================>...] - ETA: 0s - loss: 23.1005
Epoch 00016: val_loss improved from 23.28621 to 23.24700, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 503us/sample - loss: 23.2606 - val_loss: 23.2470
Epoch 17/200
 20/242 [=>............................] - ETA: 0s - loss: 21.6856180/242 [=====================>........] - ETA: 0s - loss: 23.0270
Epoch 00017: val_loss improved from 23.24700 to 23.23078, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 864us/sample - loss: 23.2040 - val_loss: 23.2308
Epoch 18/200
 20/242 [=>............................] - ETA: 0s - loss: 22.2201200/242 [=======================>......] - ETA: 0s - loss: 23.2191
Epoch 00018: val_loss improved from 23.23078 to 23.20922, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 523us/sample - loss: 23.2326 - val_loss: 23.2092
Epoch 19/200
 20/242 [=>............................] - ETA: 0s - loss: 22.5104220/242 [==========================>...] - ETA: 0s - loss: 23.1729
Epoch 00019: val_loss improved from 23.20922 to 23.19698, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 518us/sample - loss: 23.1892 - val_loss: 23.1970
Epoch 20/200
 20/242 [=>............................] - ETA: 0s - loss: 21.8183200/242 [=======================>......] - ETA: 0s - loss: 22.8077
Epoch 00020: val_loss improved from 23.19698 to 23.13838, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 530us/sample - loss: 23.1428 - val_loss: 23.1384
Epoch 21/200
 20/242 [=>............................] - ETA: 0s - loss: 23.1520220/242 [==========================>...] - ETA: 0s - loss: 22.9819
Epoch 00021: val_loss improved from 23.13838 to 23.13821, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 498us/sample - loss: 23.1015 - val_loss: 23.1382
Epoch 22/200
 20/242 [=>............................] - ETA: 0s - loss: 22.9223200/242 [=======================>......] - ETA: 0s - loss: 23.0526
Epoch 00022: val_loss did not improve from 23.13821
242/242 [==============================] - 0s 432us/sample - loss: 23.0572 - val_loss: 23.1455
Epoch 23/200
 20/242 [=>............................] - ETA: 0s - loss: 22.8214220/242 [==========================>...] - ETA: 0s - loss: 23.0262
Epoch 00023: val_loss improved from 23.13821 to 23.09268, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 513us/sample - loss: 23.1484 - val_loss: 23.0927
Epoch 24/200
 20/242 [=>............................] - ETA: 0s - loss: 22.6614200/242 [=======================>......] - ETA: 0s - loss: 23.1340
Epoch 00024: val_loss did not improve from 23.09268
242/242 [==============================] - 0s 439us/sample - loss: 23.1301 - val_loss: 23.4472
Epoch 25/200
 20/242 [=>............................] - ETA: 0s - loss: 20.1257220/242 [==========================>...] - ETA: 0s - loss: 23.3155
Epoch 00025: val_loss did not improve from 23.09268

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 435us/sample - loss: 23.1437 - val_loss: 23.1361
Epoch 26/200
 20/242 [=>............................] - ETA: 0s - loss: 20.2197180/242 [=====================>........] - ETA: 0s - loss: 22.7616
Epoch 00026: val_loss did not improve from 23.09268
242/242 [==============================] - 0s 465us/sample - loss: 23.0141 - val_loss: 23.1365
Epoch 27/200
 20/242 [=>............................] - ETA: 0s - loss: 21.5192220/242 [==========================>...] - ETA: 0s - loss: 22.8748
Epoch 00027: val_loss improved from 23.09268 to 23.07436, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 507us/sample - loss: 23.0091 - val_loss: 23.0744
Epoch 28/200
 20/242 [=>............................] - ETA: 0s - loss: 25.7823200/242 [=======================>......] - ETA: 0s - loss: 23.2502
Epoch 00028: val_loss did not improve from 23.07436
242/242 [==============================] - 0s 438us/sample - loss: 23.0150 - val_loss: 23.1609
Epoch 29/200
 20/242 [=>............................] - ETA: 0s - loss: 23.0488200/242 [=======================>......] - ETA: 0s - loss: 23.3561
Epoch 00029: val_loss did not improve from 23.07436

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 458us/sample - loss: 22.9947 - val_loss: 23.0792
Epoch 30/200
 20/242 [=>............................] - ETA: 0s - loss: 22.3660200/242 [=======================>......] - ETA: 0s - loss: 23.2441
Epoch 00030: val_loss did not improve from 23.07436
242/242 [==============================] - 0s 454us/sample - loss: 22.9892 - val_loss: 23.1127
Epoch 31/200
 20/242 [=>............................] - ETA: 0s - loss: 21.9970200/242 [=======================>......] - ETA: 0s - loss: 23.0945
Epoch 00031: val_loss did not improve from 23.07436

Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 431us/sample - loss: 22.9919 - val_loss: 23.0814
Epoch 32/200
 20/242 [=>............................] - ETA: 0s - loss: 24.2215220/242 [==========================>...] - ETA: 0s - loss: 23.1032
Epoch 00032: val_loss improved from 23.07436 to 23.07221, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2003/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 500us/sample - loss: 22.9833 - val_loss: 23.0722
Epoch 33/200
 20/242 [=>............................] - ETA: 0s - loss: 23.6516200/242 [=======================>......] - ETA: 0s - loss: 23.0902
Epoch 00033: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 443us/sample - loss: 22.9820 - val_loss: 23.0943
Epoch 34/200
 20/242 [=>............................] - ETA: 0s - loss: 23.9989200/242 [=======================>......] - ETA: 0s - loss: 22.5179
Epoch 00034: val_loss did not improve from 23.07221

Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 430us/sample - loss: 22.9816 - val_loss: 23.0960
Epoch 35/200
 20/242 [=>............................] - ETA: 0s - loss: 24.8180200/242 [=======================>......] - ETA: 0s - loss: 23.2226
Epoch 00035: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 434us/sample - loss: 22.9797 - val_loss: 23.0790
Epoch 36/200
 20/242 [=>............................] - ETA: 0s - loss: 21.6898220/242 [==========================>...] - ETA: 0s - loss: 23.0439
Epoch 00036: val_loss did not improve from 23.07221

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 407us/sample - loss: 22.9787 - val_loss: 23.0824
Epoch 37/200
 20/242 [=>............................] - ETA: 0s - loss: 24.0259200/242 [=======================>......] - ETA: 0s - loss: 23.1975
Epoch 00037: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 444us/sample - loss: 22.9784 - val_loss: 23.0858
Epoch 38/200
 20/242 [=>............................] - ETA: 0s - loss: 26.1188220/242 [==========================>...] - ETA: 0s - loss: 22.8127
Epoch 00038: val_loss did not improve from 23.07221

Epoch 00038: ReduceLROnPlateau reducing learning rate to 1e-05.
242/242 [==============================] - 0s 436us/sample - loss: 22.9780 - val_loss: 23.0890
Epoch 39/200
 20/242 [=>............................] - ETA: 0s - loss: 20.8791200/242 [=======================>......] - ETA: 0s - loss: 22.4232
Epoch 00039: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 440us/sample - loss: 22.9779 - val_loss: 23.0899
Epoch 40/200
 20/242 [=>............................] - ETA: 0s - loss: 21.5366220/242 [==========================>...] - ETA: 0s - loss: 23.1261
Epoch 00040: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 427us/sample - loss: 22.9783 - val_loss: 23.0875
Epoch 41/200
 20/242 [=>............................] - ETA: 0s - loss: 24.2556200/242 [=======================>......] - ETA: 0s - loss: 22.7459
Epoch 00041: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 434us/sample - loss: 22.9776 - val_loss: 23.0866
Epoch 42/200
 20/242 [=>............................] - ETA: 0s - loss: 21.5963200/242 [=======================>......] - ETA: 0s - loss: 22.9318
Epoch 00042: val_loss did not improve from 23.07221
242/242 [==============================] - 0s 437us/sample - loss: 22.9774 - val_loss: 23.0835
Epoch 00042: early stopping
done
