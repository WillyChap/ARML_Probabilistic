2020-11-09 16:02:20.323226: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 16:02:20.330188: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 16:02:20.330321: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c88403980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 16:02:20.330339: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 16:02:20.331641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 16:02:20.348345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:02:20.348623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:02:20.351251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:02:20.353664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:02:20.354041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:02:20.356953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:02:20.358179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:02:20.363555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:02:20.366450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:02:20.366494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:02:20.516210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:02:20.516275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:02:20.516289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:02:20.520818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 16:02:20.522781: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c8917eec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 16:02:20.522810: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 16:02:20.524943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:02:20.525014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:02:20.525030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:02:20.525045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:02:20.526319: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:02:20.526376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:02:20.526424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:02:20.526439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:02:20.529251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:02:20.529282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:02:20.529292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:02:20.529301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:02:20.532122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 16:02:20.533801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:02:20.533852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:02:20.533869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:02:20.533883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:02:20.533896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:02:20.533910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:02:20.533923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:02:20.533936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:02:20.536742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:02:20.536766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:02:20.536776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:02:20.536784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:02:20.539587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 154.0092, 149.1109
Mean and standard deviation for "p_sfc" = 985.3661, 62.1340
Mean and standard deviation for "u_tr_p" = 12.2314, 12.4309
Mean and standard deviation for "v_tr_p" = 0.4626, 13.4942
Mean and standard deviation for "Z_p" = 5576.9385, 195.6460
Mean and standard deviation for "IWV" = 13.3017, 8.0968
2020-11-09 16:02:21.760387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:02:21.760505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:02:21.760523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:02:21.760537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:02:21.760551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:02:21.760572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:02:21.760585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:02:21.760599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:02:21.763440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:02:21.765234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 16:02:21.765282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 16:02:21.765297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 16:02:21.765311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 16:02:21.765324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 16:02:21.765337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 16:02:21.765351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 16:02:21.765364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:02:21.768140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 16:02:21.768175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 16:02:21.768185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 16:02:21.768194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 16:02:21.771016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30300 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 16:02:23.383282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 16:02:24.390024: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 16:02:24.405093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 187.8592, 172.3235
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/50
 50/121 [===========>..................] - ETA: 3s - loss: 147.2870
Epoch 00001: val_loss improved from inf to 157.66148, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 3s 22ms/sample - loss: 148.8507 - val_loss: 157.6615
Epoch 2/50
 50/121 [===========>..................] - ETA: 0s - loss: 153.6674
Epoch 00002: val_loss improved from 157.66148 to 156.32466, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 656us/sample - loss: 147.2922 - val_loss: 156.3247
Epoch 3/50
 50/121 [===========>..................] - ETA: 0s - loss: 142.8642
Epoch 00003: val_loss improved from 156.32466 to 152.63937, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 620us/sample - loss: 143.5768 - val_loss: 152.6394
Epoch 4/50
 50/121 [===========>..................] - ETA: 0s - loss: 141.9443
Epoch 00004: val_loss improved from 152.63937 to 143.37576, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 613us/sample - loss: 133.8484 - val_loss: 143.3758
Epoch 5/50
 50/121 [===========>..................] - ETA: 0s - loss: 118.9913
Epoch 00005: val_loss improved from 143.37576 to 122.72288, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 613us/sample - loss: 110.1046 - val_loss: 122.7229
Epoch 6/50
 50/121 [===========>..................] - ETA: 0s - loss: 78.7598
Epoch 00006: val_loss improved from 122.72288 to 82.17851, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 593us/sample - loss: 68.7903 - val_loss: 82.1785
Epoch 7/50
 50/121 [===========>..................] - ETA: 0s - loss: 53.6991
Epoch 00007: val_loss improved from 82.17851 to 60.15431, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 604us/sample - loss: 58.4568 - val_loss: 60.1543
Epoch 8/50
 50/121 [===========>..................] - ETA: 0s - loss: 58.6277
Epoch 00008: val_loss did not improve from 60.15431
121/121 [==============================] - 0s 413us/sample - loss: 54.1062 - val_loss: 71.8522
Epoch 9/50
 50/121 [===========>..................] - ETA: 0s - loss: 41.7262
Epoch 00009: val_loss did not improve from 60.15431

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 408us/sample - loss: 41.6274 - val_loss: 87.9622
Epoch 10/50
 50/121 [===========>..................] - ETA: 0s - loss: 46.1363
Epoch 00010: val_loss did not improve from 60.15431
121/121 [==============================] - 0s 401us/sample - loss: 45.0638 - val_loss: 87.8088
Epoch 11/50
 50/121 [===========>..................] - ETA: 0s - loss: 40.0041
Epoch 00011: val_loss did not improve from 60.15431

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 405us/sample - loss: 44.2718 - val_loss: 82.2158
Epoch 12/50
 50/121 [===========>..................] - ETA: 0s - loss: 40.8693
Epoch 00012: val_loss did not improve from 60.15431
121/121 [==============================] - 0s 400us/sample - loss: 41.4200 - val_loss: 77.9158
Epoch 13/50
 50/121 [===========>..................] - ETA: 0s - loss: 39.9410
Epoch 00013: val_loss did not improve from 60.15431

Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 402us/sample - loss: 39.9485 - val_loss: 73.1497
Epoch 14/50
 50/121 [===========>..................] - ETA: 0s - loss: 40.7876
Epoch 00014: val_loss did not improve from 60.15431
121/121 [==============================] - 0s 410us/sample - loss: 39.1663 - val_loss: 70.6659
Epoch 15/50
 50/121 [===========>..................] - ETA: 0s - loss: 37.7097
Epoch 00015: val_loss did not improve from 60.15431

Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 412us/sample - loss: 38.7940 - val_loss: 68.4733
Epoch 16/50
 50/121 [===========>..................] - ETA: 0s - loss: 40.3769
Epoch 00016: val_loss did not improve from 60.15431
121/121 [==============================] - 0s 400us/sample - loss: 38.6191 - val_loss: 67.0307
Epoch 17/50
 50/121 [===========>..................] - ETA: 0s - loss: 38.6678
Epoch 00017: val_loss did not improve from 60.15431

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 403us/sample - loss: 38.4463 - val_loss: 65.7428
Epoch 00017: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/50
 50/121 [===========>..................] - ETA: 1s - loss: 161.8433
Epoch 00001: val_loss improved from inf to 149.47768, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 8ms/sample - loss: 157.9437 - val_loss: 149.4777
Epoch 2/50
 50/121 [===========>..................] - ETA: 0s - loss: 151.8281
Epoch 00002: val_loss improved from 149.47768 to 148.81973, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 635us/sample - loss: 157.0970 - val_loss: 148.8197
Epoch 3/50
 50/121 [===========>..................] - ETA: 0s - loss: 160.8379
Epoch 00003: val_loss improved from 148.81973 to 147.21148, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 633us/sample - loss: 155.5066 - val_loss: 147.2115
Epoch 4/50
 50/121 [===========>..................] - ETA: 0s - loss: 149.8616
Epoch 00004: val_loss improved from 147.21148 to 143.47426, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 615us/sample - loss: 151.5680 - val_loss: 143.4743
Epoch 5/50
 50/121 [===========>..................] - ETA: 0s - loss: 137.6584
Epoch 00005: val_loss improved from 143.47426 to 135.39499, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 627us/sample - loss: 141.7163 - val_loss: 135.3950
Epoch 6/50
 50/121 [===========>..................] - ETA: 0s - loss: 124.0456
Epoch 00006: val_loss improved from 135.39499 to 118.23554, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 613us/sample - loss: 118.0991 - val_loss: 118.2355
Epoch 7/50
 50/121 [===========>..................] - ETA: 0s - loss: 87.9268
Epoch 00007: val_loss improved from 118.23554 to 84.44448, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 622us/sample - loss: 74.7879 - val_loss: 84.4445
Epoch 8/50
 50/121 [===========>..................] - ETA: 0s - loss: 49.4313
Epoch 00008: val_loss improved from 84.44448 to 60.94498, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 607us/sample - loss: 53.1756 - val_loss: 60.9450
Epoch 9/50
 50/121 [===========>..................] - ETA: 0s - loss: 62.3744
Epoch 00009: val_loss did not improve from 60.94498
121/121 [==============================] - 0s 405us/sample - loss: 58.8273 - val_loss: 70.2863
Epoch 10/50
 50/121 [===========>..................] - ETA: 0s - loss: 47.1784
Epoch 00010: val_loss did not improve from 60.94498

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 424us/sample - loss: 45.3381 - val_loss: 87.6374
Epoch 11/50
 50/121 [===========>..................] - ETA: 0s - loss: 45.8192
Epoch 00011: val_loss did not improve from 60.94498
121/121 [==============================] - 0s 402us/sample - loss: 44.6735 - val_loss: 90.1733
Epoch 12/50
 50/121 [===========>..................] - ETA: 0s - loss: 44.4363
Epoch 00012: val_loss did not improve from 60.94498

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 404us/sample - loss: 44.6030 - val_loss: 88.6476
Epoch 13/50
 50/121 [===========>..................] - ETA: 0s - loss: 46.0371
Epoch 00013: val_loss did not improve from 60.94498
121/121 [==============================] - 0s 405us/sample - loss: 42.5629 - val_loss: 86.1649
Epoch 14/50
 50/121 [===========>..................] - ETA: 0s - loss: 41.4140
Epoch 00014: val_loss did not improve from 60.94498

Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 406us/sample - loss: 40.9492 - val_loss: 83.1037
Epoch 15/50
 50/121 [===========>..................] - ETA: 0s - loss: 40.3764
Epoch 00015: val_loss did not improve from 60.94498
121/121 [==============================] - 0s 404us/sample - loss: 39.7555 - val_loss: 81.0878
Epoch 16/50
 50/121 [===========>..................] - ETA: 0s - loss: 39.6819
Epoch 00016: val_loss did not improve from 60.94498

Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 407us/sample - loss: 39.2604 - val_loss: 79.1070
Epoch 17/50
 50/121 [===========>..................] - ETA: 0s - loss: 38.6069
Epoch 00017: val_loss did not improve from 60.94498
121/121 [==============================] - 0s 411us/sample - loss: 39.1272 - val_loss: 77.6608
Epoch 18/50
 50/121 [===========>..................] - ETA: 0s - loss: 38.1683
Epoch 00018: val_loss did not improve from 60.94498

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 403us/sample - loss: 38.7594 - val_loss: 76.3425
Epoch 00018: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/50
 50/242 [=====>........................] - ETA: 2s - loss: 158.9488
Epoch 00001: val_loss improved from inf to 148.62353, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
242/242 [==============================] - 1s 5ms/sample - loss: 153.7769 - val_loss: 148.6235
Epoch 2/50
 50/242 [=====>........................] - ETA: 0s - loss: 160.7411
Epoch 00002: val_loss improved from 148.62353 to 146.20224, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 442us/sample - loss: 151.9592 - val_loss: 146.2022
Epoch 3/50
 50/242 [=====>........................] - ETA: 0s - loss: 147.9124
Epoch 00003: val_loss improved from 146.20224 to 134.49338, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 433us/sample - loss: 143.5341 - val_loss: 134.4934
Epoch 4/50
 50/242 [=====>........................] - ETA: 0s - loss: 133.1792
Epoch 00004: val_loss improved from 134.49338 to 88.43069, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 435us/sample - loss: 105.4907 - val_loss: 88.4307
Epoch 5/50
 50/242 [=====>........................] - ETA: 0s - loss: 66.8852
Epoch 00005: val_loss improved from 88.43069 to 52.48460, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 428us/sample - loss: 61.4621 - val_loss: 52.4846
Epoch 6/50
 50/242 [=====>........................] - ETA: 0s - loss: 63.3512
Epoch 00006: val_loss did not improve from 52.48460
242/242 [==============================] - 0s 322us/sample - loss: 54.3099 - val_loss: 65.7163
Epoch 7/50
 50/242 [=====>........................] - ETA: 0s - loss: 51.5813
Epoch 00007: val_loss did not improve from 52.48460

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 323us/sample - loss: 49.3732 - val_loss: 70.5364
Epoch 8/50
 50/242 [=====>........................] - ETA: 0s - loss: 44.9356
Epoch 00008: val_loss did not improve from 52.48460
242/242 [==============================] - 0s 324us/sample - loss: 46.3267 - val_loss: 62.1597
Epoch 9/50
 50/242 [=====>........................] - ETA: 0s - loss: 41.8238
Epoch 00009: val_loss did not improve from 52.48460

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 326us/sample - loss: 42.9466 - val_loss: 55.7734
Epoch 10/50
 50/242 [=====>........................] - ETA: 0s - loss: 42.0819
Epoch 00010: val_loss did not improve from 52.48460
242/242 [==============================] - 0s 325us/sample - loss: 42.7844 - val_loss: 54.9060
Epoch 11/50
 50/242 [=====>........................] - ETA: 0s - loss: 43.8990
Epoch 00011: val_loss did not improve from 52.48460

Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 322us/sample - loss: 42.5155 - val_loss: 55.6759
Epoch 12/50
 50/242 [=====>........................] - ETA: 0s - loss: 41.9738
Epoch 00012: val_loss did not improve from 52.48460
242/242 [==============================] - 0s 320us/sample - loss: 41.6710 - val_loss: 55.9208
Epoch 13/50
 50/242 [=====>........................] - ETA: 0s - loss: 45.4119
Epoch 00013: val_loss did not improve from 52.48460

Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 318us/sample - loss: 41.4598 - val_loss: 56.0954
Epoch 14/50
 50/242 [=====>........................] - ETA: 0s - loss: 39.3035
Epoch 00014: val_loss did not improve from 52.48460
242/242 [==============================] - 0s 312us/sample - loss: 41.1614 - val_loss: 55.7151
Epoch 15/50
 50/242 [=====>........................] - ETA: 0s - loss: 41.9103
Epoch 00015: val_loss did not improve from 52.48460

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 320us/sample - loss: 41.1045 - val_loss: 55.2647
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00015: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ad00d719c88> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ad00d70cc18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d70cf98> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00d7076d8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d70cb70> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d707a20> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00d730eb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d730b00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ad00d73a400> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7670b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d767ba8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d770908> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ad00d783588> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ad00d7834e0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ad00d78fef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d78fdd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d7a79e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7a7c50> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d7a7dd8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7b1ba8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ad00d7b1d30> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ad00d719c88> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ad00d70cc18> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d70cf98> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00d7076d8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d70cb70> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d707a20> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00d730eb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d730b00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ad00d73a400> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7670b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d767ba8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d770908> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ad00d783588> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ad00d7834e0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ad00d78fef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d78fdd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d7a79e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7a7c50> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00d7a7dd8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7b1ba8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ad00d7b1d30> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2acfa3f7a4a8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7ee4a8> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 125.0148
Epoch 00001: val_loss improved from inf to 65.38015, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 8ms/sample - loss: 99.7605 - val_loss: 65.3802
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 70.1632
Epoch 00002: val_loss improved from 65.38015 to 49.32521, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 594us/sample - loss: 55.8568 - val_loss: 49.3252
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.7536
Epoch 00003: val_loss did not improve from 49.32521
121/121 [==============================] - 0s 424us/sample - loss: 48.1882 - val_loss: 51.0565
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.9311
Epoch 00004: val_loss did not improve from 49.32521

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 422us/sample - loss: 48.9259 - val_loss: 50.1853
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 49.7304
Epoch 00005: val_loss improved from 49.32521 to 48.79398, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 563us/sample - loss: 47.8440 - val_loss: 48.7940
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 49.5796
Epoch 00006: val_loss improved from 48.79398 to 47.38188, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 46.7694 - val_loss: 47.3819
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.5418
Epoch 00007: val_loss improved from 47.38188 to 46.57391, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 572us/sample - loss: 45.9033 - val_loss: 46.5739
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.8895
Epoch 00008: val_loss improved from 46.57391 to 46.25323, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 45.5464 - val_loss: 46.2532
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0338
Epoch 00009: val_loss improved from 46.25323 to 46.11259, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 561us/sample - loss: 45.4327 - val_loss: 46.1126
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.9896
Epoch 00010: val_loss improved from 46.11259 to 46.06443, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 45.3390 - val_loss: 46.0644
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.8283
Epoch 00011: val_loss improved from 46.06443 to 45.99377, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 45.2321 - val_loss: 45.9938
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.4728
Epoch 00012: val_loss improved from 45.99377 to 45.95883, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 565us/sample - loss: 45.1328 - val_loss: 45.9588
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.9841
Epoch 00013: val_loss improved from 45.95883 to 45.86648, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 571us/sample - loss: 45.0543 - val_loss: 45.8665
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.0108
Epoch 00014: val_loss improved from 45.86648 to 45.71970, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 565us/sample - loss: 44.9600 - val_loss: 45.7197
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.9929
Epoch 00015: val_loss improved from 45.71970 to 45.63719, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 44.9173 - val_loss: 45.6372
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.3137
Epoch 00016: val_loss improved from 45.63719 to 45.59752, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 558us/sample - loss: 44.8309 - val_loss: 45.5975
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.8915
Epoch 00017: val_loss did not improve from 45.59752
121/121 [==============================] - 0s 431us/sample - loss: 44.7358 - val_loss: 45.6297
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.2738
Epoch 00018: val_loss improved from 45.59752 to 45.47720, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 844us/sample - loss: 44.6727 - val_loss: 45.4772
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.8720
Epoch 00019: val_loss improved from 45.47720 to 45.36071, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 563us/sample - loss: 44.6059 - val_loss: 45.3607
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.7617
Epoch 00020: val_loss did not improve from 45.36071
121/121 [==============================] - 0s 422us/sample - loss: 44.5084 - val_loss: 45.3703
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.0112
Epoch 00021: val_loss improved from 45.36071 to 45.34092, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 560us/sample - loss: 44.4571 - val_loss: 45.3409
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.1673
Epoch 00022: val_loss improved from 45.34092 to 45.19134, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 44.3650 - val_loss: 45.1913
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.4509
Epoch 00023: val_loss improved from 45.19134 to 45.08564, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 560us/sample - loss: 44.2757 - val_loss: 45.0856
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0477
Epoch 00024: val_loss improved from 45.08564 to 44.97898, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 565us/sample - loss: 44.2058 - val_loss: 44.9790
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.5424
Epoch 00025: val_loss did not improve from 44.97898
121/121 [==============================] - 0s 429us/sample - loss: 44.1312 - val_loss: 45.0062
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0844
Epoch 00026: val_loss did not improve from 44.97898

Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 424us/sample - loss: 44.1069 - val_loss: 45.0249
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 37.8968
Epoch 00027: val_loss improved from 44.97898 to 44.85180, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 558us/sample - loss: 44.0107 - val_loss: 44.8518
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.5714
Epoch 00028: val_loss improved from 44.85180 to 44.78612, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 563us/sample - loss: 43.9655 - val_loss: 44.7861
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.7052
Epoch 00029: val_loss improved from 44.78612 to 44.75974, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 556us/sample - loss: 43.9985 - val_loss: 44.7597
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.9320
Epoch 00030: val_loss did not improve from 44.75974
121/121 [==============================] - 0s 423us/sample - loss: 43.9154 - val_loss: 44.8436
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.3630
Epoch 00031: val_loss did not improve from 44.75974

Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 422us/sample - loss: 43.9145 - val_loss: 45.0292
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 50.9531
Epoch 00032: val_loss did not improve from 44.75974
121/121 [==============================] - 0s 418us/sample - loss: 43.9398 - val_loss: 44.9230
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.8736
Epoch 00033: val_loss did not improve from 44.75974

Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 426us/sample - loss: 43.8819 - val_loss: 44.7861
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.5181
Epoch 00034: val_loss improved from 44.75974 to 44.75709, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 43.8521 - val_loss: 44.7571
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.4961
Epoch 00035: val_loss improved from 44.75709 to 44.75589, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 43.8455 - val_loss: 44.7559
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0522
Epoch 00036: val_loss improved from 44.75589 to 44.74597, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 43.8421 - val_loss: 44.7460
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.7573
Epoch 00037: val_loss improved from 44.74597 to 44.74553, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 43.8378 - val_loss: 44.7455
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.9737
Epoch 00038: val_loss did not improve from 44.74553
121/121 [==============================] - 0s 425us/sample - loss: 43.8330 - val_loss: 44.7564
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.2890
Epoch 00039: val_loss did not improve from 44.74553

Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 425us/sample - loss: 43.8295 - val_loss: 44.7576
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.4775
Epoch 00040: val_loss did not improve from 44.74553
121/121 [==============================] - 0s 421us/sample - loss: 43.8265 - val_loss: 44.7576
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.9216
Epoch 00041: val_loss did not improve from 44.74553

Epoch 00041: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 424us/sample - loss: 43.8247 - val_loss: 44.7547
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.4213
Epoch 00042: val_loss did not improve from 44.74553
121/121 [==============================] - 0s 426us/sample - loss: 43.8229 - val_loss: 44.7580
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.3880
Epoch 00043: val_loss did not improve from 44.74553
121/121 [==============================] - 0s 422us/sample - loss: 43.8218 - val_loss: 44.7533
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.8495
Epoch 00044: val_loss did not improve from 44.74553
121/121 [==============================] - 0s 423us/sample - loss: 43.8190 - val_loss: 44.7486
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.9355
Epoch 00045: val_loss improved from 44.74553 to 44.74274, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 43.8165 - val_loss: 44.7427
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.7563
Epoch 00046: val_loss improved from 44.74274 to 44.73805, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 43.8145 - val_loss: 44.7380
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.7891
Epoch 00047: val_loss improved from 44.73805 to 44.73302, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 43.8123 - val_loss: 44.7330
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.5865
Epoch 00048: val_loss improved from 44.73302 to 44.72788, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 43.8098 - val_loss: 44.7279
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.8718
Epoch 00049: val_loss improved from 44.72788 to 44.72341, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 43.8080 - val_loss: 44.7234
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 38.5962
Epoch 00050: val_loss improved from 44.72341 to 44.71936, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 565us/sample - loss: 43.8062 - val_loss: 44.7194
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.7007
Epoch 00051: val_loss improved from 44.71936 to 44.71628, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 572us/sample - loss: 43.8042 - val_loss: 44.7163
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.5533
Epoch 00052: val_loss improved from 44.71628 to 44.71512, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 583us/sample - loss: 43.8027 - val_loss: 44.7151
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.2334
Epoch 00053: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 429us/sample - loss: 43.8010 - val_loss: 44.7184
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.0947
Epoch 00054: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 420us/sample - loss: 43.8009 - val_loss: 44.7261
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.1902
Epoch 00055: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 421us/sample - loss: 43.7992 - val_loss: 44.7287
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.8889
Epoch 00056: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 421us/sample - loss: 43.7977 - val_loss: 44.7336
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.6729
Epoch 00057: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 419us/sample - loss: 43.7980 - val_loss: 44.7381
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.4552
Epoch 00058: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 419us/sample - loss: 43.7964 - val_loss: 44.7357
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.2740
Epoch 00059: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 420us/sample - loss: 43.7941 - val_loss: 44.7278
Epoch 60/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.8708
Epoch 00060: val_loss did not improve from 44.71512
121/121 [==============================] - 0s 420us/sample - loss: 43.7913 - val_loss: 44.7170
Epoch 61/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.1809
Epoch 00061: val_loss improved from 44.71512 to 44.70830, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 577us/sample - loss: 43.7896 - val_loss: 44.7083
Epoch 62/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.8783
Epoch 00062: val_loss improved from 44.70830 to 44.70814, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 578us/sample - loss: 43.7864 - val_loss: 44.7081
Epoch 63/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0987
Epoch 00063: val_loss improved from 44.70814 to 44.70615, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 572us/sample - loss: 43.7847 - val_loss: 44.7062
Epoch 64/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.4912
Epoch 00064: val_loss improved from 44.70615 to 44.70380, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 43.7825 - val_loss: 44.7038
Epoch 65/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.1779
Epoch 00065: val_loss improved from 44.70380 to 44.69978, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 571us/sample - loss: 43.7809 - val_loss: 44.6998
Epoch 66/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.1025
Epoch 00066: val_loss improved from 44.69978 to 44.69377, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 43.7784 - val_loss: 44.6938
Epoch 67/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.0298
Epoch 00067: val_loss improved from 44.69377 to 44.68730, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 43.7770 - val_loss: 44.6873
Epoch 68/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.2567
Epoch 00068: val_loss improved from 44.68730 to 44.68268, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 43.7749 - val_loss: 44.6827
Epoch 69/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.2605
Epoch 00069: val_loss improved from 44.68268 to 44.67753, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 575us/sample - loss: 43.7732 - val_loss: 44.6775
Epoch 70/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.5366
Epoch 00070: val_loss improved from 44.67753 to 44.67038, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 43.7720 - val_loss: 44.6704
Epoch 71/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.7958
Epoch 00071: val_loss improved from 44.67038 to 44.66803, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 43.7703 - val_loss: 44.6680
Epoch 72/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.9508
Epoch 00072: val_loss improved from 44.66803 to 44.66709, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 43.7686 - val_loss: 44.6671
Epoch 73/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.8513
Epoch 00073: val_loss improved from 44.66709 to 44.66564, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 588us/sample - loss: 43.7669 - val_loss: 44.6656
Epoch 74/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.5345
Epoch 00074: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 424us/sample - loss: 43.7661 - val_loss: 44.6714
Epoch 75/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.9033
Epoch 00075: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 421us/sample - loss: 43.7642 - val_loss: 44.6744
Epoch 76/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.2594
Epoch 00076: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 420us/sample - loss: 43.7615 - val_loss: 44.6732
Epoch 77/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.0820
Epoch 00077: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 441us/sample - loss: 43.7599 - val_loss: 44.6733
Epoch 78/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.3591
Epoch 00078: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 419us/sample - loss: 43.7582 - val_loss: 44.6752
Epoch 79/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.2083
Epoch 00079: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 419us/sample - loss: 43.7568 - val_loss: 44.6731
Epoch 80/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.3175
Epoch 00080: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 421us/sample - loss: 43.7552 - val_loss: 44.6714
Epoch 81/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.1981
Epoch 00081: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 422us/sample - loss: 43.7544 - val_loss: 44.6763
Epoch 82/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.3509
Epoch 00082: val_loss did not improve from 44.66564
121/121 [==============================] - 0s 422us/sample - loss: 43.7525 - val_loss: 44.6730
Epoch 83/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.7349
Epoch 00083: val_loss improved from 44.66564 to 44.66557, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 575us/sample - loss: 43.7503 - val_loss: 44.6656
Epoch 84/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.3785
Epoch 00084: val_loss improved from 44.66557 to 44.65689, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 43.7480 - val_loss: 44.6569
Epoch 85/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.6320
Epoch 00085: val_loss improved from 44.65689 to 44.64917, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 586us/sample - loss: 43.7473 - val_loss: 44.6492
Epoch 86/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.9764
Epoch 00086: val_loss improved from 44.64917 to 44.64359, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 43.7452 - val_loss: 44.6436
Epoch 87/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.6618
Epoch 00087: val_loss improved from 44.64359 to 44.64173, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 592us/sample - loss: 43.7438 - val_loss: 44.6417
Epoch 88/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.7561
Epoch 00088: val_loss improved from 44.64173 to 44.63884, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 580us/sample - loss: 43.7421 - val_loss: 44.6388
Epoch 89/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.7875
Epoch 00089: val_loss improved from 44.63884 to 44.63553, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 43.7406 - val_loss: 44.6355
Epoch 90/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.9148
Epoch 00090: val_loss improved from 44.63553 to 44.63522, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 586us/sample - loss: 43.7390 - val_loss: 44.6352
Epoch 91/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.9525
Epoch 00091: val_loss did not improve from 44.63522
121/121 [==============================] - 0s 425us/sample - loss: 43.7373 - val_loss: 44.6359
Epoch 92/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.4903
Epoch 00092: val_loss did not improve from 44.63522
121/121 [==============================] - 0s 420us/sample - loss: 43.7359 - val_loss: 44.6372
Epoch 93/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.0751
Epoch 00093: val_loss improved from 44.63522 to 44.63457, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 587us/sample - loss: 43.7334 - val_loss: 44.6346
Epoch 94/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.3911
Epoch 00094: val_loss improved from 44.63457 to 44.63228, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 581us/sample - loss: 43.7317 - val_loss: 44.6323
Epoch 95/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6888
Epoch 00095: val_loss improved from 44.63228 to 44.63209, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 43.7301 - val_loss: 44.6321
Epoch 96/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.8550
Epoch 00096: val_loss did not improve from 44.63209
121/121 [==============================] - 0s 425us/sample - loss: 43.7280 - val_loss: 44.6345
Epoch 97/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.1760
Epoch 00097: val_loss did not improve from 44.63209
121/121 [==============================] - 0s 421us/sample - loss: 43.7265 - val_loss: 44.6323
Epoch 98/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6248
Epoch 00098: val_loss improved from 44.63209 to 44.62806, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 43.7246 - val_loss: 44.6281
Epoch 99/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.0029
Epoch 00099: val_loss improved from 44.62806 to 44.62770, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 571us/sample - loss: 43.7229 - val_loss: 44.6277
Epoch 100/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.3790
Epoch 00100: val_loss improved from 44.62770 to 44.62757, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 43.7211 - val_loss: 44.6276
Epoch 101/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.2090
Epoch 00101: val_loss improved from 44.62757 to 44.62319, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 599us/sample - loss: 43.7195 - val_loss: 44.6232
Epoch 102/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.7488
Epoch 00102: val_loss improved from 44.62319 to 44.62277, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 43.7176 - val_loss: 44.6228
Epoch 103/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.5098
Epoch 00103: val_loss improved from 44.62277 to 44.61838, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 43.7159 - val_loss: 44.6184
Epoch 104/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.9612
Epoch 00104: val_loss improved from 44.61838 to 44.61627, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 575us/sample - loss: 43.7143 - val_loss: 44.6163
Epoch 105/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.3632
Epoch 00105: val_loss improved from 44.61627 to 44.61069, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 577us/sample - loss: 43.7129 - val_loss: 44.6107
Epoch 106/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.6965
Epoch 00106: val_loss improved from 44.61069 to 44.60629, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 43.7111 - val_loss: 44.6063
Epoch 107/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.7041
Epoch 00107: val_loss improved from 44.60629 to 44.60043, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 577us/sample - loss: 43.7098 - val_loss: 44.6004
Epoch 108/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6379
Epoch 00108: val_loss improved from 44.60043 to 44.59695, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 43.7079 - val_loss: 44.5970
Epoch 109/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.1560
Epoch 00109: val_loss improved from 44.59695 to 44.59453, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 43.7066 - val_loss: 44.5945
Epoch 110/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.4973
Epoch 00110: val_loss improved from 44.59453 to 44.59209, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 43.7048 - val_loss: 44.5921
Epoch 111/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.7728
Epoch 00111: val_loss improved from 44.59209 to 44.58848, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 43.7034 - val_loss: 44.5885
Epoch 112/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.6183
Epoch 00112: val_loss improved from 44.58848 to 44.58828, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 43.7014 - val_loss: 44.5883
Epoch 113/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.5651
Epoch 00113: val_loss improved from 44.58828 to 44.58823, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 43.6991 - val_loss: 44.5882
Epoch 114/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.1578
Epoch 00114: val_loss improved from 44.58823 to 44.58783, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 43.6970 - val_loss: 44.5878
Epoch 115/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6752
Epoch 00115: val_loss did not improve from 44.58783
121/121 [==============================] - 0s 424us/sample - loss: 43.6951 - val_loss: 44.5888
Epoch 116/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.3212
Epoch 00116: val_loss did not improve from 44.58783
121/121 [==============================] - 0s 421us/sample - loss: 43.6934 - val_loss: 44.5883
Epoch 117/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.7057
Epoch 00117: val_loss improved from 44.58783 to 44.58435, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 575us/sample - loss: 43.6912 - val_loss: 44.5843
Epoch 118/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.7650
Epoch 00118: val_loss improved from 44.58435 to 44.57815, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 43.6896 - val_loss: 44.5781
Epoch 119/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.8607
Epoch 00119: val_loss improved from 44.57815 to 44.57100, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 43.6886 - val_loss: 44.5710
Epoch 120/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.9102
Epoch 00120: val_loss improved from 44.57100 to 44.56673, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 564us/sample - loss: 43.6878 - val_loss: 44.5667
Epoch 121/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.9135
Epoch 00121: val_loss improved from 44.56673 to 44.56388, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 43.6869 - val_loss: 44.5639
Epoch 122/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.5535
Epoch 00122: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 424us/sample - loss: 43.6852 - val_loss: 44.5671
Epoch 123/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6776
Epoch 00123: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 422us/sample - loss: 43.6820 - val_loss: 44.5646
Epoch 124/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.7768
Epoch 00124: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 422us/sample - loss: 43.6798 - val_loss: 44.5665
Epoch 125/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.7718
Epoch 00125: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 421us/sample - loss: 43.6760 - val_loss: 44.5746
Epoch 126/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.9289
Epoch 00126: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 420us/sample - loss: 43.6748 - val_loss: 44.5846
Epoch 127/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.0598
Epoch 00127: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 419us/sample - loss: 43.6721 - val_loss: 44.5844
Epoch 128/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.9544
Epoch 00128: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 420us/sample - loss: 43.6703 - val_loss: 44.5807
Epoch 129/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.3105
Epoch 00129: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 422us/sample - loss: 43.6686 - val_loss: 44.5822
Epoch 130/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.4401
Epoch 00130: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 422us/sample - loss: 43.6677 - val_loss: 44.5851
Epoch 131/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.0767
Epoch 00131: val_loss did not improve from 44.56388
121/121 [==============================] - 0s 424us/sample - loss: 43.6652 - val_loss: 44.5799
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00131: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ad00df0bef0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ad00df024e0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df02978> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00df025c0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00df2ce48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df2c710> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00df4fba8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00df4fb00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ad00df580f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df7de48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df87898> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df8f5f8> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ad00df9ae80> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ad00dfa01d0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ad00dfadf60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00dfada90> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00dfc66d8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00dfc6940> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00dfc6ac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00dfd0898> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ad00dfd0a20> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ad00df0bef0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ad00df024e0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df02978> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00df025c0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00df2ce48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df2c710> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00df4fba8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00df4fb00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ad00df580f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df7de48> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df87898> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00df8f5f8> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ad00df9ae80> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ad00dfa01d0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ad00dfadf60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00dfada90> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00dfc66d8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00dfc6940> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00dfc6ac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00dfd0898> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ad00dfd0a20> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2acfe3f87908> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e017518> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 2s - loss: 155.7787
Epoch 00001: val_loss improved from inf to 111.46426, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 6ms/sample - loss: 146.9054 - val_loss: 111.4643
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 112.8476
Epoch 00002: val_loss improved from 111.46426 to 71.00087, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 590us/sample - loss: 99.0658 - val_loss: 71.0009
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 68.5711
Epoch 00003: val_loss improved from 71.00087 to 48.88323, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 572us/sample - loss: 60.6762 - val_loss: 48.8832
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.4499
Epoch 00004: val_loss improved from 48.88323 to 47.50101, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 564us/sample - loss: 46.2792 - val_loss: 47.5010
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.1581
Epoch 00005: val_loss did not improve from 47.50101
121/121 [==============================] - 0s 423us/sample - loss: 48.3984 - val_loss: 48.0781
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.1356
Epoch 00006: val_loss improved from 47.50101 to 46.45556, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 47.5042 - val_loss: 46.4556
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.1732
Epoch 00007: val_loss improved from 46.45556 to 46.07636, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 45.5668 - val_loss: 46.0764
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.3986
Epoch 00008: val_loss did not improve from 46.07636
121/121 [==============================] - 0s 429us/sample - loss: 45.1542 - val_loss: 46.3810
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.4816
Epoch 00009: val_loss did not improve from 46.07636

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 426us/sample - loss: 45.1943 - val_loss: 46.2833
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.1397
Epoch 00010: val_loss improved from 46.07636 to 45.93233, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 572us/sample - loss: 44.9904 - val_loss: 45.9323
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.1060
Epoch 00011: val_loss improved from 45.93233 to 45.66212, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 44.7854 - val_loss: 45.6621
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.7638
Epoch 00012: val_loss improved from 45.66212 to 45.62185, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 583us/sample - loss: 44.7499 - val_loss: 45.6218
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.3126
Epoch 00013: val_loss did not improve from 45.62185
121/121 [==============================] - 0s 428us/sample - loss: 44.6975 - val_loss: 45.6352
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.8007
Epoch 00014: val_loss did not improve from 45.62185

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 426us/sample - loss: 44.6322 - val_loss: 45.6407
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.6212
Epoch 00015: val_loss improved from 45.62185 to 45.61419, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 599us/sample - loss: 44.5963 - val_loss: 45.6142
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.5087
Epoch 00016: val_loss improved from 45.61419 to 45.57090, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 560us/sample - loss: 44.5725 - val_loss: 45.5709
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.0148
Epoch 00017: val_loss improved from 45.57090 to 45.53320, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 580us/sample - loss: 44.5533 - val_loss: 45.5332
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.5545
Epoch 00018: val_loss improved from 45.53320 to 45.49632, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 574us/sample - loss: 44.5349 - val_loss: 45.4963
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.2227
Epoch 00019: val_loss improved from 45.49632 to 45.46464, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 570us/sample - loss: 44.5142 - val_loss: 45.4646
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.6866
Epoch 00020: val_loss improved from 45.46464 to 45.43516, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 572us/sample - loss: 44.4964 - val_loss: 45.4352
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.6611
Epoch 00021: val_loss improved from 45.43516 to 45.41778, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 44.4811 - val_loss: 45.4178
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.6461
Epoch 00022: val_loss improved from 45.41778 to 45.40667, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 44.4635 - val_loss: 45.4067
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6568
Epoch 00023: val_loss did not improve from 45.40667
121/121 [==============================] - 0s 427us/sample - loss: 44.4469 - val_loss: 45.4227
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.8437
Epoch 00024: val_loss did not improve from 45.40667

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 425us/sample - loss: 44.4170 - val_loss: 45.4127
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.4536
Epoch 00025: val_loss did not improve from 45.40667
121/121 [==============================] - 0s 418us/sample - loss: 44.4053 - val_loss: 45.4096
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.0577
Epoch 00026: val_loss improved from 45.40667 to 45.40338, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 44.3969 - val_loss: 45.4034
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.4959
Epoch 00027: val_loss improved from 45.40338 to 45.38450, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 44.3892 - val_loss: 45.3845
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.6482
Epoch 00028: val_loss improved from 45.38450 to 45.38028, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 571us/sample - loss: 44.3807 - val_loss: 45.3803
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.7380
Epoch 00029: val_loss improved from 45.38028 to 45.37247, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 44.3742 - val_loss: 45.3725
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.8407
Epoch 00030: val_loss improved from 45.37247 to 45.36634, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 44.3661 - val_loss: 45.3663
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.3112
Epoch 00031: val_loss did not improve from 45.36634
121/121 [==============================] - 0s 426us/sample - loss: 44.3630 - val_loss: 45.3718
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.7076
Epoch 00032: val_loss improved from 45.36634 to 45.36611, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 44.3618 - val_loss: 45.3661
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.7866
Epoch 00033: val_loss improved from 45.36611 to 45.33042, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 44.3504 - val_loss: 45.3304
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.9037
Epoch 00034: val_loss improved from 45.33042 to 45.31233, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 564us/sample - loss: 44.3392 - val_loss: 45.3123
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6809
Epoch 00035: val_loss improved from 45.31233 to 45.29710, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 44.3310 - val_loss: 45.2971
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.1482
Epoch 00036: val_loss improved from 45.29710 to 45.28511, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 44.3255 - val_loss: 45.2851
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.1751
Epoch 00037: val_loss did not improve from 45.28511
121/121 [==============================] - 0s 424us/sample - loss: 44.3165 - val_loss: 45.2916
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.1596
Epoch 00038: val_loss did not improve from 45.28511

Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 427us/sample - loss: 44.3089 - val_loss: 45.2860
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.7656
Epoch 00039: val_loss improved from 45.28511 to 45.28367, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 575us/sample - loss: 44.3036 - val_loss: 45.2837
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 43.6097
Epoch 00040: val_loss improved from 45.28367 to 45.27972, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 44.3006 - val_loss: 45.2797
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.0193
Epoch 00041: val_loss improved from 45.27972 to 45.27571, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 560us/sample - loss: 44.2970 - val_loss: 45.2757
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.9794
Epoch 00042: val_loss improved from 45.27571 to 45.26838, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 563us/sample - loss: 44.2947 - val_loss: 45.2684
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.6955
Epoch 00043: val_loss improved from 45.26838 to 45.26596, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 564us/sample - loss: 44.2909 - val_loss: 45.2660
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.5139
Epoch 00044: val_loss improved from 45.26596 to 45.26373, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 44.2880 - val_loss: 45.2637
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.1253
Epoch 00045: val_loss improved from 45.26373 to 45.25921, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 563us/sample - loss: 44.2851 - val_loss: 45.2592
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 50.9081
Epoch 00046: val_loss improved from 45.25921 to 45.25482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 560us/sample - loss: 44.2822 - val_loss: 45.2548
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.8756
Epoch 00047: val_loss improved from 45.25482 to 45.24630, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 44.2788 - val_loss: 45.2463
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.5471
Epoch 00048: val_loss improved from 45.24630 to 45.23596, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 44.2769 - val_loss: 45.2360
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.7373
Epoch 00049: val_loss improved from 45.23596 to 45.22674, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 44.2740 - val_loss: 45.2267
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.5241
Epoch 00050: val_loss improved from 45.22674 to 45.22247, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 562us/sample - loss: 44.2734 - val_loss: 45.2225
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0232
Epoch 00051: val_loss improved from 45.22247 to 45.21771, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 561us/sample - loss: 44.2706 - val_loss: 45.2177
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.9576
Epoch 00052: val_loss improved from 45.21771 to 45.20668, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 579us/sample - loss: 44.2689 - val_loss: 45.2067
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.4703
Epoch 00053: val_loss improved from 45.20668 to 45.19988, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 575us/sample - loss: 44.2670 - val_loss: 45.1999
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.4405
Epoch 00054: val_loss improved from 45.19988 to 45.19569, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 44.2658 - val_loss: 45.1957
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.1760
Epoch 00055: val_loss improved from 45.19569 to 45.19337, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 576us/sample - loss: 44.2629 - val_loss: 45.1934
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 41.6447
Epoch 00056: val_loss improved from 45.19337 to 45.18980, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 566us/sample - loss: 44.2589 - val_loss: 45.1898
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.2875
Epoch 00057: val_loss improved from 45.18980 to 45.18667, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 44.2552 - val_loss: 45.1867
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.7510
Epoch 00058: val_loss improved from 45.18667 to 45.18258, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 569us/sample - loss: 44.2522 - val_loss: 45.1826
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.2988
Epoch 00059: val_loss improved from 45.18258 to 45.17716, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 567us/sample - loss: 44.2485 - val_loss: 45.1772
Epoch 60/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.5475
Epoch 00060: val_loss improved from 45.17716 to 45.17698, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 44.2441 - val_loss: 45.1770
Epoch 61/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.8273
Epoch 00061: val_loss improved from 45.17698 to 45.17405, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 568us/sample - loss: 44.2404 - val_loss: 45.1741
Epoch 62/200
 20/121 [===>..........................] - ETA: 0s - loss: 47.5675
Epoch 00062: val_loss improved from 45.17405 to 45.17086, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 571us/sample - loss: 44.2377 - val_loss: 45.1709
Epoch 63/200
 20/121 [===>..........................] - ETA: 0s - loss: 48.0767
Epoch 00063: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 427us/sample - loss: 44.2342 - val_loss: 45.1753
Epoch 64/200
 20/121 [===>..........................] - ETA: 0s - loss: 39.3306
Epoch 00064: val_loss did not improve from 45.17086

Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 442us/sample - loss: 44.2310 - val_loss: 45.1818
Epoch 65/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.0120
Epoch 00065: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 420us/sample - loss: 44.2267 - val_loss: 45.1814
Epoch 66/200
 20/121 [===>..........................] - ETA: 0s - loss: 45.2189
Epoch 00066: val_loss did not improve from 45.17086

Epoch 00066: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 422us/sample - loss: 44.2257 - val_loss: 45.1806
Epoch 67/200
 20/121 [===>..........................] - ETA: 0s - loss: 40.0471
Epoch 00067: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 419us/sample - loss: 44.2246 - val_loss: 45.1808
Epoch 68/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.6311
Epoch 00068: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 420us/sample - loss: 44.2233 - val_loss: 45.1834
Epoch 69/200
 20/121 [===>..........................] - ETA: 0s - loss: 37.7442
Epoch 00069: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 422us/sample - loss: 44.2223 - val_loss: 45.1863
Epoch 70/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.1584
Epoch 00070: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 423us/sample - loss: 44.2206 - val_loss: 45.1859
Epoch 71/200
 20/121 [===>..........................] - ETA: 0s - loss: 44.1177
Epoch 00071: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 420us/sample - loss: 44.2194 - val_loss: 45.1849
Epoch 72/200
 20/121 [===>..........................] - ETA: 0s - loss: 42.4895
Epoch 00072: val_loss did not improve from 45.17086
121/121 [==============================] - 0s 421us/sample - loss: 44.2184 - val_loss: 45.1839
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00072: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ad00e319a20> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ad00e329828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e329c18> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00e329dd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e32fd68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e32f630> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00e746c18> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e746b70> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ad00e74e160> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e771eb8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e77c908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e784668> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ad00e78fef0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ad00e79a240> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ad00e7a2fd0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7a2b00> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e7bd748> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7bd9b0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e7bdb38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7c8908> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ad00e7c8a90> False
Model: "model_8"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_5[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 57, 2)    578         conv2d_4[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ad00e319a20> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ad00e329828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e329c18> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00e329dd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e32fd68> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e32f630> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ad00e746c18> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e746b70> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ad00e74e160> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e771eb8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e77c908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e784668> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ad00e78fef0> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ad00e79a240> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ad00e7a2fd0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7a2b00> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e7bd748> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7bd9b0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ad00e7bdb38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7c8908> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ad00e7c8a90> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00d7c3ac8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ad00e7fefd0> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/200
 20/242 [=>............................] - ETA: 8s - loss: 181.0931
Epoch 00001: val_loss improved from inf to 87.34733, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 1s 5ms/sample - loss: 141.4666 - val_loss: 87.3473
Epoch 2/200
 20/242 [=>............................] - ETA: 0s - loss: 87.0849
Epoch 00002: val_loss improved from 87.34733 to 51.80565, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 402us/sample - loss: 62.8537 - val_loss: 51.8057
Epoch 3/200
 20/242 [=>............................] - ETA: 0s - loss: 53.2735
Epoch 00003: val_loss improved from 51.80565 to 51.14354, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 389us/sample - loss: 54.7530 - val_loss: 51.1435
Epoch 4/200
 20/242 [=>............................] - ETA: 0s - loss: 50.3348
Epoch 00004: val_loss did not improve from 51.14354
242/242 [==============================] - 0s 315us/sample - loss: 51.4106 - val_loss: 51.8030
Epoch 5/200
 20/242 [=>............................] - ETA: 0s - loss: 56.9759
Epoch 00005: val_loss improved from 51.14354 to 50.44439, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 436us/sample - loss: 51.0310 - val_loss: 50.4444
Epoch 6/200
 20/242 [=>............................] - ETA: 0s - loss: 51.9925
Epoch 00006: val_loss improved from 50.44439 to 50.31939, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 392us/sample - loss: 50.4837 - val_loss: 50.3194
Epoch 7/200
 20/242 [=>............................] - ETA: 0s - loss: 58.5519
Epoch 00007: val_loss improved from 50.31939 to 50.14807, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 387us/sample - loss: 50.0106 - val_loss: 50.1481
Epoch 8/200
 20/242 [=>............................] - ETA: 0s - loss: 50.3196
Epoch 00008: val_loss improved from 50.14807 to 49.47866, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 382us/sample - loss: 49.5050 - val_loss: 49.4787
Epoch 9/200
 20/242 [=>............................] - ETA: 0s - loss: 46.4281
Epoch 00009: val_loss improved from 49.47866 to 49.17938, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 384us/sample - loss: 49.1200 - val_loss: 49.1794
Epoch 10/200
 20/242 [=>............................] - ETA: 0s - loss: 48.2115
Epoch 00010: val_loss improved from 49.17938 to 48.71937, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 394us/sample - loss: 48.6831 - val_loss: 48.7194
Epoch 11/200
 20/242 [=>............................] - ETA: 0s - loss: 43.3183
Epoch 00011: val_loss improved from 48.71937 to 48.11084, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 398us/sample - loss: 48.3892 - val_loss: 48.1108
Epoch 12/200
 20/242 [=>............................] - ETA: 0s - loss: 43.9511
Epoch 00012: val_loss improved from 48.11084 to 47.87858, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 389us/sample - loss: 48.1325 - val_loss: 47.8786
Epoch 13/200
 20/242 [=>............................] - ETA: 0s - loss: 48.2242
Epoch 00013: val_loss improved from 47.87858 to 47.50452, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 386us/sample - loss: 47.9517 - val_loss: 47.5045
Epoch 14/200
 20/242 [=>............................] - ETA: 0s - loss: 48.8637
Epoch 00014: val_loss improved from 47.50452 to 47.47698, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 390us/sample - loss: 47.7565 - val_loss: 47.4770
Epoch 15/200
 20/242 [=>............................] - ETA: 0s - loss: 44.8134
Epoch 00015: val_loss improved from 47.47698 to 47.33464, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 387us/sample - loss: 47.5706 - val_loss: 47.3346
Epoch 16/200
 20/242 [=>............................] - ETA: 0s - loss: 44.5625
Epoch 00016: val_loss did not improve from 47.33464
242/242 [==============================] - 0s 314us/sample - loss: 47.4542 - val_loss: 47.8457
Epoch 17/200
 20/242 [=>............................] - ETA: 0s - loss: 44.5234
Epoch 00017: val_loss improved from 47.33464 to 47.31381, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 386us/sample - loss: 47.3956 - val_loss: 47.3138
Epoch 18/200
 20/242 [=>............................] - ETA: 0s - loss: 45.3487
Epoch 00018: val_loss improved from 47.31381 to 47.31259, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 393us/sample - loss: 47.3448 - val_loss: 47.3126
Epoch 19/200
 20/242 [=>............................] - ETA: 0s - loss: 51.1986
Epoch 00019: val_loss did not improve from 47.31259
242/242 [==============================] - 0s 318us/sample - loss: 47.3271 - val_loss: 47.5166
Epoch 20/200
 20/242 [=>............................] - ETA: 0s - loss: 52.6075
Epoch 00020: val_loss did not improve from 47.31259

Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 312us/sample - loss: 47.2425 - val_loss: 47.3528
Epoch 21/200
 20/242 [=>............................] - ETA: 0s - loss: 47.0450
Epoch 00021: val_loss improved from 47.31259 to 47.20771, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 384us/sample - loss: 47.1612 - val_loss: 47.2077
Epoch 22/200
 20/242 [=>............................] - ETA: 0s - loss: 53.0458
Epoch 00022: val_loss improved from 47.20771 to 47.03050, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 387us/sample - loss: 47.1766 - val_loss: 47.0305
Epoch 23/200
 20/242 [=>............................] - ETA: 0s - loss: 46.0076
Epoch 00023: val_loss did not improve from 47.03050
242/242 [==============================] - 0s 315us/sample - loss: 47.0998 - val_loss: 47.0641
Epoch 24/200
 20/242 [=>............................] - ETA: 0s - loss: 45.2958
Epoch 00024: val_loss improved from 47.03050 to 47.02255, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 396us/sample - loss: 47.0853 - val_loss: 47.0225
Epoch 25/200
 20/242 [=>............................] - ETA: 0s - loss: 40.0670
Epoch 00025: val_loss did not improve from 47.02255
242/242 [==============================] - 0s 315us/sample - loss: 47.0856 - val_loss: 47.0689
Epoch 26/200
 20/242 [=>............................] - ETA: 0s - loss: 44.4249
Epoch 00026: val_loss improved from 47.02255 to 46.83665, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2016/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 388us/sample - loss: 47.1304 - val_loss: 46.8367
Epoch 27/200
 20/242 [=>............................] - ETA: 0s - loss: 49.3363
Epoch 00027: val_loss did not improve from 46.83665
242/242 [==============================] - 0s 312us/sample - loss: 47.0584 - val_loss: 46.9618
Epoch 28/200
 20/242 [=>............................] - ETA: 0s - loss: 41.8327
Epoch 00028: val_loss did not improve from 46.83665

Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 311us/sample - loss: 47.0654 - val_loss: 47.1086
Epoch 29/200
 20/242 [=>............................] - ETA: 0s - loss: 45.6309
Epoch 00029: val_loss did not improve from 46.83665
242/242 [==============================] - 0s 313us/sample - loss: 47.0450 - val_loss: 46.9208
Epoch 30/200
 20/242 [=>............................] - ETA: 0s - loss: 44.5933
Epoch 00030: val_loss did not improve from 46.83665

Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 313us/sample - loss: 47.0245 - val_loss: 46.9559
Epoch 31/200
 20/242 [=>............................] - ETA: 0s - loss: 44.7453
Epoch 00031: val_loss did not improve from 46.83665
242/242 [==============================] - 0s 309us/sample - loss: 47.0208 - val_loss: 47.0493
Epoch 32/200
 20/242 [=>............................] - ETA: 0s - loss: 43.8430
Epoch 00032: val_loss did not improve from 46.83665

Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 311us/sample - loss: 47.0186 - val_loss: 46.9783
Epoch 33/200
 20/242 [=>............................] - ETA: 0s - loss: 41.4847
Epoch 00033: val_loss did not improve from 46.83665
242/242 [==============================] - 0s 310us/sample - loss: 47.0109 - val_loss: 46.9470
Epoch 34/200
 20/242 [=>............................] - ETA: 0s - loss: 44.3490
Epoch 00034: val_loss did not improve from 46.83665

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 312us/sample - loss: 47.0116 - val_loss: 46.9537
Epoch 35/200
 20/242 [=>............................] - ETA: 0s - loss: 42.7234
Epoch 00035: val_loss did not improve from 46.83665
242/242 [==============================] - 0s 312us/sample - loss: 47.0090 - val_loss: 46.9556
Epoch 36/200
 20/242 [=>............................] - ETA: 0s - loss: 45.1264
Epoch 00036: val_loss did not improve from 46.83665

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.
242/242 [==============================] - 0s 312us/sample - loss: 47.0098 - val_loss: 46.9728
Epoch 00036: early stopping
done
