2020-11-09 21:13:55.388576: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 21:13:55.569160: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 21:13:55.569406: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560562579df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:13:55.569430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 21:13:55.591983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 21:13:55.723205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:13:55.741050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:13:55.900492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:13:55.965006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:13:56.069892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:13:56.190741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:13:56.283180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:13:56.429009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:13:56.433201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:13:56.433296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:13:56.619050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:13:56.619106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:13:56.619135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:13:56.625364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:13:56.627655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5605632f53d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 21:13:56.627685: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 21:13:56.633921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:13:56.633998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:13:56.634017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:13:56.634033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:13:56.637007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:13:56.637026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:13:56.637042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:13:56.637057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:13:56.641028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:13:56.641069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:13:56.641080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:13:56.641090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:13:56.644096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:13:56.647838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:13:56.647904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:13:56.647923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:13:56.647939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:13:56.647954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:13:56.647970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:13:56.647985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:13:56.648000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:13:56.650955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:13:56.650985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:13:56.650996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:13:56.651006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:13:56.653990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F048
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
/glade/scratch/wchapman/Reforecast/F030
/glade/scratch/wchapman/Reforecast/F036
/glade/scratch/wchapman/Reforecast/F042
/glade/scratch/wchapman/Reforecast/F048
Training on
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc
['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 159.0099, 152.3901
Mean and standard deviation for "p_sfc" = 984.1710, 62.0373
Mean and standard deviation for "u_tr_p" = 12.6939, 12.2649
Mean and standard deviation for "v_tr_p" = 1.3191, 13.2995
Mean and standard deviation for "Z_p" = 5573.4859, 202.9148
Mean and standard deviation for "IWV" = 13.3341, 7.6461
2020-11-09 21:14:17.842707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:14:17.861542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:14:17.861604: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:14:17.861625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:14:17.861641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:14:17.861656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:14:17.861672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:14:17.861690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:14:17.864777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:14:17.967288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 21:14:17.967419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 21:14:17.968077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 21:14:17.968096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 21:14:17.968111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 21:14:17.968127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 21:14:17.968143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 21:14:17.968159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:14:17.971247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 21:14:17.971310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 21:14:17.971323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 21:14:17.971334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 21:14:17.974412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 21:14:29.050305: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 21:14:32.900772: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 21:14:32.984426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 203.1436, 182.0032
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 2061 samples, validate on 121 samples
Epoch 1/50
  50/2061 [..............................] - ETA: 4:25 - loss: 155.8021 100/2061 [>.............................] - ETA: 2:10 - loss: 152.9600 300/2061 [===>..........................] - ETA: 39s - loss: 150.0619  500/2061 [======>.......................] - ETA: 21s - loss: 153.9145 700/2061 [=========>....................] - ETA: 13s - loss: 152.2982 900/2061 [============>.................] - ETA: 8s - loss: 148.4256 1100/2061 [===============>..............] - ETA: 6s - loss: 140.41871300/2061 [=================>............] - ETA: 4s - loss: 128.39451500/2061 [====================>.........] - ETA: 2s - loss: 118.47441700/2061 [=======================>......] - ETA: 1s - loss: 110.13781900/2061 [==========================>...] - ETA: 0s - loss: 103.9448
Epoch 00001: val_loss improved from inf to 44.89425, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 8s 4ms/sample - loss: 99.2429 - val_loss: 44.8942
Epoch 2/50
  50/2061 [..............................] - ETA: 0s - loss: 44.3841 250/2061 [==>...........................] - ETA: 0s - loss: 42.5142 450/2061 [=====>........................] - ETA: 0s - loss: 41.1371 650/2061 [========>.....................] - ETA: 0s - loss: 40.5251 850/2061 [===========>..................] - ETA: 0s - loss: 40.25361050/2061 [==============>...............] - ETA: 0s - loss: 39.80591250/2061 [=================>............] - ETA: 0s - loss: 39.05981450/2061 [====================>.........] - ETA: 0s - loss: 38.45231650/2061 [=======================>......] - ETA: 0s - loss: 37.92121850/2061 [=========================>....] - ETA: 0s - loss: 37.41602050/2061 [============================>.] - ETA: 0s - loss: 36.9306
Epoch 00002: val_loss did not improve from 44.89425
2061/2061 [==============================] - 1s 283us/sample - loss: 36.9127 - val_loss: 67.2192
Epoch 3/50
  50/2061 [..............................] - ETA: 0s - loss: 31.5920 250/2061 [==>...........................] - ETA: 0s - loss: 31.2573 450/2061 [=====>........................] - ETA: 0s - loss: 31.4237 650/2061 [========>.....................] - ETA: 0s - loss: 31.2035 850/2061 [===========>..................] - ETA: 0s - loss: 31.13551050/2061 [==============>...............] - ETA: 0s - loss: 30.74771250/2061 [=================>............] - ETA: 0s - loss: 30.63841450/2061 [====================>.........] - ETA: 0s - loss: 30.70521650/2061 [=======================>......] - ETA: 0s - loss: 30.73661850/2061 [=========================>....] - ETA: 0s - loss: 30.61542050/2061 [============================>.] - ETA: 0s - loss: 30.5493
Epoch 00003: val_loss did not improve from 44.89425

Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2061/2061 [==============================] - 1s 279us/sample - loss: 30.5291 - val_loss: 46.6157
Epoch 4/50
  50/2061 [..............................] - ETA: 0s - loss: 30.1478 250/2061 [==>...........................] - ETA: 0s - loss: 29.0108 450/2061 [=====>........................] - ETA: 0s - loss: 29.8489 650/2061 [========>.....................] - ETA: 0s - loss: 29.5342 850/2061 [===========>..................] - ETA: 0s - loss: 29.43931050/2061 [==============>...............] - ETA: 0s - loss: 29.51481250/2061 [=================>............] - ETA: 0s - loss: 29.20811450/2061 [====================>.........] - ETA: 0s - loss: 29.19851650/2061 [=======================>......] - ETA: 0s - loss: 29.06311850/2061 [=========================>....] - ETA: 0s - loss: 29.01422050/2061 [============================>.] - ETA: 0s - loss: 29.0341
Epoch 00004: val_loss improved from 44.89425 to 36.13296, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 313us/sample - loss: 29.0228 - val_loss: 36.1330
Epoch 5/50
  50/2061 [..............................] - ETA: 0s - loss: 29.2614 250/2061 [==>...........................] - ETA: 0s - loss: 28.5582 450/2061 [=====>........................] - ETA: 0s - loss: 28.6533 650/2061 [========>.....................] - ETA: 0s - loss: 28.7320 850/2061 [===========>..................] - ETA: 0s - loss: 28.79561050/2061 [==============>...............] - ETA: 0s - loss: 28.93011250/2061 [=================>............] - ETA: 0s - loss: 28.71891450/2061 [====================>.........] - ETA: 0s - loss: 28.59421650/2061 [=======================>......] - ETA: 0s - loss: 28.54801850/2061 [=========================>....] - ETA: 0s - loss: 28.40082050/2061 [============================>.] - ETA: 0s - loss: 28.2749
Epoch 00005: val_loss improved from 36.13296 to 33.03211, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 28.2728 - val_loss: 33.0321
Epoch 6/50
  50/2061 [..............................] - ETA: 0s - loss: 25.9031 250/2061 [==>...........................] - ETA: 0s - loss: 27.6312 450/2061 [=====>........................] - ETA: 0s - loss: 27.6419 650/2061 [========>.....................] - ETA: 0s - loss: 27.5338 850/2061 [===========>..................] - ETA: 0s - loss: 27.86281050/2061 [==============>...............] - ETA: 0s - loss: 27.96401250/2061 [=================>............] - ETA: 0s - loss: 28.13921450/2061 [====================>.........] - ETA: 0s - loss: 28.12761650/2061 [=======================>......] - ETA: 0s - loss: 28.00441850/2061 [=========================>....] - ETA: 0s - loss: 27.95652050/2061 [============================>.] - ETA: 0s - loss: 27.9546
Epoch 00006: val_loss improved from 33.03211 to 29.52524, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 27.9670 - val_loss: 29.5252
Epoch 7/50
  50/2061 [..............................] - ETA: 0s - loss: 28.1032 250/2061 [==>...........................] - ETA: 0s - loss: 28.3108 450/2061 [=====>........................] - ETA: 0s - loss: 28.1794 650/2061 [========>.....................] - ETA: 0s - loss: 27.9627 850/2061 [===========>..................] - ETA: 0s - loss: 27.93941050/2061 [==============>...............] - ETA: 0s - loss: 27.92841250/2061 [=================>............] - ETA: 0s - loss: 27.98901450/2061 [====================>.........] - ETA: 0s - loss: 28.09241650/2061 [=======================>......] - ETA: 0s - loss: 28.00741850/2061 [=========================>....] - ETA: 0s - loss: 27.91962050/2061 [============================>.] - ETA: 0s - loss: 27.8113
Epoch 00007: val_loss improved from 29.52524 to 27.69602, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 298us/sample - loss: 27.7919 - val_loss: 27.6960
Epoch 8/50
  50/2061 [..............................] - ETA: 0s - loss: 28.5044 250/2061 [==>...........................] - ETA: 0s - loss: 27.6112 450/2061 [=====>........................] - ETA: 0s - loss: 27.7952 650/2061 [========>.....................] - ETA: 0s - loss: 27.8985 850/2061 [===========>..................] - ETA: 0s - loss: 27.73771050/2061 [==============>...............] - ETA: 0s - loss: 27.66271250/2061 [=================>............] - ETA: 0s - loss: 27.63201450/2061 [====================>.........] - ETA: 0s - loss: 27.69911650/2061 [=======================>......] - ETA: 0s - loss: 27.63381850/2061 [=========================>....] - ETA: 0s - loss: 27.55312050/2061 [============================>.] - ETA: 0s - loss: 27.4214
Epoch 00008: val_loss improved from 27.69602 to 25.40795, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 27.4426 - val_loss: 25.4079
Epoch 9/50
  50/2061 [..............................] - ETA: 0s - loss: 25.9079 250/2061 [==>...........................] - ETA: 0s - loss: 28.1478 450/2061 [=====>........................] - ETA: 0s - loss: 27.9285 650/2061 [========>.....................] - ETA: 0s - loss: 27.5111 850/2061 [===========>..................] - ETA: 0s - loss: 27.37011050/2061 [==============>...............] - ETA: 0s - loss: 27.49361250/2061 [=================>............] - ETA: 0s - loss: 27.54111450/2061 [====================>.........] - ETA: 0s - loss: 27.54711650/2061 [=======================>......] - ETA: 0s - loss: 27.42541850/2061 [=========================>....] - ETA: 0s - loss: 27.45692050/2061 [============================>.] - ETA: 0s - loss: 27.4492
Epoch 00009: val_loss improved from 25.40795 to 25.09435, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 294us/sample - loss: 27.4520 - val_loss: 25.0943
Epoch 10/50
  50/2061 [..............................] - ETA: 0s - loss: 26.7799 250/2061 [==>...........................] - ETA: 0s - loss: 27.5616 450/2061 [=====>........................] - ETA: 0s - loss: 26.9916 650/2061 [========>.....................] - ETA: 0s - loss: 26.8040 850/2061 [===========>..................] - ETA: 0s - loss: 26.87951050/2061 [==============>...............] - ETA: 0s - loss: 27.04501250/2061 [=================>............] - ETA: 0s - loss: 27.03071400/2061 [===================>..........] - ETA: 0s - loss: 27.00541550/2061 [=====================>........] - ETA: 0s - loss: 26.94891750/2061 [========================>.....] - ETA: 0s - loss: 27.01391950/2061 [===========================>..] - ETA: 0s - loss: 27.0521
Epoch 00010: val_loss did not improve from 25.09435
2061/2061 [==============================] - 1s 297us/sample - loss: 26.9888 - val_loss: 25.4133
Epoch 11/50
  50/2061 [..............................] - ETA: 0s - loss: 25.4189 250/2061 [==>...........................] - ETA: 0s - loss: 27.8624 450/2061 [=====>........................] - ETA: 0s - loss: 27.3434 650/2061 [========>.....................] - ETA: 0s - loss: 27.3940 850/2061 [===========>..................] - ETA: 0s - loss: 27.33261050/2061 [==============>...............] - ETA: 0s - loss: 27.33481250/2061 [=================>............] - ETA: 0s - loss: 27.41881450/2061 [====================>.........] - ETA: 0s - loss: 27.33961650/2061 [=======================>......] - ETA: 0s - loss: 27.38221850/2061 [=========================>....] - ETA: 0s - loss: 27.37912050/2061 [============================>.] - ETA: 0s - loss: 27.4220
Epoch 00011: val_loss improved from 25.09435 to 25.00194, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 290us/sample - loss: 27.4103 - val_loss: 25.0019
Epoch 12/50
  50/2061 [..............................] - ETA: 0s - loss: 27.0375 250/2061 [==>...........................] - ETA: 0s - loss: 26.9459 450/2061 [=====>........................] - ETA: 0s - loss: 27.0022 650/2061 [========>.....................] - ETA: 0s - loss: 26.9567 850/2061 [===========>..................] - ETA: 0s - loss: 26.92591050/2061 [==============>...............] - ETA: 0s - loss: 27.02121250/2061 [=================>............] - ETA: 0s - loss: 26.96541450/2061 [====================>.........] - ETA: 0s - loss: 26.95291650/2061 [=======================>......] - ETA: 0s - loss: 27.09621850/2061 [=========================>....] - ETA: 0s - loss: 27.04252050/2061 [============================>.] - ETA: 0s - loss: 27.0331
Epoch 00012: val_loss improved from 25.00194 to 24.41424, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 290us/sample - loss: 27.0310 - val_loss: 24.4142
Epoch 13/50
  50/2061 [..............................] - ETA: 0s - loss: 27.3397 250/2061 [==>...........................] - ETA: 0s - loss: 28.2824 450/2061 [=====>........................] - ETA: 0s - loss: 27.7644 650/2061 [========>.....................] - ETA: 0s - loss: 27.3928 850/2061 [===========>..................] - ETA: 0s - loss: 27.10381050/2061 [==============>...............] - ETA: 0s - loss: 27.20161250/2061 [=================>............] - ETA: 0s - loss: 27.05131450/2061 [====================>.........] - ETA: 0s - loss: 27.04981650/2061 [=======================>......] - ETA: 0s - loss: 27.07851850/2061 [=========================>....] - ETA: 0s - loss: 26.92012050/2061 [============================>.] - ETA: 0s - loss: 26.9501
Epoch 00013: val_loss improved from 24.41424 to 24.15869, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 293us/sample - loss: 26.9452 - val_loss: 24.1587
Epoch 14/50
  50/2061 [..............................] - ETA: 0s - loss: 25.7813 250/2061 [==>...........................] - ETA: 0s - loss: 25.9046 450/2061 [=====>........................] - ETA: 0s - loss: 26.0751 650/2061 [========>.....................] - ETA: 0s - loss: 26.2092 850/2061 [===========>..................] - ETA: 0s - loss: 26.33041050/2061 [==============>...............] - ETA: 0s - loss: 26.49571250/2061 [=================>............] - ETA: 0s - loss: 26.56371450/2061 [====================>.........] - ETA: 0s - loss: 26.55181650/2061 [=======================>......] - ETA: 0s - loss: 26.59661850/2061 [=========================>....] - ETA: 0s - loss: 26.71392050/2061 [============================>.] - ETA: 0s - loss: 26.6272
Epoch 00014: val_loss did not improve from 24.15869
2061/2061 [==============================] - 1s 279us/sample - loss: 26.6272 - val_loss: 24.2589
Epoch 15/50
  50/2061 [..............................] - ETA: 0s - loss: 24.5243 250/2061 [==>...........................] - ETA: 0s - loss: 25.9552 450/2061 [=====>........................] - ETA: 0s - loss: 26.3879 650/2061 [========>.....................] - ETA: 0s - loss: 26.7241 850/2061 [===========>..................] - ETA: 0s - loss: 26.63661050/2061 [==============>...............] - ETA: 0s - loss: 26.54851250/2061 [=================>............] - ETA: 0s - loss: 26.54451450/2061 [====================>.........] - ETA: 0s - loss: 26.46851650/2061 [=======================>......] - ETA: 0s - loss: 26.64711850/2061 [=========================>....] - ETA: 0s - loss: 26.60382050/2061 [============================>.] - ETA: 0s - loss: 26.6931
Epoch 00015: val_loss did not improve from 24.15869

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2061/2061 [==============================] - 1s 282us/sample - loss: 26.7042 - val_loss: 24.2044
Epoch 16/50
  50/2061 [..............................] - ETA: 0s - loss: 26.5349 250/2061 [==>...........................] - ETA: 0s - loss: 26.5961 450/2061 [=====>........................] - ETA: 0s - loss: 26.8704 650/2061 [========>.....................] - ETA: 0s - loss: 26.3238 850/2061 [===========>..................] - ETA: 0s - loss: 26.81561050/2061 [==============>...............] - ETA: 0s - loss: 26.71861250/2061 [=================>............] - ETA: 0s - loss: 26.54761450/2061 [====================>.........] - ETA: 0s - loss: 26.55171650/2061 [=======================>......] - ETA: 0s - loss: 26.45941850/2061 [=========================>....] - ETA: 0s - loss: 26.45482050/2061 [============================>.] - ETA: 0s - loss: 26.5469
Epoch 00016: val_loss improved from 24.15869 to 23.92211, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 299us/sample - loss: 26.5566 - val_loss: 23.9221
Epoch 17/50
  50/2061 [..............................] - ETA: 0s - loss: 25.6639 250/2061 [==>...........................] - ETA: 0s - loss: 26.1553 450/2061 [=====>........................] - ETA: 0s - loss: 26.1380 650/2061 [========>.....................] - ETA: 0s - loss: 25.9077 850/2061 [===========>..................] - ETA: 0s - loss: 26.24621050/2061 [==============>...............] - ETA: 0s - loss: 26.44901250/2061 [=================>............] - ETA: 0s - loss: 26.47361450/2061 [====================>.........] - ETA: 0s - loss: 26.50311650/2061 [=======================>......] - ETA: 0s - loss: 26.57981850/2061 [=========================>....] - ETA: 0s - loss: 26.49552050/2061 [============================>.] - ETA: 0s - loss: 26.5515
Epoch 00017: val_loss did not improve from 23.92211
2061/2061 [==============================] - 1s 279us/sample - loss: 26.5539 - val_loss: 23.9624
Epoch 18/50
  50/2061 [..............................] - ETA: 0s - loss: 25.0884 250/2061 [==>...........................] - ETA: 0s - loss: 26.4344 450/2061 [=====>........................] - ETA: 0s - loss: 27.0166 650/2061 [========>.....................] - ETA: 0s - loss: 26.8321 850/2061 [===========>..................] - ETA: 0s - loss: 26.45341050/2061 [==============>...............] - ETA: 0s - loss: 26.41901250/2061 [=================>............] - ETA: 0s - loss: 26.34751450/2061 [====================>.........] - ETA: 0s - loss: 26.42951650/2061 [=======================>......] - ETA: 0s - loss: 26.51461850/2061 [=========================>....] - ETA: 0s - loss: 26.47092050/2061 [============================>.] - ETA: 0s - loss: 26.5128
Epoch 00018: val_loss improved from 23.92211 to 23.91164, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 291us/sample - loss: 26.5074 - val_loss: 23.9116
Epoch 19/50
  50/2061 [..............................] - ETA: 0s - loss: 28.3290 250/2061 [==>...........................] - ETA: 0s - loss: 26.8025 450/2061 [=====>........................] - ETA: 0s - loss: 26.7486 650/2061 [========>.....................] - ETA: 0s - loss: 26.4757 850/2061 [===========>..................] - ETA: 0s - loss: 26.42841050/2061 [==============>...............] - ETA: 0s - loss: 26.51901250/2061 [=================>............] - ETA: 0s - loss: 26.40431450/2061 [====================>.........] - ETA: 0s - loss: 26.47401650/2061 [=======================>......] - ETA: 0s - loss: 26.46811850/2061 [=========================>....] - ETA: 0s - loss: 26.46562050/2061 [============================>.] - ETA: 0s - loss: 26.4289
Epoch 00019: val_loss improved from 23.91164 to 23.75110, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 297us/sample - loss: 26.4321 - val_loss: 23.7511
Epoch 20/50
  50/2061 [..............................] - ETA: 0s - loss: 25.8953 250/2061 [==>...........................] - ETA: 0s - loss: 26.9513 450/2061 [=====>........................] - ETA: 0s - loss: 26.7289 650/2061 [========>.....................] - ETA: 0s - loss: 26.9862 850/2061 [===========>..................] - ETA: 0s - loss: 26.65121050/2061 [==============>...............] - ETA: 0s - loss: 26.60451250/2061 [=================>............] - ETA: 0s - loss: 26.50921450/2061 [====================>.........] - ETA: 0s - loss: 26.40711650/2061 [=======================>......] - ETA: 0s - loss: 26.38571850/2061 [=========================>....] - ETA: 0s - loss: 26.45532050/2061 [============================>.] - ETA: 0s - loss: 26.4293
Epoch 00020: val_loss improved from 23.75110 to 23.56507, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 26.4301 - val_loss: 23.5651
Epoch 21/50
  50/2061 [..............................] - ETA: 0s - loss: 26.2144 250/2061 [==>...........................] - ETA: 0s - loss: 26.9935 450/2061 [=====>........................] - ETA: 0s - loss: 26.7646 650/2061 [========>.....................] - ETA: 0s - loss: 26.5319 850/2061 [===========>..................] - ETA: 0s - loss: 26.50371050/2061 [==============>...............] - ETA: 0s - loss: 26.45241250/2061 [=================>............] - ETA: 0s - loss: 26.46041450/2061 [====================>.........] - ETA: 0s - loss: 26.41051650/2061 [=======================>......] - ETA: 0s - loss: 26.46081850/2061 [=========================>....] - ETA: 0s - loss: 26.50042050/2061 [============================>.] - ETA: 0s - loss: 26.3860
Epoch 00021: val_loss did not improve from 23.56507
2061/2061 [==============================] - 1s 278us/sample - loss: 26.3781 - val_loss: 23.7162
Epoch 22/50
  50/2061 [..............................] - ETA: 0s - loss: 26.0292 250/2061 [==>...........................] - ETA: 0s - loss: 27.8015 450/2061 [=====>........................] - ETA: 0s - loss: 26.9689 650/2061 [========>.....................] - ETA: 0s - loss: 26.6042 850/2061 [===========>..................] - ETA: 0s - loss: 26.26631050/2061 [==============>...............] - ETA: 0s - loss: 26.29051250/2061 [=================>............] - ETA: 0s - loss: 26.39311450/2061 [====================>.........] - ETA: 0s - loss: 26.31431650/2061 [=======================>......] - ETA: 0s - loss: 26.45901850/2061 [=========================>....] - ETA: 0s - loss: 26.51082050/2061 [============================>.] - ETA: 0s - loss: 26.5386
Epoch 00022: val_loss did not improve from 23.56507

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2061/2061 [==============================] - 1s 284us/sample - loss: 26.5391 - val_loss: 23.5728
Epoch 23/50
  50/2061 [..............................] - ETA: 0s - loss: 27.6940 250/2061 [==>...........................] - ETA: 0s - loss: 27.2150 450/2061 [=====>........................] - ETA: 0s - loss: 26.4976 650/2061 [========>.....................] - ETA: 0s - loss: 26.4350 850/2061 [===========>..................] - ETA: 0s - loss: 26.68641050/2061 [==============>...............] - ETA: 0s - loss: 26.63251250/2061 [=================>............] - ETA: 0s - loss: 26.37691450/2061 [====================>.........] - ETA: 0s - loss: 26.56391650/2061 [=======================>......] - ETA: 0s - loss: 26.45131850/2061 [=========================>....] - ETA: 0s - loss: 26.46682050/2061 [============================>.] - ETA: 0s - loss: 26.4406
Epoch 00023: val_loss improved from 23.56507 to 23.48586, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 301us/sample - loss: 26.4354 - val_loss: 23.4859
Epoch 24/50
  50/2061 [..............................] - ETA: 0s - loss: 27.6096 250/2061 [==>...........................] - ETA: 0s - loss: 26.5236 450/2061 [=====>........................] - ETA: 0s - loss: 26.5177 650/2061 [========>.....................] - ETA: 0s - loss: 26.5948 850/2061 [===========>..................] - ETA: 0s - loss: 26.38391100/2061 [===============>..............] - ETA: 0s - loss: 26.01421300/2061 [=================>............] - ETA: 0s - loss: 26.12271500/2061 [====================>.........] - ETA: 0s - loss: 26.19551700/2061 [=======================>......] - ETA: 0s - loss: 26.16871900/2061 [==========================>...] - ETA: 0s - loss: 26.1970
Epoch 00024: val_loss improved from 23.48586 to 23.46053, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 294us/sample - loss: 26.1937 - val_loss: 23.4605
Epoch 25/50
  50/2061 [..............................] - ETA: 0s - loss: 26.0292 250/2061 [==>...........................] - ETA: 0s - loss: 26.1753 450/2061 [=====>........................] - ETA: 0s - loss: 26.1388 650/2061 [========>.....................] - ETA: 0s - loss: 26.0303 850/2061 [===========>..................] - ETA: 0s - loss: 26.04441050/2061 [==============>...............] - ETA: 0s - loss: 26.12901250/2061 [=================>............] - ETA: 0s - loss: 26.19571450/2061 [====================>.........] - ETA: 0s - loss: 26.23621650/2061 [=======================>......] - ETA: 0s - loss: 26.25331850/2061 [=========================>....] - ETA: 0s - loss: 26.23592050/2061 [============================>.] - ETA: 0s - loss: 26.2142
Epoch 00025: val_loss improved from 23.46053 to 23.44689, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 26.2287 - val_loss: 23.4469
Epoch 26/50
  50/2061 [..............................] - ETA: 0s - loss: 26.7663 250/2061 [==>...........................] - ETA: 0s - loss: 25.8049 450/2061 [=====>........................] - ETA: 0s - loss: 26.0006 650/2061 [========>.....................] - ETA: 0s - loss: 26.2173 850/2061 [===========>..................] - ETA: 0s - loss: 26.13301050/2061 [==============>...............] - ETA: 0s - loss: 26.08681250/2061 [=================>............] - ETA: 0s - loss: 26.22921450/2061 [====================>.........] - ETA: 0s - loss: 26.23001650/2061 [=======================>......] - ETA: 0s - loss: 26.27031850/2061 [=========================>....] - ETA: 0s - loss: 26.32802050/2061 [============================>.] - ETA: 0s - loss: 26.2738
Epoch 00026: val_loss did not improve from 23.44689
2061/2061 [==============================] - 1s 275us/sample - loss: 26.2641 - val_loss: 23.4779
Epoch 27/50
  50/2061 [..............................] - ETA: 0s - loss: 26.1659 250/2061 [==>...........................] - ETA: 0s - loss: 26.4684 450/2061 [=====>........................] - ETA: 0s - loss: 26.4355 650/2061 [========>.....................] - ETA: 0s - loss: 26.4737 850/2061 [===========>..................] - ETA: 0s - loss: 26.23901000/2061 [=============>................] - ETA: 0s - loss: 26.12251200/2061 [================>.............] - ETA: 0s - loss: 26.11481400/2061 [===================>..........] - ETA: 0s - loss: 26.15621600/2061 [======================>.......] - ETA: 0s - loss: 26.25951800/2061 [=========================>....] - ETA: 0s - loss: 26.21472000/2061 [============================>.] - ETA: 0s - loss: 26.2281
Epoch 00027: val_loss did not improve from 23.44689

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2061/2061 [==============================] - 1s 296us/sample - loss: 26.2521 - val_loss: 23.4773
Epoch 28/50
  50/2061 [..............................] - ETA: 0s - loss: 26.6359 250/2061 [==>...........................] - ETA: 0s - loss: 25.7526 450/2061 [=====>........................] - ETA: 0s - loss: 26.0595 650/2061 [========>.....................] - ETA: 0s - loss: 25.9811 850/2061 [===========>..................] - ETA: 0s - loss: 26.17071050/2061 [==============>...............] - ETA: 0s - loss: 26.25321250/2061 [=================>............] - ETA: 0s - loss: 26.22241450/2061 [====================>.........] - ETA: 0s - loss: 26.15061650/2061 [=======================>......] - ETA: 0s - loss: 26.15331850/2061 [=========================>....] - ETA: 0s - loss: 26.15712050/2061 [============================>.] - ETA: 0s - loss: 26.1891
Epoch 00028: val_loss did not improve from 23.44689
2061/2061 [==============================] - 1s 276us/sample - loss: 26.1746 - val_loss: 23.4558
Epoch 29/50
  50/2061 [..............................] - ETA: 0s - loss: 25.2093 250/2061 [==>...........................] - ETA: 0s - loss: 26.0745 450/2061 [=====>........................] - ETA: 0s - loss: 26.3303 650/2061 [========>.....................] - ETA: 0s - loss: 26.2529 850/2061 [===========>..................] - ETA: 0s - loss: 26.39221050/2061 [==============>...............] - ETA: 0s - loss: 26.36301250/2061 [=================>............] - ETA: 0s - loss: 26.23351450/2061 [====================>.........] - ETA: 0s - loss: 26.22021650/2061 [=======================>......] - ETA: 0s - loss: 26.20031850/2061 [=========================>....] - ETA: 0s - loss: 26.18312050/2061 [============================>.] - ETA: 0s - loss: 26.1898
Epoch 00029: val_loss improved from 23.44689 to 23.42185, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 297us/sample - loss: 26.1811 - val_loss: 23.4219
Epoch 30/50
  50/2061 [..............................] - ETA: 0s - loss: 29.3057 250/2061 [==>...........................] - ETA: 0s - loss: 26.2242 450/2061 [=====>........................] - ETA: 0s - loss: 26.2516 650/2061 [========>.....................] - ETA: 0s - loss: 26.2323 850/2061 [===========>..................] - ETA: 0s - loss: 26.31981050/2061 [==============>...............] - ETA: 0s - loss: 26.38281250/2061 [=================>............] - ETA: 0s - loss: 26.17271450/2061 [====================>.........] - ETA: 0s - loss: 26.17081650/2061 [=======================>......] - ETA: 0s - loss: 26.18491850/2061 [=========================>....] - ETA: 0s - loss: 26.18882050/2061 [============================>.] - ETA: 0s - loss: 26.1625
Epoch 00030: val_loss improved from 23.42185 to 23.40806, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 299us/sample - loss: 26.1766 - val_loss: 23.4081
Epoch 31/50
  50/2061 [..............................] - ETA: 0s - loss: 25.4844 250/2061 [==>...........................] - ETA: 0s - loss: 25.5349 450/2061 [=====>........................] - ETA: 0s - loss: 25.9005 650/2061 [========>.....................] - ETA: 0s - loss: 25.7786 850/2061 [===========>..................] - ETA: 0s - loss: 25.89001050/2061 [==============>...............] - ETA: 0s - loss: 25.95841250/2061 [=================>............] - ETA: 0s - loss: 26.03981450/2061 [====================>.........] - ETA: 0s - loss: 26.13181650/2061 [=======================>......] - ETA: 0s - loss: 26.09451850/2061 [=========================>....] - ETA: 0s - loss: 26.13772050/2061 [============================>.] - ETA: 0s - loss: 26.2541
Epoch 00031: val_loss did not improve from 23.40806
2061/2061 [==============================] - 1s 279us/sample - loss: 26.2463 - val_loss: 23.4154
Epoch 32/50
  50/2061 [..............................] - ETA: 0s - loss: 26.0854 250/2061 [==>...........................] - ETA: 0s - loss: 25.9984 450/2061 [=====>........................] - ETA: 0s - loss: 26.0924 650/2061 [========>.....................] - ETA: 0s - loss: 26.3068 850/2061 [===========>..................] - ETA: 0s - loss: 26.30121050/2061 [==============>...............] - ETA: 0s - loss: 26.33351250/2061 [=================>............] - ETA: 0s - loss: 26.23451450/2061 [====================>.........] - ETA: 0s - loss: 26.26481650/2061 [=======================>......] - ETA: 0s - loss: 26.26091850/2061 [=========================>....] - ETA: 0s - loss: 26.29122050/2061 [============================>.] - ETA: 0s - loss: 26.2392
Epoch 00032: val_loss improved from 23.40806 to 23.39540, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 297us/sample - loss: 26.2410 - val_loss: 23.3954
Epoch 33/50
  50/2061 [..............................] - ETA: 0s - loss: 25.3532 250/2061 [==>...........................] - ETA: 0s - loss: 26.2677 450/2061 [=====>........................] - ETA: 0s - loss: 26.1884 650/2061 [========>.....................] - ETA: 0s - loss: 26.0046 850/2061 [===========>..................] - ETA: 0s - loss: 26.22041050/2061 [==============>...............] - ETA: 0s - loss: 26.12101250/2061 [=================>............] - ETA: 0s - loss: 26.10271450/2061 [====================>.........] - ETA: 0s - loss: 26.08981650/2061 [=======================>......] - ETA: 0s - loss: 26.16691850/2061 [=========================>....] - ETA: 0s - loss: 26.14032050/2061 [============================>.] - ETA: 0s - loss: 26.1453
Epoch 00033: val_loss improved from 23.39540 to 23.38586, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 291us/sample - loss: 26.1566 - val_loss: 23.3859
Epoch 34/50
  50/2061 [..............................] - ETA: 0s - loss: 26.3444 250/2061 [==>...........................] - ETA: 0s - loss: 25.2332 450/2061 [=====>........................] - ETA: 0s - loss: 25.9800 650/2061 [========>.....................] - ETA: 0s - loss: 26.1222 850/2061 [===========>..................] - ETA: 0s - loss: 26.34871050/2061 [==============>...............] - ETA: 0s - loss: 26.23111250/2061 [=================>............] - ETA: 0s - loss: 26.15991450/2061 [====================>.........] - ETA: 0s - loss: 26.19151650/2061 [=======================>......] - ETA: 0s - loss: 26.17941850/2061 [=========================>....] - ETA: 0s - loss: 26.13732050/2061 [============================>.] - ETA: 0s - loss: 26.1675
Epoch 00034: val_loss did not improve from 23.38586
2061/2061 [==============================] - 1s 280us/sample - loss: 26.1885 - val_loss: 23.3875
Epoch 35/50
  50/2061 [..............................] - ETA: 0s - loss: 25.6423 250/2061 [==>...........................] - ETA: 0s - loss: 25.5405 450/2061 [=====>........................] - ETA: 0s - loss: 26.1743 650/2061 [========>.....................] - ETA: 0s - loss: 25.8479 850/2061 [===========>..................] - ETA: 0s - loss: 25.93851050/2061 [==============>...............] - ETA: 0s - loss: 26.01641250/2061 [=================>............] - ETA: 0s - loss: 26.08111450/2061 [====================>.........] - ETA: 0s - loss: 26.01411650/2061 [=======================>......] - ETA: 0s - loss: 26.03831850/2061 [=========================>....] - ETA: 0s - loss: 26.10932050/2061 [============================>.] - ETA: 0s - loss: 26.1062
Epoch 00035: val_loss improved from 23.38586 to 23.37764, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 26.1114 - val_loss: 23.3776
Epoch 36/50
  50/2061 [..............................] - ETA: 0s - loss: 25.5104 250/2061 [==>...........................] - ETA: 0s - loss: 26.2108 450/2061 [=====>........................] - ETA: 0s - loss: 26.9458 650/2061 [========>.....................] - ETA: 0s - loss: 26.6914 850/2061 [===========>..................] - ETA: 0s - loss: 26.74541050/2061 [==============>...............] - ETA: 0s - loss: 26.47901250/2061 [=================>............] - ETA: 0s - loss: 26.47331450/2061 [====================>.........] - ETA: 0s - loss: 26.36361650/2061 [=======================>......] - ETA: 0s - loss: 26.33601850/2061 [=========================>....] - ETA: 0s - loss: 26.26852050/2061 [============================>.] - ETA: 0s - loss: 26.2801
Epoch 00036: val_loss did not improve from 23.37764
2061/2061 [==============================] - 1s 276us/sample - loss: 26.2909 - val_loss: 23.3988
Epoch 37/50
  50/2061 [..............................] - ETA: 0s - loss: 28.8078 250/2061 [==>...........................] - ETA: 0s - loss: 27.8314 450/2061 [=====>........................] - ETA: 0s - loss: 27.2418 650/2061 [========>.....................] - ETA: 0s - loss: 26.7225 850/2061 [===========>..................] - ETA: 0s - loss: 26.48111050/2061 [==============>...............] - ETA: 0s - loss: 26.38041250/2061 [=================>............] - ETA: 0s - loss: 26.40981450/2061 [====================>.........] - ETA: 0s - loss: 26.38861650/2061 [=======================>......] - ETA: 0s - loss: 26.27791850/2061 [=========================>....] - ETA: 0s - loss: 26.27742050/2061 [============================>.] - ETA: 0s - loss: 26.2365
Epoch 00037: val_loss improved from 23.37764 to 23.37665, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 293us/sample - loss: 26.2231 - val_loss: 23.3766
Epoch 38/50
  50/2061 [..............................] - ETA: 0s - loss: 24.6369 250/2061 [==>...........................] - ETA: 0s - loss: 26.0546 450/2061 [=====>........................] - ETA: 0s - loss: 25.7881 650/2061 [========>.....................] - ETA: 0s - loss: 25.9251 850/2061 [===========>..................] - ETA: 0s - loss: 26.12081050/2061 [==============>...............] - ETA: 0s - loss: 26.04991250/2061 [=================>............] - ETA: 0s - loss: 25.98311450/2061 [====================>.........] - ETA: 0s - loss: 25.94651650/2061 [=======================>......] - ETA: 0s - loss: 26.13781850/2061 [=========================>....] - ETA: 0s - loss: 26.18302050/2061 [============================>.] - ETA: 0s - loss: 26.2238
Epoch 00038: val_loss improved from 23.37665 to 23.36714, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 296us/sample - loss: 26.2289 - val_loss: 23.3671
Epoch 39/50
  50/2061 [..............................] - ETA: 0s - loss: 26.3135 250/2061 [==>...........................] - ETA: 0s - loss: 26.2766 450/2061 [=====>........................] - ETA: 0s - loss: 25.9981 650/2061 [========>.....................] - ETA: 0s - loss: 26.2638 850/2061 [===========>..................] - ETA: 0s - loss: 25.95731050/2061 [==============>...............] - ETA: 0s - loss: 26.12471250/2061 [=================>............] - ETA: 0s - loss: 26.11691450/2061 [====================>.........] - ETA: 0s - loss: 26.26941650/2061 [=======================>......] - ETA: 0s - loss: 26.27551850/2061 [=========================>....] - ETA: 0s - loss: 26.31202050/2061 [============================>.] - ETA: 0s - loss: 26.1885
Epoch 00039: val_loss did not improve from 23.36714
2061/2061 [==============================] - 1s 281us/sample - loss: 26.1849 - val_loss: 23.3826
Epoch 40/50
  50/2061 [..............................] - ETA: 0s - loss: 25.0028 250/2061 [==>...........................] - ETA: 0s - loss: 26.6422 450/2061 [=====>........................] - ETA: 0s - loss: 26.4250 650/2061 [========>.....................] - ETA: 0s - loss: 26.2732 850/2061 [===========>..................] - ETA: 0s - loss: 26.34351050/2061 [==============>...............] - ETA: 0s - loss: 26.26361250/2061 [=================>............] - ETA: 0s - loss: 26.24581450/2061 [====================>.........] - ETA: 0s - loss: 26.30721650/2061 [=======================>......] - ETA: 0s - loss: 26.25451850/2061 [=========================>....] - ETA: 0s - loss: 26.24412050/2061 [============================>.] - ETA: 0s - loss: 26.2013
Epoch 00040: val_loss did not improve from 23.36714

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2061/2061 [==============================] - 1s 280us/sample - loss: 26.1885 - val_loss: 23.3676
Epoch 41/50
  50/2061 [..............................] - ETA: 0s - loss: 24.7735 250/2061 [==>...........................] - ETA: 0s - loss: 25.5609 450/2061 [=====>........................] - ETA: 0s - loss: 25.8247 650/2061 [========>.....................] - ETA: 0s - loss: 25.9333 850/2061 [===========>..................] - ETA: 0s - loss: 25.95671050/2061 [==============>...............] - ETA: 0s - loss: 26.00581250/2061 [=================>............] - ETA: 0s - loss: 26.04771450/2061 [====================>.........] - ETA: 0s - loss: 26.15491650/2061 [=======================>......] - ETA: 0s - loss: 26.05041850/2061 [=========================>....] - ETA: 0s - loss: 26.12602050/2061 [============================>.] - ETA: 0s - loss: 26.1423
Epoch 00041: val_loss improved from 23.36714 to 23.35166, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 295us/sample - loss: 26.1457 - val_loss: 23.3517
Epoch 42/50
  50/2061 [..............................] - ETA: 0s - loss: 23.8231 250/2061 [==>...........................] - ETA: 0s - loss: 25.1487 450/2061 [=====>........................] - ETA: 0s - loss: 25.4299 650/2061 [========>.....................] - ETA: 0s - loss: 25.7787 850/2061 [===========>..................] - ETA: 0s - loss: 25.96731050/2061 [==============>...............] - ETA: 0s - loss: 26.24621250/2061 [=================>............] - ETA: 0s - loss: 26.33181450/2061 [====================>.........] - ETA: 0s - loss: 26.24161650/2061 [=======================>......] - ETA: 0s - loss: 26.09991850/2061 [=========================>....] - ETA: 0s - loss: 26.13052050/2061 [============================>.] - ETA: 0s - loss: 26.0812
Epoch 00042: val_loss did not improve from 23.35166
2061/2061 [==============================] - 1s 278us/sample - loss: 26.0985 - val_loss: 23.3537
Epoch 43/50
  50/2061 [..............................] - ETA: 0s - loss: 28.2060 250/2061 [==>...........................] - ETA: 0s - loss: 27.2440 450/2061 [=====>........................] - ETA: 0s - loss: 26.7130 650/2061 [========>.....................] - ETA: 0s - loss: 26.3807 850/2061 [===========>..................] - ETA: 0s - loss: 26.11801050/2061 [==============>...............] - ETA: 0s - loss: 26.03891250/2061 [=================>............] - ETA: 0s - loss: 26.20161450/2061 [====================>.........] - ETA: 0s - loss: 26.24881650/2061 [=======================>......] - ETA: 0s - loss: 26.14311850/2061 [=========================>....] - ETA: 0s - loss: 26.12052050/2061 [============================>.] - ETA: 0s - loss: 26.1521
Epoch 00043: val_loss did not improve from 23.35166

Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-05.
2061/2061 [==============================] - 1s 281us/sample - loss: 26.1780 - val_loss: 23.3570
Epoch 44/50
  50/2061 [..............................] - ETA: 0s - loss: 26.9908 250/2061 [==>...........................] - ETA: 0s - loss: 26.5955 450/2061 [=====>........................] - ETA: 0s - loss: 25.7324 650/2061 [========>.....................] - ETA: 0s - loss: 25.8105 850/2061 [===========>..................] - ETA: 0s - loss: 25.98701050/2061 [==============>...............] - ETA: 0s - loss: 25.98671250/2061 [=================>............] - ETA: 0s - loss: 25.98311450/2061 [====================>.........] - ETA: 0s - loss: 25.97491650/2061 [=======================>......] - ETA: 0s - loss: 26.13351850/2061 [=========================>....] - ETA: 0s - loss: 26.1692
Epoch 00044: val_loss did not improve from 23.35166
2061/2061 [==============================] - 1s 275us/sample - loss: 26.1620 - val_loss: 23.3530
Epoch 45/50
  50/2061 [..............................] - ETA: 0s - loss: 27.0186 250/2061 [==>...........................] - ETA: 0s - loss: 26.7731 450/2061 [=====>........................] - ETA: 0s - loss: 26.9175 650/2061 [========>.....................] - ETA: 0s - loss: 26.5983 850/2061 [===========>..................] - ETA: 0s - loss: 26.36461050/2061 [==============>...............] - ETA: 0s - loss: 26.28881250/2061 [=================>............] - ETA: 0s - loss: 26.33231450/2061 [====================>.........] - ETA: 0s - loss: 26.27591650/2061 [=======================>......] - ETA: 0s - loss: 26.18171850/2061 [=========================>....] - ETA: 0s - loss: 26.19252050/2061 [============================>.] - ETA: 0s - loss: 26.2560
Epoch 00045: val_loss did not improve from 23.35166
2061/2061 [==============================] - 1s 274us/sample - loss: 26.2587 - val_loss: 23.3687
Epoch 46/50
  50/2061 [..............................] - ETA: 1s - loss: 24.6206 200/2061 [=>............................] - ETA: 0s - loss: 25.1853 400/2061 [====>.........................] - ETA: 0s - loss: 25.5099 600/2061 [=======>......................] - ETA: 0s - loss: 25.7974 800/2061 [==========>...................] - ETA: 0s - loss: 25.98561000/2061 [=============>................] - ETA: 0s - loss: 26.01061200/2061 [================>.............] - ETA: 0s - loss: 26.25901400/2061 [===================>..........] - ETA: 0s - loss: 26.36221600/2061 [======================>.......] - ETA: 0s - loss: 26.36311800/2061 [=========================>....] - ETA: 0s - loss: 26.23052000/2061 [============================>.] - ETA: 0s - loss: 26.2428
Epoch 00046: val_loss improved from 23.35166 to 23.34684, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 317us/sample - loss: 26.2686 - val_loss: 23.3468
Epoch 47/50
  50/2061 [..............................] - ETA: 0s - loss: 25.0958 250/2061 [==>...........................] - ETA: 0s - loss: 25.3831 450/2061 [=====>........................] - ETA: 0s - loss: 25.5924 650/2061 [========>.....................] - ETA: 0s - loss: 25.5960 850/2061 [===========>..................] - ETA: 0s - loss: 25.85381050/2061 [==============>...............] - ETA: 0s - loss: 25.88331250/2061 [=================>............] - ETA: 0s - loss: 25.92881450/2061 [====================>.........] - ETA: 0s - loss: 25.93881650/2061 [=======================>......] - ETA: 0s - loss: 25.98331850/2061 [=========================>....] - ETA: 0s - loss: 26.13822050/2061 [============================>.] - ETA: 0s - loss: 26.0697
Epoch 00047: val_loss did not improve from 23.34684
2061/2061 [==============================] - 1s 286us/sample - loss: 26.1111 - val_loss: 23.3525
Epoch 48/50
  50/2061 [..............................] - ETA: 0s - loss: 25.3326 250/2061 [==>...........................] - ETA: 0s - loss: 26.2724 450/2061 [=====>........................] - ETA: 0s - loss: 26.5631 650/2061 [========>.....................] - ETA: 0s - loss: 26.4349 850/2061 [===========>..................] - ETA: 0s - loss: 26.39851050/2061 [==============>...............] - ETA: 0s - loss: 26.47531250/2061 [=================>............] - ETA: 0s - loss: 26.43091450/2061 [====================>.........] - ETA: 0s - loss: 26.44371650/2061 [=======================>......] - ETA: 0s - loss: 26.32881850/2061 [=========================>....] - ETA: 0s - loss: 26.26462050/2061 [============================>.] - ETA: 0s - loss: 26.2406
Epoch 00048: val_loss did not improve from 23.34684
2061/2061 [==============================] - 1s 283us/sample - loss: 26.2302 - val_loss: 23.3491
Epoch 49/50
  50/2061 [..............................] - ETA: 0s - loss: 25.9455 250/2061 [==>...........................] - ETA: 0s - loss: 26.0289 450/2061 [=====>........................] - ETA: 0s - loss: 25.7912 650/2061 [========>.....................] - ETA: 0s - loss: 25.7008 900/2061 [============>.................] - ETA: 0s - loss: 25.82841100/2061 [===============>..............] - ETA: 0s - loss: 25.92391300/2061 [=================>............] - ETA: 0s - loss: 25.88211500/2061 [====================>.........] - ETA: 0s - loss: 25.89321700/2061 [=======================>......] - ETA: 0s - loss: 26.10451900/2061 [==========================>...] - ETA: 0s - loss: 26.0746
Epoch 00049: val_loss did not improve from 23.34684
2061/2061 [==============================] - 1s 268us/sample - loss: 26.1663 - val_loss: 23.3484
Epoch 50/50
  50/2061 [..............................] - ETA: 0s - loss: 26.6519 250/2061 [==>...........................] - ETA: 0s - loss: 26.6807 450/2061 [=====>........................] - ETA: 0s - loss: 26.4062 650/2061 [========>.....................] - ETA: 0s - loss: 25.9934 850/2061 [===========>..................] - ETA: 0s - loss: 25.88491050/2061 [==============>...............] - ETA: 0s - loss: 25.98681250/2061 [=================>............] - ETA: 0s - loss: 26.02771450/2061 [====================>.........] - ETA: 0s - loss: 26.01181650/2061 [=======================>......] - ETA: 0s - loss: 26.18121850/2061 [=========================>....] - ETA: 0s - loss: 26.07312050/2061 [============================>.] - ETA: 0s - loss: 26.1114
Epoch 00050: val_loss improved from 23.34684 to 23.34433, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
2061/2061 [==============================] - 1s 289us/sample - loss: 26.1258 - val_loss: 23.3443
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 2061 samples, validate on 121 samples
Epoch 1/50
  50/2061 [..............................] - ETA: 3:40 - loss: nan 200/2061 [=>............................] - ETA: 51s - loss: nan  400/2061 [====>.........................] - ETA: 23s - loss: nan 600/2061 [=======>......................] - ETA: 13s - loss: nan 800/2061 [==========>...................] - ETA: 8s - loss: nan 1000/2061 [=============>................] - ETA: 6s - loss: nan1200/2061 [================>.............] - ETA: 4s - loss: nan1400/2061 [===================>..........] - ETA: 2s - loss: nan1600/2061 [======================>.......] - ETA: 1s - loss: nan1800/2061 [=========================>....] - ETA: 0s - loss: nan1950/2061 [===========================>..] - ETA: 0s - loss: nan
Epoch 00001: val_loss did not improve from inf
2061/2061 [==============================] - 6s 3ms/sample - loss: nan - val_loss: nan
Epoch 2/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2061/2061 [==============================] - 1s 279us/sample - loss: nan - val_loss: nan
Epoch 3/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
2061/2061 [==============================] - 1s 280us/sample - loss: nan - val_loss: nan
Epoch 4/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1100/2061 [===============>..............] - ETA: 0s - loss: nan1300/2061 [=================>............] - ETA: 0s - loss: nan1550/2061 [=====================>........] - ETA: 0s - loss: nan1800/2061 [=========================>....] - ETA: 0s - loss: nan2000/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2061/2061 [==============================] - 1s 266us/sample - loss: nan - val_loss: nan
Epoch 5/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
2061/2061 [==============================] - 1s 267us/sample - loss: nan - val_loss: nan
Epoch 6/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2061/2061 [==============================] - 1s 290us/sample - loss: nan - val_loss: nan
Epoch 7/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
2061/2061 [==============================] - 1s 289us/sample - loss: nan - val_loss: nan
Epoch 8/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1500/2061 [====================>.........] - ETA: 0s - loss: nan1700/2061 [=======================>......] - ETA: 0s - loss: nan1900/2061 [==========================>...] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2061/2061 [==============================] - 1s 297us/sample - loss: nan - val_loss: nan
Epoch 9/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
2061/2061 [==============================] - 1s 275us/sample - loss: nan - val_loss: nan
Epoch 10/50
  50/2061 [..............................] - ETA: 0s - loss: nan 250/2061 [==>...........................] - ETA: 0s - loss: nan 450/2061 [=====>........................] - ETA: 0s - loss: nan 650/2061 [========>.....................] - ETA: 0s - loss: nan 850/2061 [===========>..................] - ETA: 0s - loss: nan1050/2061 [==============>...............] - ETA: 0s - loss: nan1250/2061 [=================>............] - ETA: 0s - loss: nan1450/2061 [====================>.........] - ETA: 0s - loss: nan1650/2061 [=======================>......] - ETA: 0s - loss: nan1850/2061 [=========================>....] - ETA: 0s - loss: nan2050/2061 [============================>.] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2061/2061 [==============================] - 1s 280us/sample - loss: nan - val_loss: nan
Epoch 00010: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2007_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2008_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2010_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2001_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2006_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2009_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2004_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2000_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2003_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2002_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2005_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 2182 samples, validate on 121 samples
Epoch 1/50
  50/2182 [..............................] - ETA: 49s - loss: nan 200/2182 [=>............................] - ETA: 12s - loss: nan 400/2182 [====>.........................] - ETA: 5s - loss: nan  600/2182 [=======>......................] - ETA: 3s - loss: nan 800/2182 [=========>....................] - ETA: 2s - loss: nan1000/2182 [============>.................] - ETA: 1s - loss: nan1200/2182 [===============>..............] - ETA: 1s - loss: nan1400/2182 [==================>...........] - ETA: 0s - loss: nan1600/2182 [====================>.........] - ETA: 0s - loss: nan1800/2182 [=======================>......] - ETA: 0s - loss: nan2000/2182 [==========================>...] - ETA: 0s - loss: nan
Epoch 00001: val_loss did not improve from inf
2182/2182 [==============================] - 2s 1ms/sample - loss: nan - val_loss: nan
Epoch 2/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00002: val_loss did not improve from inf

Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
2182/2182 [==============================] - 1s 291us/sample - loss: nan - val_loss: nan
Epoch 3/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00003: val_loss did not improve from inf
2182/2182 [==============================] - 1s 274us/sample - loss: nan - val_loss: nan
Epoch 4/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 700/2182 [========>.....................] - ETA: 0s - loss: nan 900/2182 [===========>..................] - ETA: 0s - loss: nan1100/2182 [==============>...............] - ETA: 0s - loss: nan1300/2182 [================>.............] - ETA: 0s - loss: nan1500/2182 [===================>..........] - ETA: 0s - loss: nan1700/2182 [======================>.......] - ETA: 0s - loss: nan1900/2182 [=========================>....] - ETA: 0s - loss: nan2100/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00004: val_loss did not improve from inf

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
2182/2182 [==============================] - 1s 270us/sample - loss: nan - val_loss: nan
Epoch 5/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00005: val_loss did not improve from inf
2182/2182 [==============================] - 1s 288us/sample - loss: nan - val_loss: nan
Epoch 6/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00006: val_loss did not improve from inf

Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
2182/2182 [==============================] - 1s 286us/sample - loss: nan - val_loss: nan
Epoch 7/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00007: val_loss did not improve from inf
2182/2182 [==============================] - 1s 281us/sample - loss: nan - val_loss: nan
Epoch 8/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00008: val_loss did not improve from inf

Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
2182/2182 [==============================] - 1s 277us/sample - loss: nan - val_loss: nan
Epoch 9/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00009: val_loss did not improve from inf
2182/2182 [==============================] - 1s 280us/sample - loss: nan - val_loss: nan
Epoch 10/50
  50/2182 [..............................] - ETA: 0s - loss: nan 250/2182 [==>...........................] - ETA: 0s - loss: nan 450/2182 [=====>........................] - ETA: 0s - loss: nan 650/2182 [=======>......................] - ETA: 0s - loss: nan 850/2182 [==========>...................] - ETA: 0s - loss: nan1050/2182 [=============>................] - ETA: 0s - loss: nan1250/2182 [================>.............] - ETA: 0s - loss: nan1450/2182 [==================>...........] - ETA: 0s - loss: nan1650/2182 [=====================>........] - ETA: 0s - loss: nan1850/2182 [========================>.....] - ETA: 0s - loss: nan2050/2182 [===========================>..] - ETA: 0s - loss: nan
Epoch 00010: val_loss did not improve from inf

Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
2182/2182 [==============================] - 1s 277us/sample - loss: nan - val_loss: nan
Epoch 00010: early stopping
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8afa8f1cc0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b8afad0cfd0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad133c8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8afad0cc88> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afad0c1d0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad0c908> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8afad30e48> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afad37e48> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8afad37710> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad673c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad67cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad70c18> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b8afad82898> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b8afad827f0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b8afad8c0b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad95160> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afada7cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afada7978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afadafc50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afadaf7f0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b8afadb9ba8> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2b8afa8f1cc0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2b8afad0cfd0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad133c8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8afad0cc88> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afad0c1d0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad0c908> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2b8afad30e48> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afad37e48> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2b8afad37710> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad673c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad67cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad70c18> False
<tensorflow.python.keras.layers.merge.Add object at 0x2b8afad82898> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2b8afad827f0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2b8afad8c0b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afad95160> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afada7cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afada7978> False
<tensorflow.python.keras.layers.core.Activation object at 0x2b8afadafc50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afadaf7f0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2b8afadb9ba8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8a8ffce5c0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2b8afadee908> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 194.3896
Epoch 00001: val_loss improved from inf to 145.58887, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 2s 13ms/sample - loss: 177.8903 - val_loss: 145.5889
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 153.3709
Epoch 00002: val_loss improved from 145.58887 to 82.45346, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 115.7744 - val_loss: 82.4535
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 80.6116
Epoch 00003: val_loss improved from 82.45346 to 48.35532, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 684us/sample - loss: 64.0849 - val_loss: 48.3553
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 46.6496
Epoch 00004: val_loss improved from 48.35532 to 39.10861, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 672us/sample - loss: 42.1082 - val_loss: 39.1086
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 34.9310
Epoch 00005: val_loss improved from 39.10861 to 35.36574, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 725us/sample - loss: 36.1420 - val_loss: 35.3657
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 33.7809
Epoch 00006: val_loss improved from 35.36574 to 28.26844, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 717us/sample - loss: 31.5978 - val_loss: 28.2684
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.0814
Epoch 00007: val_loss improved from 28.26844 to 26.17202, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 666us/sample - loss: 26.4966 - val_loss: 26.1720
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.8619
Epoch 00008: val_loss improved from 26.17202 to 25.93228, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 662us/sample - loss: 26.6361 - val_loss: 25.9323
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3030
Epoch 00009: val_loss improved from 25.93228 to 25.72836, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 678us/sample - loss: 25.8033 - val_loss: 25.7284
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.6374
Epoch 00010: val_loss improved from 25.72836 to 25.51542, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 734us/sample - loss: 25.5409 - val_loss: 25.5154
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.8652
Epoch 00011: val_loss improved from 25.51542 to 25.10379, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 744us/sample - loss: 25.2423 - val_loss: 25.1038
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.0494
Epoch 00012: val_loss improved from 25.10379 to 24.88736, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 720us/sample - loss: 25.0178 - val_loss: 24.8874
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.9620
Epoch 00013: val_loss improved from 24.88736 to 24.78092, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 712us/sample - loss: 24.8380 - val_loss: 24.7809
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.2443
Epoch 00014: val_loss improved from 24.78092 to 24.51441, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 713us/sample - loss: 24.5778 - val_loss: 24.5144
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.7416
Epoch 00015: val_loss did not improve from 24.51441
121/121 [==============================] - 0s 513us/sample - loss: 24.4515 - val_loss: 24.5554
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.6268
Epoch 00016: val_loss improved from 24.51441 to 24.32241, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 921us/sample - loss: 24.3263 - val_loss: 24.3224
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3669
Epoch 00017: val_loss improved from 24.32241 to 24.17932, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 724us/sample - loss: 24.3383 - val_loss: 24.1793
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4812
Epoch 00018: val_loss did not improve from 24.17932
121/121 [==============================] - 0s 538us/sample - loss: 24.1376 - val_loss: 24.3253
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7313
Epoch 00019: val_loss improved from 24.17932 to 24.04516, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 681us/sample - loss: 24.0017 - val_loss: 24.0452
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.3408
Epoch 00020: val_loss improved from 24.04516 to 24.00516, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 706us/sample - loss: 24.0245 - val_loss: 24.0052
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9047
Epoch 00021: val_loss did not improve from 24.00516
121/121 [==============================] - 0s 519us/sample - loss: 23.8514 - val_loss: 24.0504
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.8007
Epoch 00022: val_loss improved from 24.00516 to 23.89804, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 677us/sample - loss: 23.7314 - val_loss: 23.8980
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3016
Epoch 00023: val_loss did not improve from 23.89804
121/121 [==============================] - 0s 573us/sample - loss: 23.7015 - val_loss: 23.9037
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8849
Epoch 00024: val_loss did not improve from 23.89804

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 527us/sample - loss: 23.6359 - val_loss: 23.9234
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1313
Epoch 00025: val_loss improved from 23.89804 to 23.84103, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 674us/sample - loss: 23.6337 - val_loss: 23.8410
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9792
Epoch 00026: val_loss improved from 23.84103 to 23.80761, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 672us/sample - loss: 23.6192 - val_loss: 23.8076
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6395
Epoch 00027: val_loss improved from 23.80761 to 23.77139, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 692us/sample - loss: 23.6852 - val_loss: 23.7714
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 26.4967
Epoch 00028: val_loss did not improve from 23.77139
121/121 [==============================] - 0s 516us/sample - loss: 23.5871 - val_loss: 24.0454
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0917
Epoch 00029: val_loss did not improve from 23.77139

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 520us/sample - loss: 23.5842 - val_loss: 23.8063
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3396
Epoch 00030: val_loss did not improve from 23.77139
121/121 [==============================] - 0s 516us/sample - loss: 23.5552 - val_loss: 23.7775
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.2832
Epoch 00031: val_loss did not improve from 23.77139

Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 542us/sample - loss: 23.5283 - val_loss: 23.7924
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3656
Epoch 00032: val_loss did not improve from 23.77139
121/121 [==============================] - 0s 537us/sample - loss: 23.5193 - val_loss: 23.8007
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0802
Epoch 00033: val_loss did not improve from 23.77139

Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 518us/sample - loss: 23.5201 - val_loss: 23.8025
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.3670
Epoch 00034: val_loss did not improve from 23.77139
121/121 [==============================] - 0s 509us/sample - loss: 23.5171 - val_loss: 23.8036
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.7797
Epoch 00035: val_loss did not improve from 23.77139

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 527us/sample - loss: 23.5158 - val_loss: 23.7990
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.4394
Epoch 00036: val_loss did not improve from 23.77139
121/121 [==============================] - 0s 521us/sample - loss: 23.5119 - val_loss: 23.7966
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 24.8108
Epoch 00037: val_loss did not improve from 23.77139

Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 529us/sample - loss: 23.5108 - val_loss: 23.7987
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:209: RuntimeWarning: divide by zero encountered in log
  predictor_matrix[:,:,:,indicesPRED] = numpy.log(predictor_matrix[:,:,:,indicesPRED])
Epoch 00037: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F048/test/F048_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/train/F048_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F048/validate/F048_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
Traceback (most recent call last):
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "Run_CNN_FineTune_ByYear.py", line 347, in <module>
    model.load_weights(Wsave_name)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 234, in load_weights
    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1187, in load_weights
    py_checkpoint_reader.NewCheckpointReader(filepath)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 99, in NewCheckpointReader
    error_translator(e)
  File "/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F048/StartingYear2000/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
