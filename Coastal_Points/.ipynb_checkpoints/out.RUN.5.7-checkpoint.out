2020-11-09 19:56:49.959217: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-09 19:56:49.966878: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2020-11-09 19:56:49.967070: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b08ef4360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-09 19:56:49.967092: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-09 19:56:49.968771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-11-09 19:56:50.075700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:56:50.106739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:56:50.194643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:56:50.228535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:56:50.283879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:56:50.337095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:56:50.394462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:56:50.424212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:56:50.433479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:56:50.433602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:56:50.638986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:56:50.639081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:56:50.639100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:56:50.645564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 19:56:50.647819: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b09c6fa10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-09 19:56:50.647857: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-11-09 19:56:50.656566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:56:50.656673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:56:50.656694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:56:50.656710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:56:50.659915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:56:50.659936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:56:50.659951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:56:50.659967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:56:50.664505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:56:50.664567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:56:50.664580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:56:50.664591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:56:50.680977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 19:56:50.684452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:56:50.684546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:56:50.684570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:56:50.684587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:56:50.684602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:56:50.684617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:56:50.684632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:56:50.684648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:56:50.687635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:56:50.687676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:56:50.687688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:56:50.687699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:56:50.690698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
#############################################
post processing forecast: F024
#############################################
['/device:GPU:0']
#################################################
#################################################
SUCCESS: Found GPU: /device:GPU:0
#################################################
#################################################
We are here: /glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal
...Searching...: /glade/scratch/wchapman/AnEnCNN_good/Data/WestCoast/
/glade/scratch/wchapman/Reforecast/F000
/glade/scratch/wchapman/Reforecast/F006
/glade/scratch/wchapman/Reforecast/F012
/glade/scratch/wchapman/Reforecast/F018
/glade/scratch/wchapman/Reforecast/F024
Training on
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1985_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1986_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1987_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1988_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1989_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1990_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1991_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1992_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1993_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1994_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1995_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1996_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1997_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1998_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_1999_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2000_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2001_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2002_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2003_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2004_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2005_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2006_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2007_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2008_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2009_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2010_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc
/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc
Validating on
/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc
Testing on
/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc
['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
trainging yearss ['/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc']
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVT" = 164.8902, 156.8746
Mean and standard deviation for "p_sfc" = 984.2164, 61.8815
Mean and standard deviation for "u_tr_p" = 12.3434, 12.4062
Mean and standard deviation for "v_tr_p" = 1.3898, 13.4257
Mean and standard deviation for "Z_p" = 5583.7053, 199.9498
Mean and standard deviation for "IWV" = 13.8505, 7.9771
2020-11-09 19:56:58.456677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:56:58.457376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:56:58.457402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:56:58.457420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:56:58.457435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:56:58.457450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:56:58.457467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:56:58.457482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:56:58.460585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:56:58.462621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-11-09 19:56:58.462692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-11-09 19:56:58.462712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-11-09 19:56:58.462727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-11-09 19:56:58.462742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-11-09 19:56:58.462758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-11-09 19:56:58.462773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-11-09 19:56:58.462789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:56:58.466659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-09 19:56:58.466713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-09 19:56:58.466726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-09 19:56:58.466736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-09 19:56:58.469840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30591 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)
2020-11-09 19:57:01.538686: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-11-09 19:57:02.941409: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-11-09 19:57:02.957163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...


Mean and standard deviation for "IVTm" = 207.0601, 181.1471
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 72, 60, 6)    0           input_1[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d[0][0]             
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d[0][0]              
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add (Add)                       (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate[0][0]                
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d (Cropping2D)         (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
Train on 728 samples, validate on 121 samples
Epoch 1/50
 50/728 [=>............................] - ETA: 51s - loss: 159.7276100/728 [===>..........................] - ETA: 24s - loss: 159.4521300/728 [===========>..................] - ETA: 5s - loss: 157.4885 500/728 [===================>..........] - ETA: 1s - loss: 155.2152700/728 [===========================>..] - ETA: 0s - loss: 154.3128
Epoch 00001: val_loss improved from inf to 137.37105, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 5s 6ms/sample - loss: 153.4717 - val_loss: 137.3711
Epoch 2/50
 50/728 [=>............................] - ETA: 0s - loss: 121.7181200/728 [=======>......................] - ETA: 0s - loss: 112.4364350/728 [=============>................] - ETA: 0s - loss: 95.4881 550/728 [=====================>........] - ETA: 0s - loss: 81.2346
Epoch 00002: val_loss improved from 137.37105 to 54.53978, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 451us/sample - loss: 72.3096 - val_loss: 54.5398
Epoch 3/50
 50/728 [=>............................] - ETA: 0s - loss: 42.3323250/728 [=========>....................] - ETA: 0s - loss: 43.4798450/728 [=================>............] - ETA: 0s - loss: 42.4003650/728 [=========================>....] - ETA: 0s - loss: 41.9352
Epoch 00003: val_loss improved from 54.53978 to 45.21491, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 365us/sample - loss: 41.5530 - val_loss: 45.2149
Epoch 4/50
 50/728 [=>............................] - ETA: 0s - loss: 37.7866250/728 [=========>....................] - ETA: 0s - loss: 37.8915450/728 [=================>............] - ETA: 0s - loss: 36.9780650/728 [=========================>....] - ETA: 0s - loss: 35.9613
Epoch 00004: val_loss did not improve from 45.21491
728/728 [==============================] - 0s 319us/sample - loss: 35.5737 - val_loss: 52.4068
Epoch 5/50
 50/728 [=>............................] - ETA: 0s - loss: 32.9267250/728 [=========>....................] - ETA: 0s - loss: 31.7832450/728 [=================>............] - ETA: 0s - loss: 31.1448650/728 [=========================>....] - ETA: 0s - loss: 31.0108
Epoch 00005: val_loss did not improve from 45.21491

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
728/728 [==============================] - 0s 317us/sample - loss: 30.6717 - val_loss: 61.6594
Epoch 6/50
 50/728 [=>............................] - ETA: 0s - loss: 27.8438250/728 [=========>....................] - ETA: 0s - loss: 27.1401450/728 [=================>............] - ETA: 0s - loss: 27.1274650/728 [=========================>....] - ETA: 0s - loss: 27.0766
Epoch 00006: val_loss did not improve from 45.21491
728/728 [==============================] - 0s 317us/sample - loss: 27.1376 - val_loss: 55.6466
Epoch 7/50
 50/728 [=>............................] - ETA: 0s - loss: 26.7027250/728 [=========>....................] - ETA: 0s - loss: 26.9450450/728 [=================>............] - ETA: 0s - loss: 26.5333650/728 [=========================>....] - ETA: 0s - loss: 26.2916
Epoch 00007: val_loss did not improve from 45.21491

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
728/728 [==============================] - 0s 321us/sample - loss: 26.1320 - val_loss: 55.7934
Epoch 8/50
 50/728 [=>............................] - ETA: 0s - loss: 25.3710250/728 [=========>....................] - ETA: 0s - loss: 25.7646450/728 [=================>............] - ETA: 0s - loss: 25.5860650/728 [=========================>....] - ETA: 0s - loss: 25.3858
Epoch 00008: val_loss did not improve from 45.21491
728/728 [==============================] - 0s 329us/sample - loss: 25.6005 - val_loss: 50.1018
Epoch 9/50
 50/728 [=>............................] - ETA: 0s - loss: 25.4584250/728 [=========>....................] - ETA: 0s - loss: 24.8247450/728 [=================>............] - ETA: 0s - loss: 24.7308650/728 [=========================>....] - ETA: 0s - loss: 24.8343
Epoch 00009: val_loss improved from 45.21491 to 43.63719, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 356us/sample - loss: 25.0646 - val_loss: 43.6372
Epoch 10/50
 50/728 [=>............................] - ETA: 0s - loss: 24.9862250/728 [=========>....................] - ETA: 0s - loss: 25.1891450/728 [=================>............] - ETA: 0s - loss: 25.1208650/728 [=========================>....] - ETA: 0s - loss: 25.0472
Epoch 00010: val_loss improved from 43.63719 to 42.44461, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 383us/sample - loss: 24.9791 - val_loss: 42.4446
Epoch 11/50
 50/728 [=>............................] - ETA: 0s - loss: 24.2226250/728 [=========>....................] - ETA: 0s - loss: 24.6329450/728 [=================>............] - ETA: 0s - loss: 24.8387650/728 [=========================>....] - ETA: 0s - loss: 24.7692
Epoch 00011: val_loss improved from 42.44461 to 39.28348, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 364us/sample - loss: 24.7330 - val_loss: 39.2835
Epoch 12/50
 50/728 [=>............................] - ETA: 0s - loss: 25.5789250/728 [=========>....................] - ETA: 0s - loss: 24.5666450/728 [=================>............] - ETA: 0s - loss: 24.2243650/728 [=========================>....] - ETA: 0s - loss: 24.9374
Epoch 00012: val_loss improved from 39.28348 to 33.09984, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 383us/sample - loss: 24.8508 - val_loss: 33.0998
Epoch 13/50
 50/728 [=>............................] - ETA: 0s - loss: 24.9128250/728 [=========>....................] - ETA: 0s - loss: 24.8789450/728 [=================>............] - ETA: 0s - loss: 24.8657650/728 [=========================>....] - ETA: 0s - loss: 24.4670
Epoch 00013: val_loss did not improve from 33.09984
728/728 [==============================] - 0s 318us/sample - loss: 24.3683 - val_loss: 34.5149
Epoch 14/50
 50/728 [=>............................] - ETA: 0s - loss: 24.8384250/728 [=========>....................] - ETA: 0s - loss: 24.0105450/728 [=================>............] - ETA: 0s - loss: 23.8418650/728 [=========================>....] - ETA: 0s - loss: 23.8043
Epoch 00014: val_loss improved from 33.09984 to 30.05807, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 358us/sample - loss: 23.8983 - val_loss: 30.0581
Epoch 15/50
 50/728 [=>............................] - ETA: 0s - loss: 25.9808250/728 [=========>....................] - ETA: 0s - loss: 23.8374450/728 [=================>............] - ETA: 0s - loss: 23.8036650/728 [=========================>....] - ETA: 0s - loss: 23.6797
Epoch 00015: val_loss improved from 30.05807 to 28.99523, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 357us/sample - loss: 23.6189 - val_loss: 28.9952
Epoch 16/50
 50/728 [=>............................] - ETA: 0s - loss: 23.1670250/728 [=========>....................] - ETA: 0s - loss: 23.8930450/728 [=================>............] - ETA: 0s - loss: 23.7151650/728 [=========================>....] - ETA: 0s - loss: 23.5898
Epoch 00016: val_loss improved from 28.99523 to 28.17651, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 388us/sample - loss: 23.4955 - val_loss: 28.1765
Epoch 17/50
 50/728 [=>............................] - ETA: 0s - loss: 22.7819250/728 [=========>....................] - ETA: 0s - loss: 22.9301450/728 [=================>............] - ETA: 0s - loss: 23.2861650/728 [=========================>....] - ETA: 0s - loss: 23.2714
Epoch 00017: val_loss improved from 28.17651 to 25.53582, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 364us/sample - loss: 23.3090 - val_loss: 25.5358
Epoch 18/50
 50/728 [=>............................] - ETA: 0s - loss: 22.7688250/728 [=========>....................] - ETA: 0s - loss: 22.7083450/728 [=================>............] - ETA: 0s - loss: 22.8157650/728 [=========================>....] - ETA: 0s - loss: 22.9206
Epoch 00018: val_loss improved from 25.53582 to 24.40821, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 359us/sample - loss: 23.0076 - val_loss: 24.4082
Epoch 19/50
 50/728 [=>............................] - ETA: 0s - loss: 22.3002250/728 [=========>....................] - ETA: 0s - loss: 23.1331450/728 [=================>............] - ETA: 0s - loss: 23.2460650/728 [=========================>....] - ETA: 0s - loss: 22.9735
Epoch 00019: val_loss did not improve from 24.40821
728/728 [==============================] - 0s 321us/sample - loss: 22.8921 - val_loss: 25.1796
Epoch 20/50
 50/728 [=>............................] - ETA: 0s - loss: 22.4590250/728 [=========>....................] - ETA: 0s - loss: 22.9843450/728 [=================>............] - ETA: 0s - loss: 23.0647650/728 [=========================>....] - ETA: 0s - loss: 23.1646
Epoch 00020: val_loss improved from 24.40821 to 23.31188, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 374us/sample - loss: 23.1797 - val_loss: 23.3119
Epoch 21/50
 50/728 [=>............................] - ETA: 0s - loss: 22.0799250/728 [=========>....................] - ETA: 0s - loss: 21.9115450/728 [=================>............] - ETA: 0s - loss: 22.4333650/728 [=========================>....] - ETA: 0s - loss: 22.7380
Epoch 00021: val_loss improved from 23.31188 to 22.21745, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 369us/sample - loss: 22.7829 - val_loss: 22.2174
Epoch 22/50
 50/728 [=>............................] - ETA: 0s - loss: 23.7285250/728 [=========>....................] - ETA: 0s - loss: 22.8730450/728 [=================>............] - ETA: 0s - loss: 22.9446650/728 [=========================>....] - ETA: 0s - loss: 22.9068
Epoch 00022: val_loss did not improve from 22.21745
728/728 [==============================] - 0s 319us/sample - loss: 22.7821 - val_loss: 23.3362
Epoch 23/50
 50/728 [=>............................] - ETA: 0s - loss: 24.6475250/728 [=========>....................] - ETA: 0s - loss: 22.7955450/728 [=================>............] - ETA: 0s - loss: 22.5247650/728 [=========================>....] - ETA: 0s - loss: 22.5201
Epoch 00023: val_loss did not improve from 22.21745

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
728/728 [==============================] - 0s 303us/sample - loss: 22.4418 - val_loss: 22.5731
Epoch 24/50
 50/728 [=>............................] - ETA: 0s - loss: 21.9330250/728 [=========>....................] - ETA: 0s - loss: 22.2145450/728 [=================>............] - ETA: 0s - loss: 22.3161650/728 [=========================>....] - ETA: 0s - loss: 22.2909
Epoch 00024: val_loss improved from 22.21745 to 21.67762, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 359us/sample - loss: 22.2013 - val_loss: 21.6776
Epoch 25/50
 50/728 [=>............................] - ETA: 0s - loss: 21.3256250/728 [=========>....................] - ETA: 0s - loss: 21.9268450/728 [=================>............] - ETA: 0s - loss: 22.0054650/728 [=========================>....] - ETA: 0s - loss: 22.0352
Epoch 00025: val_loss improved from 21.67762 to 21.53274, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 365us/sample - loss: 22.0872 - val_loss: 21.5327
Epoch 26/50
 50/728 [=>............................] - ETA: 0s - loss: 22.3485250/728 [=========>....................] - ETA: 0s - loss: 21.8970450/728 [=================>............] - ETA: 0s - loss: 22.1478650/728 [=========================>....] - ETA: 0s - loss: 22.1469
Epoch 00026: val_loss improved from 21.53274 to 21.46772, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 414us/sample - loss: 22.0372 - val_loss: 21.4677
Epoch 27/50
 50/728 [=>............................] - ETA: 0s - loss: 21.5979250/728 [=========>....................] - ETA: 0s - loss: 22.7028450/728 [=================>............] - ETA: 0s - loss: 22.3287650/728 [=========================>....] - ETA: 0s - loss: 22.1016
Epoch 00027: val_loss improved from 21.46772 to 21.36779, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 350us/sample - loss: 21.9865 - val_loss: 21.3678
Epoch 28/50
 50/728 [=>............................] - ETA: 0s - loss: 21.7901250/728 [=========>....................] - ETA: 0s - loss: 21.9840450/728 [=================>............] - ETA: 0s - loss: 21.8815650/728 [=========================>....] - ETA: 0s - loss: 21.7461
Epoch 00028: val_loss improved from 21.36779 to 21.22885, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 373us/sample - loss: 21.8169 - val_loss: 21.2289
Epoch 29/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5937250/728 [=========>....................] - ETA: 0s - loss: 21.4278450/728 [=================>............] - ETA: 0s - loss: 21.6717650/728 [=========================>....] - ETA: 0s - loss: 21.9552
Epoch 00029: val_loss improved from 21.22885 to 21.03046, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 359us/sample - loss: 21.9198 - val_loss: 21.0305
Epoch 30/50
 50/728 [=>............................] - ETA: 0s - loss: 25.5683250/728 [=========>....................] - ETA: 0s - loss: 22.7397450/728 [=================>............] - ETA: 0s - loss: 22.3114650/728 [=========================>....] - ETA: 0s - loss: 22.0853
Epoch 00030: val_loss did not improve from 21.03046
728/728 [==============================] - 0s 316us/sample - loss: 22.1282 - val_loss: 21.3385
Epoch 31/50
 50/728 [=>............................] - ETA: 0s - loss: 20.8181250/728 [=========>....................] - ETA: 0s - loss: 22.0661450/728 [=================>............] - ETA: 0s - loss: 21.9302650/728 [=========================>....] - ETA: 0s - loss: 21.9305
Epoch 00031: val_loss improved from 21.03046 to 20.88954, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 371us/sample - loss: 21.9117 - val_loss: 20.8895
Epoch 32/50
 50/728 [=>............................] - ETA: 0s - loss: 21.6945250/728 [=========>....................] - ETA: 0s - loss: 21.4188450/728 [=================>............] - ETA: 0s - loss: 21.4370650/728 [=========================>....] - ETA: 0s - loss: 21.5020
Epoch 00032: val_loss improved from 20.88954 to 20.88138, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 360us/sample - loss: 21.7132 - val_loss: 20.8814
Epoch 33/50
 50/728 [=>............................] - ETA: 0s - loss: 21.0150250/728 [=========>....................] - ETA: 0s - loss: 21.6425450/728 [=================>............] - ETA: 0s - loss: 21.4300650/728 [=========================>....] - ETA: 0s - loss: 21.7846
Epoch 00033: val_loss improved from 20.88138 to 20.75859, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 372us/sample - loss: 21.6866 - val_loss: 20.7586
Epoch 34/50
 50/728 [=>............................] - ETA: 0s - loss: 21.2803250/728 [=========>....................] - ETA: 0s - loss: 20.9160450/728 [=================>............] - ETA: 0s - loss: 21.5500650/728 [=========================>....] - ETA: 0s - loss: 21.6922
Epoch 00034: val_loss improved from 20.75859 to 20.71644, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 367us/sample - loss: 21.6329 - val_loss: 20.7164
Epoch 35/50
 50/728 [=>............................] - ETA: 0s - loss: 22.9174250/728 [=========>....................] - ETA: 0s - loss: 21.9598450/728 [=================>............] - ETA: 0s - loss: 21.6656650/728 [=========================>....] - ETA: 0s - loss: 21.5846
Epoch 00035: val_loss improved from 20.71644 to 20.68550, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 367us/sample - loss: 21.5366 - val_loss: 20.6855
Epoch 36/50
 50/728 [=>............................] - ETA: 0s - loss: 20.8582250/728 [=========>....................] - ETA: 0s - loss: 21.4339450/728 [=================>............] - ETA: 0s - loss: 21.4363650/728 [=========================>....] - ETA: 0s - loss: 21.4675
Epoch 00036: val_loss improved from 20.68550 to 20.60901, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 370us/sample - loss: 21.5031 - val_loss: 20.6090
Epoch 37/50
 50/728 [=>............................] - ETA: 0s - loss: 20.8796250/728 [=========>....................] - ETA: 0s - loss: 20.9797450/728 [=================>............] - ETA: 0s - loss: 21.2839650/728 [=========================>....] - ETA: 0s - loss: 21.9052
Epoch 00037: val_loss improved from 20.60901 to 20.54625, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 357us/sample - loss: 21.8356 - val_loss: 20.5463
Epoch 38/50
 50/728 [=>............................] - ETA: 0s - loss: 21.0887250/728 [=========>....................] - ETA: 0s - loss: 22.0390450/728 [=================>............] - ETA: 0s - loss: 22.1033650/728 [=========================>....] - ETA: 0s - loss: 22.0615
Epoch 00038: val_loss improved from 20.54625 to 20.51707, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 361us/sample - loss: 21.9837 - val_loss: 20.5171
Epoch 39/50
 50/728 [=>............................] - ETA: 0s - loss: 20.7246250/728 [=========>....................] - ETA: 0s - loss: 21.0671450/728 [=================>............] - ETA: 0s - loss: 21.3894650/728 [=========================>....] - ETA: 0s - loss: 21.4392
Epoch 00039: val_loss improved from 20.51707 to 20.48789, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 492us/sample - loss: 21.4506 - val_loss: 20.4879
Epoch 40/50
 50/728 [=>............................] - ETA: 0s - loss: 21.1874250/728 [=========>....................] - ETA: 0s - loss: 21.0362450/728 [=================>............] - ETA: 0s - loss: 21.3476650/728 [=========================>....] - ETA: 0s - loss: 21.3296
Epoch 00040: val_loss improved from 20.48789 to 20.42031, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 361us/sample - loss: 21.3130 - val_loss: 20.4203
Epoch 41/50
 50/728 [=>............................] - ETA: 0s - loss: 20.6284250/728 [=========>....................] - ETA: 0s - loss: 21.3891450/728 [=================>............] - ETA: 0s - loss: 21.3404650/728 [=========================>....] - ETA: 0s - loss: 21.3169
Epoch 00041: val_loss improved from 20.42031 to 20.40613, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 361us/sample - loss: 21.3715 - val_loss: 20.4061
Epoch 42/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5959250/728 [=========>....................] - ETA: 0s - loss: 21.2125450/728 [=================>............] - ETA: 0s - loss: 21.0882650/728 [=========================>....] - ETA: 0s - loss: 21.1185
Epoch 00042: val_loss improved from 20.40613 to 20.34118, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 366us/sample - loss: 21.3290 - val_loss: 20.3412
Epoch 43/50
 50/728 [=>............................] - ETA: 0s - loss: 21.4505250/728 [=========>....................] - ETA: 0s - loss: 21.3958450/728 [=================>............] - ETA: 0s - loss: 21.4095650/728 [=========================>....] - ETA: 0s - loss: 21.4298
Epoch 00043: val_loss improved from 20.34118 to 20.29558, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 360us/sample - loss: 21.4042 - val_loss: 20.2956
Epoch 44/50
 50/728 [=>............................] - ETA: 0s - loss: 20.8608150/728 [=====>........................] - ETA: 0s - loss: 21.6983300/728 [===========>..................] - ETA: 0s - loss: 21.3512500/728 [===================>..........] - ETA: 0s - loss: 21.2784700/728 [===========================>..] - ETA: 0s - loss: 21.2749
Epoch 00044: val_loss improved from 20.29558 to 20.26456, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 428us/sample - loss: 21.2462 - val_loss: 20.2646
Epoch 45/50
 50/728 [=>............................] - ETA: 0s - loss: 20.2732250/728 [=========>....................] - ETA: 0s - loss: 21.1164450/728 [=================>............] - ETA: 0s - loss: 21.2897650/728 [=========================>....] - ETA: 0s - loss: 21.2586
Epoch 00045: val_loss improved from 20.26456 to 20.20027, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 368us/sample - loss: 21.2652 - val_loss: 20.2003
Epoch 46/50
 50/728 [=>............................] - ETA: 0s - loss: 22.0110250/728 [=========>....................] - ETA: 0s - loss: 21.3669450/728 [=================>............] - ETA: 0s - loss: 21.2226650/728 [=========================>....] - ETA: 0s - loss: 21.0842
Epoch 00046: val_loss did not improve from 20.20027
728/728 [==============================] - 0s 327us/sample - loss: 21.2859 - val_loss: 20.2280
Epoch 47/50
 50/728 [=>............................] - ETA: 0s - loss: 21.1216250/728 [=========>....................] - ETA: 0s - loss: 21.3770450/728 [=================>............] - ETA: 0s - loss: 21.3135650/728 [=========================>....] - ETA: 0s - loss: 21.3302
Epoch 00047: val_loss improved from 20.20027 to 20.16827, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 360us/sample - loss: 21.4909 - val_loss: 20.1683
Epoch 48/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5275250/728 [=========>....................] - ETA: 0s - loss: 20.8575450/728 [=================>............] - ETA: 0s - loss: 20.9426650/728 [=========================>....] - ETA: 0s - loss: 20.9574
Epoch 00048: val_loss improved from 20.16827 to 20.12994, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 355us/sample - loss: 20.9752 - val_loss: 20.1299
Epoch 49/50
 50/728 [=>............................] - ETA: 0s - loss: 20.9274250/728 [=========>....................] - ETA: 0s - loss: 21.4102450/728 [=================>............] - ETA: 0s - loss: 21.3332650/728 [=========================>....] - ETA: 0s - loss: 21.1592
Epoch 00049: val_loss improved from 20.12994 to 20.08842, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 365us/sample - loss: 21.1656 - val_loss: 20.0884
Epoch 50/50
 50/728 [=>............................] - ETA: 0s - loss: 20.0060250/728 [=========>....................] - ETA: 0s - loss: 20.4399450/728 [=================>............] - ETA: 0s - loss: 21.1341650/728 [=========================>....] - ETA: 0s - loss: 21.0251
Epoch 00050: val_loss improved from 20.08842 to 20.02414, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
728/728 [==============================] - 0s 353us/sample - loss: 21.0166 - val_loss: 20.0241
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 72, 60, 6)    0           input_2[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_1[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_1 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_1[0][0]                      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_1[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_1 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
Train on 728 samples, validate on 121 samples
Epoch 1/50
 50/728 [=>............................] - ETA: 13s - loss: 171.4477150/728 [=====>........................] - ETA: 3s - loss: 163.5554 350/728 [=============>................] - ETA: 1s - loss: 161.4209550/728 [=====================>........] - ETA: 0s - loss: 156.9691
Epoch 00001: val_loss improved from inf to 116.08188, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 1s 2ms/sample - loss: 148.9130 - val_loss: 116.0819
Epoch 2/50
 50/728 [=>............................] - ETA: 0s - loss: 91.2871250/728 [=========>....................] - ETA: 0s - loss: 65.5364450/728 [=================>............] - ETA: 0s - loss: 63.2288650/728 [=========================>....] - ETA: 0s - loss: 57.8101
Epoch 00002: val_loss improved from 116.08188 to 72.98398, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 345us/sample - loss: 56.3136 - val_loss: 72.9840
Epoch 3/50
 50/728 [=>............................] - ETA: 0s - loss: 51.0959250/728 [=========>....................] - ETA: 0s - loss: 46.0085450/728 [=================>............] - ETA: 0s - loss: 43.4228650/728 [=========================>....] - ETA: 0s - loss: 42.3615
Epoch 00003: val_loss improved from 72.98398 to 61.84128, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 331us/sample - loss: 41.9443 - val_loss: 61.8413
Epoch 4/50
 50/728 [=>............................] - ETA: 0s - loss: 35.8003250/728 [=========>....................] - ETA: 0s - loss: 36.3875450/728 [=================>............] - ETA: 0s - loss: 36.0959650/728 [=========================>....] - ETA: 0s - loss: 35.2305
Epoch 00004: val_loss did not improve from 61.84128
728/728 [==============================] - 0s 299us/sample - loss: 35.0845 - val_loss: 81.0253
Epoch 5/50
 50/728 [=>............................] - ETA: 0s - loss: 34.6055250/728 [=========>....................] - ETA: 0s - loss: 31.3331450/728 [=================>............] - ETA: 0s - loss: 30.5263650/728 [=========================>....] - ETA: 0s - loss: 30.0177
Epoch 00005: val_loss did not improve from 61.84128

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
728/728 [==============================] - 0s 307us/sample - loss: 29.8298 - val_loss: 76.3196
Epoch 6/50
 50/728 [=>............................] - ETA: 0s - loss: 26.5977250/728 [=========>....................] - ETA: 0s - loss: 27.4913450/728 [=================>............] - ETA: 0s - loss: 26.9551650/728 [=========================>....] - ETA: 0s - loss: 26.8962
Epoch 00006: val_loss did not improve from 61.84128
728/728 [==============================] - 0s 296us/sample - loss: 26.8638 - val_loss: 69.2630
Epoch 7/50
 50/728 [=>............................] - ETA: 0s - loss: 27.3846250/728 [=========>....................] - ETA: 0s - loss: 26.6097450/728 [=================>............] - ETA: 0s - loss: 25.9382550/728 [=====================>........] - ETA: 0s - loss: 26.0635
Epoch 00007: val_loss did not improve from 61.84128

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
728/728 [==============================] - 0s 350us/sample - loss: 25.8175 - val_loss: 66.4703
Epoch 8/50
 50/728 [=>............................] - ETA: 0s - loss: 25.2404250/728 [=========>....................] - ETA: 0s - loss: 25.2021450/728 [=================>............] - ETA: 0s - loss: 25.0506650/728 [=========================>....] - ETA: 0s - loss: 24.7697
Epoch 00008: val_loss improved from 61.84128 to 57.98326, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 336us/sample - loss: 24.9095 - val_loss: 57.9833
Epoch 9/50
 50/728 [=>............................] - ETA: 0s - loss: 25.3973250/728 [=========>....................] - ETA: 0s - loss: 24.6528450/728 [=================>............] - ETA: 0s - loss: 24.5649650/728 [=========================>....] - ETA: 0s - loss: 24.5903
Epoch 00009: val_loss improved from 57.98326 to 54.10356, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 325us/sample - loss: 24.4411 - val_loss: 54.1036
Epoch 10/50
 50/728 [=>............................] - ETA: 0s - loss: 23.4505250/728 [=========>....................] - ETA: 0s - loss: 24.2178500/728 [===================>..........] - ETA: 0s - loss: 24.3688
Epoch 00010: val_loss improved from 54.10356 to 48.61209, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 323us/sample - loss: 24.2933 - val_loss: 48.6121
Epoch 11/50
 50/728 [=>............................] - ETA: 0s - loss: 22.8618250/728 [=========>....................] - ETA: 0s - loss: 23.9293500/728 [===================>..........] - ETA: 0s - loss: 23.7260700/728 [===========================>..] - ETA: 0s - loss: 23.7560
Epoch 00011: val_loss improved from 48.61209 to 42.39884, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 328us/sample - loss: 23.9137 - val_loss: 42.3988
Epoch 12/50
 50/728 [=>............................] - ETA: 0s - loss: 23.8037250/728 [=========>....................] - ETA: 0s - loss: 23.8806450/728 [=================>............] - ETA: 0s - loss: 23.8033650/728 [=========================>....] - ETA: 0s - loss: 23.7744
Epoch 00012: val_loss improved from 42.39884 to 41.38482, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 330us/sample - loss: 23.6928 - val_loss: 41.3848
Epoch 13/50
 50/728 [=>............................] - ETA: 0s - loss: 26.1896250/728 [=========>....................] - ETA: 0s - loss: 23.9174450/728 [=================>............] - ETA: 0s - loss: 23.8271650/728 [=========================>....] - ETA: 0s - loss: 23.5886
Epoch 00013: val_loss improved from 41.38482 to 36.22821, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 338us/sample - loss: 23.6315 - val_loss: 36.2282
Epoch 14/50
 50/728 [=>............................] - ETA: 0s - loss: 23.5402250/728 [=========>....................] - ETA: 0s - loss: 22.9568450/728 [=================>............] - ETA: 0s - loss: 23.4501650/728 [=========================>....] - ETA: 0s - loss: 23.1867
Epoch 00014: val_loss improved from 36.22821 to 33.44756, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 361us/sample - loss: 23.2256 - val_loss: 33.4476
Epoch 15/50
 50/728 [=>............................] - ETA: 0s - loss: 22.2847250/728 [=========>....................] - ETA: 0s - loss: 22.7853450/728 [=================>............] - ETA: 0s - loss: 23.0292650/728 [=========================>....] - ETA: 0s - loss: 22.9226
Epoch 00015: val_loss improved from 33.44756 to 29.92707, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 635us/sample - loss: 22.9525 - val_loss: 29.9271
Epoch 16/50
 50/728 [=>............................] - ETA: 0s - loss: 22.5683250/728 [=========>....................] - ETA: 0s - loss: 22.4524450/728 [=================>............] - ETA: 0s - loss: 22.4997650/728 [=========================>....] - ETA: 0s - loss: 22.7617
Epoch 00016: val_loss improved from 29.92707 to 28.20407, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 348us/sample - loss: 22.7279 - val_loss: 28.2041
Epoch 17/50
 50/728 [=>............................] - ETA: 0s - loss: 22.3887250/728 [=========>....................] - ETA: 0s - loss: 22.7091450/728 [=================>............] - ETA: 0s - loss: 22.7284650/728 [=========================>....] - ETA: 0s - loss: 22.6728
Epoch 00017: val_loss improved from 28.20407 to 27.17241, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 367us/sample - loss: 22.7520 - val_loss: 27.1724
Epoch 18/50
 50/728 [=>............................] - ETA: 0s - loss: 21.7964250/728 [=========>....................] - ETA: 0s - loss: 22.5720450/728 [=================>............] - ETA: 0s - loss: 22.5609650/728 [=========================>....] - ETA: 0s - loss: 22.6862
Epoch 00018: val_loss improved from 27.17241 to 25.84892, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 347us/sample - loss: 22.6067 - val_loss: 25.8489
Epoch 19/50
 50/728 [=>............................] - ETA: 0s - loss: 22.1581250/728 [=========>....................] - ETA: 0s - loss: 22.1425450/728 [=================>............] - ETA: 0s - loss: 22.5439650/728 [=========================>....] - ETA: 0s - loss: 22.3962
Epoch 00019: val_loss improved from 25.84892 to 23.77030, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 354us/sample - loss: 22.3263 - val_loss: 23.7703
Epoch 20/50
 50/728 [=>............................] - ETA: 0s - loss: 22.5669250/728 [=========>....................] - ETA: 0s - loss: 22.4812450/728 [=================>............] - ETA: 0s - loss: 21.9543650/728 [=========================>....] - ETA: 0s - loss: 22.0153
Epoch 00020: val_loss improved from 23.77030 to 23.62831, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 346us/sample - loss: 22.1183 - val_loss: 23.6283
Epoch 21/50
 50/728 [=>............................] - ETA: 0s - loss: 21.4678250/728 [=========>....................] - ETA: 0s - loss: 21.8493450/728 [=================>............] - ETA: 0s - loss: 21.4909650/728 [=========================>....] - ETA: 0s - loss: 21.8507
Epoch 00021: val_loss improved from 23.62831 to 22.22491, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 340us/sample - loss: 22.0238 - val_loss: 22.2249
Epoch 22/50
 50/728 [=>............................] - ETA: 0s - loss: 22.8166250/728 [=========>....................] - ETA: 0s - loss: 22.2926450/728 [=================>............] - ETA: 0s - loss: 22.2203650/728 [=========================>....] - ETA: 0s - loss: 22.2569
Epoch 00022: val_loss did not improve from 22.22491
728/728 [==============================] - 0s 321us/sample - loss: 22.1813 - val_loss: 23.3267
Epoch 23/50
 50/728 [=>............................] - ETA: 0s - loss: 22.7158250/728 [=========>....................] - ETA: 0s - loss: 21.2956450/728 [=================>............] - ETA: 0s - loss: 21.6312650/728 [=========================>....] - ETA: 0s - loss: 21.7880
Epoch 00023: val_loss improved from 22.22491 to 21.18273, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 358us/sample - loss: 21.8972 - val_loss: 21.1827
Epoch 24/50
 50/728 [=>............................] - ETA: 0s - loss: 21.9096250/728 [=========>....................] - ETA: 0s - loss: 21.1512450/728 [=================>............] - ETA: 0s - loss: 21.2828650/728 [=========================>....] - ETA: 0s - loss: 21.7225
Epoch 00024: val_loss improved from 21.18273 to 20.95848, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 363us/sample - loss: 21.6978 - val_loss: 20.9585
Epoch 25/50
 50/728 [=>............................] - ETA: 0s - loss: 22.2603250/728 [=========>....................] - ETA: 0s - loss: 22.0758450/728 [=================>............] - ETA: 0s - loss: 22.1153650/728 [=========================>....] - ETA: 0s - loss: 22.1113
Epoch 00025: val_loss did not improve from 20.95848
728/728 [==============================] - 0s 330us/sample - loss: 22.0242 - val_loss: 21.1340
Epoch 26/50
 50/728 [=>............................] - ETA: 0s - loss: 20.8926250/728 [=========>....................] - ETA: 0s - loss: 22.2439450/728 [=================>............] - ETA: 0s - loss: 21.8750650/728 [=========================>....] - ETA: 0s - loss: 21.6724
Epoch 00026: val_loss did not improve from 20.95848

Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
728/728 [==============================] - 0s 315us/sample - loss: 21.6417 - val_loss: 21.3030
Epoch 27/50
 50/728 [=>............................] - ETA: 0s - loss: 20.8781250/728 [=========>....................] - ETA: 0s - loss: 20.8977450/728 [=================>............] - ETA: 0s - loss: 20.8696650/728 [=========================>....] - ETA: 0s - loss: 21.1771
Epoch 00027: val_loss improved from 20.95848 to 20.35225, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 368us/sample - loss: 21.2212 - val_loss: 20.3523
Epoch 28/50
 50/728 [=>............................] - ETA: 0s - loss: 21.0175250/728 [=========>....................] - ETA: 0s - loss: 21.1137450/728 [=================>............] - ETA: 0s - loss: 21.2382650/728 [=========================>....] - ETA: 0s - loss: 21.2485
Epoch 00028: val_loss improved from 20.35225 to 20.28518, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 367us/sample - loss: 21.2071 - val_loss: 20.2852
Epoch 29/50
 50/728 [=>............................] - ETA: 0s - loss: 21.3325250/728 [=========>....................] - ETA: 0s - loss: 21.0530450/728 [=================>............] - ETA: 0s - loss: 20.9351650/728 [=========================>....] - ETA: 0s - loss: 21.0671
Epoch 00029: val_loss did not improve from 20.28518
728/728 [==============================] - 0s 325us/sample - loss: 21.1316 - val_loss: 20.3083
Epoch 30/50
 50/728 [=>............................] - ETA: 0s - loss: 21.1706250/728 [=========>....................] - ETA: 0s - loss: 21.4058450/728 [=================>............] - ETA: 0s - loss: 21.1438650/728 [=========================>....] - ETA: 0s - loss: 21.1947
Epoch 00030: val_loss improved from 20.28518 to 19.99630, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 358us/sample - loss: 21.1836 - val_loss: 19.9963
Epoch 31/50
 50/728 [=>............................] - ETA: 0s - loss: 20.7744250/728 [=========>....................] - ETA: 0s - loss: 21.6777450/728 [=================>............] - ETA: 0s - loss: 21.3626650/728 [=========================>....] - ETA: 0s - loss: 21.2078
Epoch 00031: val_loss improved from 19.99630 to 19.93888, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 392us/sample - loss: 21.1241 - val_loss: 19.9389
Epoch 32/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5245200/728 [=======>......................] - ETA: 0s - loss: 21.2464400/728 [===============>..............] - ETA: 0s - loss: 21.2874600/728 [=======================>......] - ETA: 0s - loss: 21.0888
Epoch 00032: val_loss improved from 19.93888 to 19.86458, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 386us/sample - loss: 21.1609 - val_loss: 19.8646
Epoch 33/50
 50/728 [=>............................] - ETA: 0s - loss: 20.9057250/728 [=========>....................] - ETA: 0s - loss: 20.6803450/728 [=================>............] - ETA: 0s - loss: 20.8969650/728 [=========================>....] - ETA: 0s - loss: 20.8838
Epoch 00033: val_loss improved from 19.86458 to 19.69872, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 381us/sample - loss: 20.9093 - val_loss: 19.6987
Epoch 34/50
 50/728 [=>............................] - ETA: 0s - loss: 20.7157250/728 [=========>....................] - ETA: 0s - loss: 21.1585450/728 [=================>............] - ETA: 0s - loss: 21.0239650/728 [=========================>....] - ETA: 0s - loss: 20.8292
Epoch 00034: val_loss did not improve from 19.69872
728/728 [==============================] - 0s 323us/sample - loss: 20.8906 - val_loss: 19.7779
Epoch 35/50
 50/728 [=>............................] - ETA: 0s - loss: 20.6235250/728 [=========>....................] - ETA: 0s - loss: 20.9799450/728 [=================>............] - ETA: 0s - loss: 20.7980650/728 [=========================>....] - ETA: 0s - loss: 20.7959
Epoch 00035: val_loss improved from 19.69872 to 19.62782, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 387us/sample - loss: 20.8914 - val_loss: 19.6278
Epoch 36/50
 50/728 [=>............................] - ETA: 0s - loss: 20.4272250/728 [=========>....................] - ETA: 0s - loss: 21.0791450/728 [=================>............] - ETA: 0s - loss: 20.9356650/728 [=========================>....] - ETA: 0s - loss: 20.9633
Epoch 00036: val_loss improved from 19.62782 to 19.52801, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 368us/sample - loss: 20.9677 - val_loss: 19.5280
Epoch 37/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5529250/728 [=========>....................] - ETA: 0s - loss: 20.5770450/728 [=================>............] - ETA: 0s - loss: 20.5910650/728 [=========================>....] - ETA: 0s - loss: 20.7517
Epoch 00037: val_loss did not improve from 19.52801
728/728 [==============================] - 0s 338us/sample - loss: 20.7754 - val_loss: 19.5418
Epoch 38/50
 50/728 [=>............................] - ETA: 0s - loss: 20.3529200/728 [=======>......................] - ETA: 0s - loss: 21.1042400/728 [===============>..............] - ETA: 0s - loss: 20.7826600/728 [=======================>......] - ETA: 0s - loss: 20.8622
Epoch 00038: val_loss improved from 19.52801 to 19.41973, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 404us/sample - loss: 20.8247 - val_loss: 19.4197
Epoch 39/50
 50/728 [=>............................] - ETA: 0s - loss: 21.4141250/728 [=========>....................] - ETA: 0s - loss: 20.9748450/728 [=================>............] - ETA: 0s - loss: 20.7208650/728 [=========================>....] - ETA: 0s - loss: 20.6941
Epoch 00039: val_loss did not improve from 19.41973
728/728 [==============================] - 0s 334us/sample - loss: 20.7738 - val_loss: 19.4381
Epoch 40/50
 50/728 [=>............................] - ETA: 0s - loss: 21.8709250/728 [=========>....................] - ETA: 0s - loss: 20.9398450/728 [=================>............] - ETA: 0s - loss: 20.9612650/728 [=========================>....] - ETA: 0s - loss: 21.0076
Epoch 00040: val_loss did not improve from 19.41973

Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
728/728 [==============================] - 0s 332us/sample - loss: 21.1097 - val_loss: 19.4362
Epoch 41/50
 50/728 [=>............................] - ETA: 0s - loss: 20.7730250/728 [=========>....................] - ETA: 0s - loss: 20.2874450/728 [=================>............] - ETA: 0s - loss: 20.6258650/728 [=========================>....] - ETA: 0s - loss: 20.4813
Epoch 00041: val_loss improved from 19.41973 to 19.39522, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 400us/sample - loss: 20.7198 - val_loss: 19.3952
Epoch 42/50
 50/728 [=>............................] - ETA: 0s - loss: 20.7384250/728 [=========>....................] - ETA: 0s - loss: 20.4638400/728 [===============>..............] - ETA: 0s - loss: 20.4396600/728 [=======================>......] - ETA: 0s - loss: 20.6562
Epoch 00042: val_loss improved from 19.39522 to 19.28732, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 410us/sample - loss: 20.5277 - val_loss: 19.2873
Epoch 43/50
 50/728 [=>............................] - ETA: 0s - loss: 20.4706250/728 [=========>....................] - ETA: 0s - loss: 20.5710450/728 [=================>............] - ETA: 0s - loss: 20.6154650/728 [=========================>....] - ETA: 0s - loss: 20.6431
Epoch 00043: val_loss did not improve from 19.28732
728/728 [==============================] - 0s 323us/sample - loss: 20.6048 - val_loss: 19.3045
Epoch 44/50
 50/728 [=>............................] - ETA: 0s - loss: 21.5979250/728 [=========>....................] - ETA: 0s - loss: 20.8467450/728 [=================>............] - ETA: 0s - loss: 20.7771650/728 [=========================>....] - ETA: 0s - loss: 20.5583
Epoch 00044: val_loss improved from 19.28732 to 19.25716, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 368us/sample - loss: 20.5413 - val_loss: 19.2572
Epoch 45/50
 50/728 [=>............................] - ETA: 0s - loss: 20.7632250/728 [=========>....................] - ETA: 0s - loss: 20.4127450/728 [=================>............] - ETA: 0s - loss: 20.7656650/728 [=========================>....] - ETA: 0s - loss: 20.7686
Epoch 00045: val_loss did not improve from 19.25716
728/728 [==============================] - 0s 348us/sample - loss: 20.6636 - val_loss: 19.3362
Epoch 46/50
 50/728 [=>............................] - ETA: 0s - loss: 21.1078250/728 [=========>....................] - ETA: 0s - loss: 20.8960450/728 [=================>............] - ETA: 0s - loss: 20.6588650/728 [=========================>....] - ETA: 0s - loss: 20.6418
Epoch 00046: val_loss improved from 19.25716 to 19.25694, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 377us/sample - loss: 20.7109 - val_loss: 19.2569
Epoch 47/50
 50/728 [=>............................] - ETA: 0s - loss: 19.4638250/728 [=========>....................] - ETA: 0s - loss: 20.3795450/728 [=================>............] - ETA: 0s - loss: 20.1836650/728 [=========================>....] - ETA: 0s - loss: 20.3591
Epoch 00047: val_loss did not improve from 19.25694
728/728 [==============================] - 0s 341us/sample - loss: 20.5634 - val_loss: 19.2953
Epoch 48/50
 50/728 [=>............................] - ETA: 0s - loss: 20.5353250/728 [=========>....................] - ETA: 0s - loss: 20.3456450/728 [=================>............] - ETA: 0s - loss: 20.5824650/728 [=========================>....] - ETA: 0s - loss: 20.6222
Epoch 00048: val_loss improved from 19.25694 to 19.17086, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
728/728 [==============================] - 0s 378us/sample - loss: 20.5709 - val_loss: 19.1709
Epoch 49/50
 50/728 [=>............................] - ETA: 0s - loss: 19.7706250/728 [=========>....................] - ETA: 0s - loss: 20.5582450/728 [=================>............] - ETA: 0s - loss: 20.4206650/728 [=========================>....] - ETA: 0s - loss: 20.6248
Epoch 00049: val_loss did not improve from 19.17086
728/728 [==============================] - 0s 327us/sample - loss: 20.5852 - val_loss: 19.2103
Epoch 50/50
 50/728 [=>............................] - ETA: 0s - loss: 23.8965250/728 [=========>....................] - ETA: 0s - loss: 21.6861450/728 [=================>............] - ETA: 0s - loss: 21.0870650/728 [=========================>....] - ETA: 0s - loss: 20.8387
Epoch 00050: val_loss did not improve from 19.17086

Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
728/728 [==============================] - 0s 318us/sample - loss: 20.8454 - val_loss: 19.2626
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2011_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2012_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2014_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2015_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2013_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_2 (ZeroPadding2D (None, 72, 60, 6)    0           input_3[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_2[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_2 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_2[0][0]                      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_2[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_2 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
....saving.... /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
Train on 849 samples, validate on 121 samples
Epoch 1/50
 50/849 [>.............................] - ETA: 13s - loss: 165.2931150/849 [====>.........................] - ETA: 4s - loss: 155.8992 300/849 [=========>....................] - ETA: 1s - loss: 158.7229500/849 [================>.............] - ETA: 0s - loss: 155.5960650/849 [=====================>........] - ETA: 0s - loss: 152.3866
Epoch 00001: val_loss improved from inf to 99.74290, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 2s 3ms/sample - loss: 142.3669 - val_loss: 99.7429
Epoch 2/50
 50/849 [>.............................] - ETA: 0s - loss: 63.8007250/849 [=======>......................] - ETA: 0s - loss: 57.0080450/849 [==============>...............] - ETA: 0s - loss: 55.3095650/849 [=====================>........] - ETA: 0s - loss: 51.5541
Epoch 00002: val_loss improved from 99.74290 to 71.31297, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 355us/sample - loss: 50.0103 - val_loss: 71.3130
Epoch 3/50
 50/849 [>.............................] - ETA: 0s - loss: 42.9358250/849 [=======>......................] - ETA: 0s - loss: 40.0977450/849 [==============>...............] - ETA: 0s - loss: 39.7190650/849 [=====================>........] - ETA: 0s - loss: 38.3789
Epoch 00003: val_loss did not improve from 71.31297
849/849 [==============================] - 0s 326us/sample - loss: 37.4631 - val_loss: 77.7174
Epoch 4/50
 50/849 [>.............................] - ETA: 0s - loss: 31.0183250/849 [=======>......................] - ETA: 0s - loss: 32.7162450/849 [==============>...............] - ETA: 0s - loss: 32.0482650/849 [=====================>........] - ETA: 0s - loss: 31.5029
Epoch 00004: val_loss did not improve from 71.31297

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
849/849 [==============================] - 0s 323us/sample - loss: 31.1877 - val_loss: 81.5016
Epoch 5/50
 50/849 [>.............................] - ETA: 0s - loss: 29.8701250/849 [=======>......................] - ETA: 0s - loss: 29.7813450/849 [==============>...............] - ETA: 0s - loss: 28.7840650/849 [=====================>........] - ETA: 0s - loss: 28.5476
Epoch 00005: val_loss did not improve from 71.31297
849/849 [==============================] - 0s 316us/sample - loss: 28.4011 - val_loss: 75.9119
Epoch 6/50
 50/849 [>.............................] - ETA: 0s - loss: 26.5945250/849 [=======>......................] - ETA: 0s - loss: 27.4994450/849 [==============>...............] - ETA: 0s - loss: 27.1598650/849 [=====================>........] - ETA: 0s - loss: 27.0790
Epoch 00006: val_loss improved from 71.31297 to 68.42107, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 369us/sample - loss: 26.8700 - val_loss: 68.4211
Epoch 7/50
 50/849 [>.............................] - ETA: 0s - loss: 25.8914250/849 [=======>......................] - ETA: 0s - loss: 25.9207450/849 [==============>...............] - ETA: 0s - loss: 25.8890600/849 [====================>.........] - ETA: 0s - loss: 25.8791800/849 [===========================>..] - ETA: 0s - loss: 25.7235
Epoch 00007: val_loss improved from 68.42107 to 61.87244, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 395us/sample - loss: 25.6297 - val_loss: 61.8724
Epoch 8/50
 50/849 [>.............................] - ETA: 0s - loss: 24.5496250/849 [=======>......................] - ETA: 0s - loss: 24.9987450/849 [==============>...............] - ETA: 0s - loss: 25.0078650/849 [=====================>........] - ETA: 0s - loss: 24.8716
Epoch 00008: val_loss improved from 61.87244 to 55.70480, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 390us/sample - loss: 24.7031 - val_loss: 55.7048
Epoch 9/50
 50/849 [>.............................] - ETA: 0s - loss: 23.2568250/849 [=======>......................] - ETA: 0s - loss: 23.1746450/849 [==============>...............] - ETA: 0s - loss: 23.7597600/849 [====================>.........] - ETA: 0s - loss: 23.6336800/849 [===========================>..] - ETA: 0s - loss: 23.8120
Epoch 00009: val_loss improved from 55.70480 to 43.05762, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 399us/sample - loss: 23.8213 - val_loss: 43.0576
Epoch 10/50
 50/849 [>.............................] - ETA: 0s - loss: 23.1024250/849 [=======>......................] - ETA: 0s - loss: 23.4190450/849 [==============>...............] - ETA: 0s - loss: 23.6875650/849 [=====================>........] - ETA: 0s - loss: 23.7022
Epoch 00010: val_loss improved from 43.05762 to 40.36514, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 397us/sample - loss: 23.4605 - val_loss: 40.3651
Epoch 11/50
 50/849 [>.............................] - ETA: 0s - loss: 22.2186250/849 [=======>......................] - ETA: 0s - loss: 23.4528450/849 [==============>...............] - ETA: 0s - loss: 22.8396650/849 [=====================>........] - ETA: 0s - loss: 22.7747
Epoch 00011: val_loss improved from 40.36514 to 39.34251, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 384us/sample - loss: 22.7936 - val_loss: 39.3425
Epoch 12/50
 50/849 [>.............................] - ETA: 0s - loss: 21.2532250/849 [=======>......................] - ETA: 0s - loss: 22.8915450/849 [==============>...............] - ETA: 0s - loss: 22.8190650/849 [=====================>........] - ETA: 0s - loss: 22.8161
Epoch 00012: val_loss improved from 39.34251 to 37.55120, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 376us/sample - loss: 22.9206 - val_loss: 37.5512
Epoch 13/50
 50/849 [>.............................] - ETA: 0s - loss: 22.2572250/849 [=======>......................] - ETA: 0s - loss: 23.4512450/849 [==============>...............] - ETA: 0s - loss: 23.2078650/849 [=====================>........] - ETA: 0s - loss: 22.6451
Epoch 00013: val_loss improved from 37.55120 to 30.41807, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 347us/sample - loss: 22.7035 - val_loss: 30.4181
Epoch 14/50
 50/849 [>.............................] - ETA: 0s - loss: 21.9932250/849 [=======>......................] - ETA: 0s - loss: 22.3688450/849 [==============>...............] - ETA: 0s - loss: 22.0790650/849 [=====================>........] - ETA: 0s - loss: 22.3375
Epoch 00014: val_loss improved from 30.41807 to 22.71701, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 335us/sample - loss: 22.7178 - val_loss: 22.7170
Epoch 15/50
 50/849 [>.............................] - ETA: 0s - loss: 23.2399250/849 [=======>......................] - ETA: 0s - loss: 22.5626450/849 [==============>...............] - ETA: 0s - loss: 22.0821650/849 [=====================>........] - ETA: 0s - loss: 22.0499
Epoch 00015: val_loss improved from 22.71701 to 22.39088, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 337us/sample - loss: 21.8952 - val_loss: 22.3909
Epoch 16/50
 50/849 [>.............................] - ETA: 0s - loss: 21.2592250/849 [=======>......................] - ETA: 0s - loss: 20.7205450/849 [==============>...............] - ETA: 0s - loss: 20.4688650/849 [=====================>........] - ETA: 0s - loss: 20.8859
Epoch 00016: val_loss improved from 22.39088 to 20.60372, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 364us/sample - loss: 21.1785 - val_loss: 20.6037
Epoch 17/50
 50/849 [>.............................] - ETA: 0s - loss: 22.1968250/849 [=======>......................] - ETA: 0s - loss: 20.8281400/849 [=============>................] - ETA: 0s - loss: 21.3320600/849 [====================>.........] - ETA: 0s - loss: 21.2980800/849 [===========================>..] - ETA: 0s - loss: 20.9870
Epoch 00017: val_loss did not improve from 20.60372
849/849 [==============================] - 0s 356us/sample - loss: 20.8888 - val_loss: 22.3517
Epoch 18/50
 50/849 [>.............................] - ETA: 0s - loss: 19.6687250/849 [=======>......................] - ETA: 0s - loss: 20.2022450/849 [==============>...............] - ETA: 0s - loss: 20.2168650/849 [=====================>........] - ETA: 0s - loss: 20.2723
Epoch 00018: val_loss did not improve from 20.60372

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
849/849 [==============================] - 0s 315us/sample - loss: 20.2752 - val_loss: 22.1941
Epoch 19/50
 50/849 [>.............................] - ETA: 0s - loss: 20.1009250/849 [=======>......................] - ETA: 0s - loss: 20.2352450/849 [==============>...............] - ETA: 0s - loss: 20.3848650/849 [=====================>........] - ETA: 0s - loss: 20.3822
Epoch 00019: val_loss did not improve from 20.60372
849/849 [==============================] - 0s 317us/sample - loss: 20.2030 - val_loss: 20.7658
Epoch 20/50
 50/849 [>.............................] - ETA: 0s - loss: 18.9163250/849 [=======>......................] - ETA: 0s - loss: 19.8494450/849 [==============>...............] - ETA: 0s - loss: 20.1105650/849 [=====================>........] - ETA: 0s - loss: 20.2727
Epoch 00020: val_loss improved from 20.60372 to 19.48252, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 390us/sample - loss: 20.2349 - val_loss: 19.4825
Epoch 21/50
 50/849 [>.............................] - ETA: 0s - loss: 20.3276250/849 [=======>......................] - ETA: 0s - loss: 20.0266450/849 [==============>...............] - ETA: 0s - loss: 20.2167650/849 [=====================>........] - ETA: 0s - loss: 19.8946
Epoch 00021: val_loss did not improve from 19.48252
849/849 [==============================] - 0s 309us/sample - loss: 19.9185 - val_loss: 19.7719
Epoch 22/50
 50/849 [>.............................] - ETA: 0s - loss: 19.6758250/849 [=======>......................] - ETA: 0s - loss: 20.1750450/849 [==============>...............] - ETA: 0s - loss: 20.0902650/849 [=====================>........] - ETA: 0s - loss: 20.1120
Epoch 00022: val_loss did not improve from 19.48252

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
849/849 [==============================] - 0s 311us/sample - loss: 20.1174 - val_loss: 19.5364
Epoch 23/50
 50/849 [>.............................] - ETA: 0s - loss: 19.6544250/849 [=======>......................] - ETA: 0s - loss: 20.4068400/849 [=============>................] - ETA: 0s - loss: 20.0409550/849 [==================>...........] - ETA: 0s - loss: 20.0174700/849 [=======================>......] - ETA: 0s - loss: 19.8923
Epoch 00023: val_loss did not improve from 19.48252
849/849 [==============================] - 0s 380us/sample - loss: 20.0544 - val_loss: 19.5290
Epoch 24/50
 50/849 [>.............................] - ETA: 0s - loss: 19.3669200/849 [======>.......................] - ETA: 0s - loss: 19.3342350/849 [===========>..................] - ETA: 0s - loss: 19.4264500/849 [================>.............] - ETA: 0s - loss: 19.7454700/849 [=======================>......] - ETA: 0s - loss: 19.7788
Epoch 00024: val_loss improved from 19.48252 to 19.14513, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 419us/sample - loss: 19.7946 - val_loss: 19.1451
Epoch 25/50
 50/849 [>.............................] - ETA: 0s - loss: 21.0010200/849 [======>.......................] - ETA: 0s - loss: 19.9356350/849 [===========>..................] - ETA: 0s - loss: 19.7305550/849 [==================>...........] - ETA: 0s - loss: 19.8629700/849 [=======================>......] - ETA: 0s - loss: 19.9279
Epoch 00025: val_loss improved from 19.14513 to 19.09106, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 468us/sample - loss: 19.8656 - val_loss: 19.0911
Epoch 26/50
 50/849 [>.............................] - ETA: 0s - loss: 18.7023200/849 [======>.......................] - ETA: 0s - loss: 21.1271400/849 [=============>................] - ETA: 0s - loss: 20.3796550/849 [==================>...........] - ETA: 0s - loss: 20.2003700/849 [=======================>......] - ETA: 0s - loss: 20.0208
Epoch 00026: val_loss did not improve from 19.09106
849/849 [==============================] - 0s 384us/sample - loss: 19.9466 - val_loss: 19.1543
Epoch 27/50
 50/849 [>.............................] - ETA: 0s - loss: 19.2655200/849 [======>.......................] - ETA: 0s - loss: 19.5583350/849 [===========>..................] - ETA: 0s - loss: 19.4161500/849 [================>.............] - ETA: 0s - loss: 19.7912700/849 [=======================>......] - ETA: 0s - loss: 19.9165
Epoch 00027: val_loss improved from 19.09106 to 18.84313, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 413us/sample - loss: 19.8744 - val_loss: 18.8431
Epoch 28/50
 50/849 [>.............................] - ETA: 0s - loss: 19.1218250/849 [=======>......................] - ETA: 0s - loss: 19.4836450/849 [==============>...............] - ETA: 0s - loss: 19.7211600/849 [====================>.........] - ETA: 0s - loss: 19.7104800/849 [===========================>..] - ETA: 0s - loss: 19.8202
Epoch 00028: val_loss improved from 18.84313 to 18.82394, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 388us/sample - loss: 19.7956 - val_loss: 18.8239
Epoch 29/50
 50/849 [>.............................] - ETA: 0s - loss: 19.2565200/849 [======>.......................] - ETA: 0s - loss: 19.4219400/849 [=============>................] - ETA: 0s - loss: 19.4069600/849 [====================>.........] - ETA: 0s - loss: 19.7006800/849 [===========================>..] - ETA: 0s - loss: 19.6259
Epoch 00029: val_loss improved from 18.82394 to 18.72153, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 377us/sample - loss: 19.5893 - val_loss: 18.7215
Epoch 30/50
 50/849 [>.............................] - ETA: 0s - loss: 20.1345250/849 [=======>......................] - ETA: 0s - loss: 19.4715450/849 [==============>...............] - ETA: 0s - loss: 19.3020650/849 [=====================>........] - ETA: 0s - loss: 19.2592
Epoch 00030: val_loss did not improve from 18.72153
849/849 [==============================] - 0s 344us/sample - loss: 19.4987 - val_loss: 18.8088
Epoch 31/50
 50/849 [>.............................] - ETA: 0s - loss: 18.9977250/849 [=======>......................] - ETA: 0s - loss: 19.5185450/849 [==============>...............] - ETA: 0s - loss: 19.8067650/849 [=====================>........] - ETA: 0s - loss: 19.7424
Epoch 00031: val_loss improved from 18.72153 to 18.59449, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 363us/sample - loss: 19.7201 - val_loss: 18.5945
Epoch 32/50
 50/849 [>.............................] - ETA: 0s - loss: 19.1808200/849 [======>.......................] - ETA: 0s - loss: 19.3599400/849 [=============>................] - ETA: 0s - loss: 19.2866600/849 [====================>.........] - ETA: 0s - loss: 19.4357800/849 [===========================>..] - ETA: 0s - loss: 19.4135
Epoch 00032: val_loss did not improve from 18.59449
849/849 [==============================] - 0s 341us/sample - loss: 19.7432 - val_loss: 18.7033
Epoch 33/50
 50/849 [>.............................] - ETA: 0s - loss: 19.2849250/849 [=======>......................] - ETA: 0s - loss: 19.7317450/849 [==============>...............] - ETA: 0s - loss: 19.6967650/849 [=====================>........] - ETA: 0s - loss: 19.4979
Epoch 00033: val_loss improved from 18.59449 to 18.52784, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 367us/sample - loss: 19.5789 - val_loss: 18.5278
Epoch 34/50
 50/849 [>.............................] - ETA: 0s - loss: 19.1087250/849 [=======>......................] - ETA: 0s - loss: 19.3271450/849 [==============>...............] - ETA: 0s - loss: 19.4744650/849 [=====================>........] - ETA: 0s - loss: 19.3783
Epoch 00034: val_loss did not improve from 18.52784
849/849 [==============================] - 0s 330us/sample - loss: 19.4780 - val_loss: 18.6022
Epoch 35/50
 50/849 [>.............................] - ETA: 0s - loss: 19.0180250/849 [=======>......................] - ETA: 0s - loss: 19.2967450/849 [==============>...............] - ETA: 0s - loss: 19.3213650/849 [=====================>........] - ETA: 0s - loss: 19.4814
Epoch 00035: val_loss improved from 18.52784 to 18.45612, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 369us/sample - loss: 19.4336 - val_loss: 18.4561
Epoch 36/50
 50/849 [>.............................] - ETA: 0s - loss: 19.6688250/849 [=======>......................] - ETA: 0s - loss: 19.8555450/849 [==============>...............] - ETA: 0s - loss: 19.5353650/849 [=====================>........] - ETA: 0s - loss: 19.3993
Epoch 00036: val_loss did not improve from 18.45612
849/849 [==============================] - 0s 326us/sample - loss: 19.4525 - val_loss: 18.5863
Epoch 37/50
 50/849 [>.............................] - ETA: 0s - loss: 19.4825250/849 [=======>......................] - ETA: 0s - loss: 19.3946400/849 [=============>................] - ETA: 0s - loss: 19.5323600/849 [====================>.........] - ETA: 0s - loss: 19.3570800/849 [===========================>..] - ETA: 0s - loss: 19.4267
Epoch 00037: val_loss improved from 18.45612 to 18.41139, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 379us/sample - loss: 19.3594 - val_loss: 18.4114
Epoch 38/50
 50/849 [>.............................] - ETA: 0s - loss: 19.3532250/849 [=======>......................] - ETA: 0s - loss: 19.2550450/849 [==============>...............] - ETA: 0s - loss: 19.6448650/849 [=====================>........] - ETA: 0s - loss: 19.6841
Epoch 00038: val_loss did not improve from 18.41139
849/849 [==============================] - 0s 317us/sample - loss: 19.6507 - val_loss: 18.4431
Epoch 39/50
 50/849 [>.............................] - ETA: 0s - loss: 18.3484250/849 [=======>......................] - ETA: 0s - loss: 19.3478450/849 [==============>...............] - ETA: 0s - loss: 19.3390650/849 [=====================>........] - ETA: 0s - loss: 19.3375
Epoch 00039: val_loss improved from 18.41139 to 18.40000, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 367us/sample - loss: 19.5138 - val_loss: 18.4000
Epoch 40/50
 50/849 [>.............................] - ETA: 0s - loss: 19.3949250/849 [=======>......................] - ETA: 0s - loss: 19.0506450/849 [==============>...............] - ETA: 0s - loss: 19.6383650/849 [=====================>........] - ETA: 0s - loss: 19.4502
Epoch 00040: val_loss improved from 18.40000 to 18.37374, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 358us/sample - loss: 19.5033 - val_loss: 18.3737
Epoch 41/50
 50/849 [>.............................] - ETA: 0s - loss: 19.3617250/849 [=======>......................] - ETA: 0s - loss: 20.2894450/849 [==============>...............] - ETA: 0s - loss: 20.0232650/849 [=====================>........] - ETA: 0s - loss: 19.7133
Epoch 00041: val_loss did not improve from 18.37374
849/849 [==============================] - 0s 321us/sample - loss: 19.6047 - val_loss: 18.4001
Epoch 42/50
 50/849 [>.............................] - ETA: 0s - loss: 18.0765250/849 [=======>......................] - ETA: 0s - loss: 18.9279450/849 [==============>...............] - ETA: 0s - loss: 19.8213650/849 [=====================>........] - ETA: 0s - loss: 19.6912
Epoch 00042: val_loss improved from 18.37374 to 18.34181, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 359us/sample - loss: 19.6528 - val_loss: 18.3418
Epoch 43/50
 50/849 [>.............................] - ETA: 0s - loss: 18.6949250/849 [=======>......................] - ETA: 0s - loss: 19.4685450/849 [==============>...............] - ETA: 0s - loss: 19.8763650/849 [=====================>........] - ETA: 0s - loss: 19.6080
Epoch 00043: val_loss improved from 18.34181 to 18.31023, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 360us/sample - loss: 19.6682 - val_loss: 18.3102
Epoch 44/50
 50/849 [>.............................] - ETA: 0s - loss: 19.2195200/849 [======>.......................] - ETA: 0s - loss: 19.6755400/849 [=============>................] - ETA: 0s - loss: 19.8528600/849 [====================>.........] - ETA: 0s - loss: 19.6096800/849 [===========================>..] - ETA: 0s - loss: 19.5644
Epoch 00044: val_loss improved from 18.31023 to 18.30735, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 373us/sample - loss: 19.5370 - val_loss: 18.3073
Epoch 45/50
 50/849 [>.............................] - ETA: 0s - loss: 19.1480250/849 [=======>......................] - ETA: 0s - loss: 19.6783450/849 [==============>...............] - ETA: 0s - loss: 19.2953650/849 [=====================>........] - ETA: 0s - loss: 19.5356
Epoch 00045: val_loss improved from 18.30735 to 18.27635, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 370us/sample - loss: 19.4455 - val_loss: 18.2764
Epoch 46/50
 50/849 [>.............................] - ETA: 0s - loss: 20.4268250/849 [=======>......................] - ETA: 0s - loss: 19.1881450/849 [==============>...............] - ETA: 0s - loss: 19.4527650/849 [=====================>........] - ETA: 0s - loss: 19.4285
Epoch 00046: val_loss improved from 18.27635 to 18.24903, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 381us/sample - loss: 19.4650 - val_loss: 18.2490
Epoch 47/50
 50/849 [>.............................] - ETA: 0s - loss: 18.6487250/849 [=======>......................] - ETA: 0s - loss: 19.4650450/849 [==============>...............] - ETA: 0s - loss: 19.3809650/849 [=====================>........] - ETA: 0s - loss: 19.2504
Epoch 00047: val_loss did not improve from 18.24903
849/849 [==============================] - 0s 329us/sample - loss: 19.3536 - val_loss: 18.3700
Epoch 48/50
 50/849 [>.............................] - ETA: 0s - loss: 19.7991250/849 [=======>......................] - ETA: 0s - loss: 19.0062450/849 [==============>...............] - ETA: 0s - loss: 19.3067650/849 [=====================>........] - ETA: 0s - loss: 19.3887
Epoch 00048: val_loss improved from 18.24903 to 18.18522, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 378us/sample - loss: 19.2975 - val_loss: 18.1852
Epoch 49/50
 50/849 [>.............................] - ETA: 0s - loss: 18.5526250/849 [=======>......................] - ETA: 0s - loss: 19.4457450/849 [==============>...............] - ETA: 0s - loss: 19.6112650/849 [=====================>........] - ETA: 0s - loss: 19.4777
Epoch 00049: val_loss did not improve from 18.18522
849/849 [==============================] - 0s 334us/sample - loss: 19.3993 - val_loss: 18.2346
Epoch 50/50
 50/849 [>.............................] - ETA: 0s - loss: 18.6652250/849 [=======>......................] - ETA: 0s - loss: 19.6855450/849 [==============>...............] - ETA: 0s - loss: 19.4541650/849 [=====================>........] - ETA: 0s - loss: 19.4288
Epoch 00050: val_loss improved from 18.18522 to 18.17713, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
849/849 [==============================] - 0s 351us/sample - loss: 19.3701 - val_loss: 18.1771
/glade/work/wchapman/miniconda3/envs/tfp/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/glade/work/wchapman/AnEn/CNN/Coastal_Points_LogNormal/utilsProb.py:216: RuntimeWarning: invalid value encountered in log
  post_matrix = numpy.log(post_matrix)
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
xnan: 0
ynan: 0
x_tstnan: 0
y_tstnan: 0
['/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc', '/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc']
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_val_2016_test_2017.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ae0df0d49b0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ae0df0da860> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df0da240> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0df0da128> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df0c2cc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df0dde10> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0df50dcc0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df50dc18> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ae0df517208> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df53bf60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df5449b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df54a710> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ae0df555f98> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ae0df55e2e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ae0df56acf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df56abe0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df5847f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df584a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df584be0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df58e9b0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ae0df58eb38> False
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 72, 60, 6)    0           input_4[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_3 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_3[0][0]                      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_3[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_3 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 71, 57, 32)   608         cropping2d_3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 71, 57, 2)    578         conv2d[0][0]                     
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ae0df0d49b0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ae0df0da860> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df0da240> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0df0da128> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df0c2cc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df0dde10> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0df50dcc0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df50dc18> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ae0df517208> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df53bf60> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df5449b0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df54a710> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ae0df555f98> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ae0df55e2e8> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ae0df56acf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df56abe0> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df5847f0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df584a58> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0df584be0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df58e9b0> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ae0df58eb38> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0de5194e0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df5c1f60> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 4s - loss: 71.6659
Epoch 00001: val_loss improved from inf to 35.30059, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 1s 11ms/sample - loss: 55.4295 - val_loss: 35.3006
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 36.8446
Epoch 00002: val_loss improved from 35.30059 to 29.17673, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 839us/sample - loss: 30.2257 - val_loss: 29.1767
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.9560
Epoch 00003: val_loss improved from 29.17673 to 25.45358, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 862us/sample - loss: 27.5773 - val_loss: 25.4536
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.5113
Epoch 00004: val_loss improved from 25.45358 to 21.73382, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 709us/sample - loss: 22.7531 - val_loss: 21.7338
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2227
Epoch 00005: val_loss did not improve from 21.73382
121/121 [==============================] - 0s 537us/sample - loss: 22.1940 - val_loss: 21.7763
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3543
Epoch 00006: val_loss did not improve from 21.73382

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 630us/sample - loss: 21.4363 - val_loss: 21.7845
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0586
Epoch 00007: val_loss improved from 21.73382 to 21.45780, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 799us/sample - loss: 21.4227 - val_loss: 21.4578
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0393
Epoch 00008: val_loss improved from 21.45780 to 21.25339, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 856us/sample - loss: 21.1888 - val_loss: 21.2534
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3258
Epoch 00009: val_loss improved from 21.25339 to 21.22369, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 779us/sample - loss: 21.1529 - val_loss: 21.2237
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.7553
Epoch 00010: val_loss improved from 21.22369 to 21.20299, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 764us/sample - loss: 21.1202 - val_loss: 21.2030
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4327
Epoch 00011: val_loss did not improve from 21.20299
121/121 [==============================] - 0s 605us/sample - loss: 21.0728 - val_loss: 21.2116
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9728
Epoch 00012: val_loss improved from 21.20299 to 21.16147, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 951us/sample - loss: 21.0381 - val_loss: 21.1615
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6987
Epoch 00013: val_loss improved from 21.16147 to 21.14581, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 830us/sample - loss: 21.0050 - val_loss: 21.1458
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6258
Epoch 00014: val_loss improved from 21.14581 to 21.11335, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 820us/sample - loss: 20.9774 - val_loss: 21.1134
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1867
Epoch 00015: val_loss improved from 21.11335 to 21.06687, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 830us/sample - loss: 20.9430 - val_loss: 21.0669
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7387
Epoch 00016: val_loss improved from 21.06687 to 21.02698, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 808us/sample - loss: 20.9385 - val_loss: 21.0270
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9547
Epoch 00017: val_loss did not improve from 21.02698
121/121 [==============================] - 0s 655us/sample - loss: 20.8849 - val_loss: 21.0396
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0996
Epoch 00018: val_loss did not improve from 21.02698

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 606us/sample - loss: 20.8751 - val_loss: 21.0274
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3742
Epoch 00019: val_loss improved from 21.02698 to 20.98747, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 2ms/sample - loss: 20.8368 - val_loss: 20.9875
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8488
Epoch 00020: val_loss improved from 20.98747 to 20.95162, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 772us/sample - loss: 20.8148 - val_loss: 20.9516
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9640
Epoch 00021: val_loss improved from 20.95162 to 20.93240, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 818us/sample - loss: 20.8051 - val_loss: 20.9324
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.5381
Epoch 00022: val_loss improved from 20.93240 to 20.92954, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 764us/sample - loss: 20.7990 - val_loss: 20.9295
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5592
Epoch 00023: val_loss did not improve from 20.92954
121/121 [==============================] - 0s 563us/sample - loss: 20.7831 - val_loss: 20.9318
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8104
Epoch 00024: val_loss did not improve from 20.92954

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 551us/sample - loss: 20.7668 - val_loss: 20.9309
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5138
Epoch 00025: val_loss improved from 20.92954 to 20.92052, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 754us/sample - loss: 20.7529 - val_loss: 20.9205
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9699
Epoch 00026: val_loss improved from 20.92052 to 20.90647, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 925us/sample - loss: 20.7453 - val_loss: 20.9065
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4713
Epoch 00027: val_loss improved from 20.90647 to 20.89593, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 856us/sample - loss: 20.7372 - val_loss: 20.8959
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3993
Epoch 00028: val_loss improved from 20.89593 to 20.88772, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.7325 - val_loss: 20.8877
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4056
Epoch 00029: val_loss improved from 20.88772 to 20.88195, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.7268 - val_loss: 20.8819
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4536
Epoch 00030: val_loss improved from 20.88195 to 20.87415, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 793us/sample - loss: 20.7213 - val_loss: 20.8741
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8943
Epoch 00031: val_loss improved from 20.87415 to 20.86552, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 880us/sample - loss: 20.7150 - val_loss: 20.8655
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1501
Epoch 00032: val_loss improved from 20.86552 to 20.86152, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 898us/sample - loss: 20.7093 - val_loss: 20.8615
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4615
Epoch 00033: val_loss improved from 20.86152 to 20.85992, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 910us/sample - loss: 20.6996 - val_loss: 20.8599
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2799
Epoch 00034: val_loss did not improve from 20.85992
121/121 [==============================] - 0s 619us/sample - loss: 20.6906 - val_loss: 20.8622
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3212
Epoch 00035: val_loss improved from 20.85992 to 20.85651, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 878us/sample - loss: 20.6843 - val_loss: 20.8565
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8288
Epoch 00036: val_loss improved from 20.85651 to 20.84677, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 812us/sample - loss: 20.6795 - val_loss: 20.8468
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.5911
Epoch 00037: val_loss improved from 20.84677 to 20.84377, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 817us/sample - loss: 20.6715 - val_loss: 20.8438
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2870
Epoch 00038: val_loss did not improve from 20.84377
121/121 [==============================] - 0s 613us/sample - loss: 20.6626 - val_loss: 20.8467
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5622
Epoch 00039: val_loss did not improve from 20.84377

Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 626us/sample - loss: 20.6567 - val_loss: 20.8490
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4794
Epoch 00040: val_loss did not improve from 20.84377
121/121 [==============================] - 0s 573us/sample - loss: 20.6523 - val_loss: 20.8451
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9631
Epoch 00041: val_loss improved from 20.84377 to 20.84364, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 788us/sample - loss: 20.6500 - val_loss: 20.8436
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8475
Epoch 00042: val_loss did not improve from 20.84364
121/121 [==============================] - 0s 638us/sample - loss: 20.6485 - val_loss: 20.8518
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6766
Epoch 00043: val_loss did not improve from 20.84364

Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 586us/sample - loss: 20.6478 - val_loss: 20.8518
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9800
Epoch 00044: val_loss did not improve from 20.84364
121/121 [==============================] - 0s 566us/sample - loss: 20.6455 - val_loss: 20.8488
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.6782
Epoch 00045: val_loss improved from 20.84364 to 20.84352, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 814us/sample - loss: 20.6435 - val_loss: 20.8435
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7282
Epoch 00046: val_loss improved from 20.84352 to 20.83785, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 788us/sample - loss: 20.6409 - val_loss: 20.8379
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1023
Epoch 00047: val_loss improved from 20.83785 to 20.83508, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 820us/sample - loss: 20.6390 - val_loss: 20.8351
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6943
Epoch 00048: val_loss improved from 20.83508 to 20.83398, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 727us/sample - loss: 20.6375 - val_loss: 20.8340
Epoch 49/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1538
Epoch 00049: val_loss improved from 20.83398 to 20.83205, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 755us/sample - loss: 20.6363 - val_loss: 20.8320
Epoch 50/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8336
Epoch 00050: val_loss improved from 20.83205 to 20.82825, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 845us/sample - loss: 20.6350 - val_loss: 20.8283
Epoch 51/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5485
Epoch 00051: val_loss improved from 20.82825 to 20.82469, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 836us/sample - loss: 20.6331 - val_loss: 20.8247
Epoch 52/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0814
Epoch 00052: val_loss improved from 20.82469 to 20.82012, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 815us/sample - loss: 20.6311 - val_loss: 20.8201
Epoch 53/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.5711
Epoch 00053: val_loss improved from 20.82012 to 20.81635, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 767us/sample - loss: 20.6303 - val_loss: 20.8164
Epoch 54/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4708
Epoch 00054: val_loss improved from 20.81635 to 20.81414, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 750us/sample - loss: 20.6287 - val_loss: 20.8141
Epoch 55/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7834
Epoch 00055: val_loss improved from 20.81414 to 20.81257, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 812us/sample - loss: 20.6274 - val_loss: 20.8126
Epoch 56/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1050
Epoch 00056: val_loss improved from 20.81257 to 20.81067, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 797us/sample - loss: 20.6264 - val_loss: 20.8107
Epoch 57/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8746
Epoch 00057: val_loss improved from 20.81067 to 20.80829, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 825us/sample - loss: 20.6254 - val_loss: 20.8083
Epoch 58/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8283
Epoch 00058: val_loss improved from 20.80829 to 20.80719, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 885us/sample - loss: 20.6244 - val_loss: 20.8072
Epoch 59/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7873
Epoch 00059: val_loss improved from 20.80719 to 20.80491, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 781us/sample - loss: 20.6230 - val_loss: 20.8049
Epoch 60/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9036
Epoch 00060: val_loss improved from 20.80491 to 20.80253, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 819us/sample - loss: 20.6218 - val_loss: 20.8025
Epoch 61/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.6920
Epoch 00061: val_loss improved from 20.80253 to 20.80083, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 923us/sample - loss: 20.6206 - val_loss: 20.8008
Epoch 62/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6470
Epoch 00062: val_loss improved from 20.80083 to 20.79983, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 783us/sample - loss: 20.6195 - val_loss: 20.7998
Epoch 63/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6469
Epoch 00063: val_loss improved from 20.79983 to 20.79906, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 756us/sample - loss: 20.6184 - val_loss: 20.7991
Epoch 64/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2690
Epoch 00064: val_loss did not improve from 20.79906
121/121 [==============================] - 0s 616us/sample - loss: 20.6172 - val_loss: 20.7993
Epoch 65/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0757
Epoch 00065: val_loss improved from 20.79906 to 20.79847, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 769us/sample - loss: 20.6158 - val_loss: 20.7985
Epoch 66/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7056
Epoch 00066: val_loss improved from 20.79847 to 20.79775, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 776us/sample - loss: 20.6147 - val_loss: 20.7978
Epoch 67/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5606
Epoch 00067: val_loss improved from 20.79775 to 20.79600, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 823us/sample - loss: 20.6136 - val_loss: 20.7960
Epoch 68/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6555
Epoch 00068: val_loss improved from 20.79600 to 20.79493, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 780us/sample - loss: 20.6124 - val_loss: 20.7949
Epoch 69/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9762
Epoch 00069: val_loss improved from 20.79493 to 20.79437, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 807us/sample - loss: 20.6113 - val_loss: 20.7944
Epoch 70/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9670
Epoch 00070: val_loss improved from 20.79437 to 20.79311, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 825us/sample - loss: 20.6102 - val_loss: 20.7931
Epoch 71/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4612
Epoch 00071: val_loss improved from 20.79311 to 20.79140, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 774us/sample - loss: 20.6089 - val_loss: 20.7914
Epoch 72/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.2385
Epoch 00072: val_loss improved from 20.79140 to 20.78874, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.6075 - val_loss: 20.7887
Epoch 73/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9013
Epoch 00073: val_loss improved from 20.78874 to 20.78687, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 816us/sample - loss: 20.6066 - val_loss: 20.7869
Epoch 74/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9132
Epoch 00074: val_loss improved from 20.78687 to 20.78684, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 873us/sample - loss: 20.6057 - val_loss: 20.7868
Epoch 75/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2083
Epoch 00075: val_loss improved from 20.78684 to 20.78544, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 904us/sample - loss: 20.6043 - val_loss: 20.7854
Epoch 76/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9165
Epoch 00076: val_loss improved from 20.78544 to 20.78404, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 785us/sample - loss: 20.6032 - val_loss: 20.7840
Epoch 77/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1072
Epoch 00077: val_loss improved from 20.78404 to 20.78272, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 865us/sample - loss: 20.6019 - val_loss: 20.7827
Epoch 78/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7949
Epoch 00078: val_loss improved from 20.78272 to 20.78178, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.6007 - val_loss: 20.7818
Epoch 79/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4906
Epoch 00079: val_loss improved from 20.78178 to 20.77952, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 769us/sample - loss: 20.5996 - val_loss: 20.7795
Epoch 80/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5940
Epoch 00080: val_loss improved from 20.77952 to 20.77808, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 806us/sample - loss: 20.5983 - val_loss: 20.7781
Epoch 81/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0570
Epoch 00081: val_loss improved from 20.77808 to 20.77746, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 874us/sample - loss: 20.5970 - val_loss: 20.7775
Epoch 82/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7479
Epoch 00082: val_loss did not improve from 20.77746
121/121 [==============================] - 0s 610us/sample - loss: 20.5957 - val_loss: 20.7776
Epoch 83/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.7274
Epoch 00083: val_loss improved from 20.77746 to 20.77615, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 908us/sample - loss: 20.5946 - val_loss: 20.7761
Epoch 84/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2494
Epoch 00084: val_loss improved from 20.77615 to 20.77429, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 937us/sample - loss: 20.5932 - val_loss: 20.7743
Epoch 85/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.1793
Epoch 00085: val_loss improved from 20.77429 to 20.77269, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 795us/sample - loss: 20.5921 - val_loss: 20.7727
Epoch 86/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8427
Epoch 00086: val_loss did not improve from 20.77269
121/121 [==============================] - 0s 543us/sample - loss: 20.5906 - val_loss: 20.7727
Epoch 87/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1410
Epoch 00087: val_loss improved from 20.77269 to 20.77144, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 786us/sample - loss: 20.5893 - val_loss: 20.7714
Epoch 88/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5059
Epoch 00088: val_loss improved from 20.77144 to 20.76968, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 839us/sample - loss: 20.5881 - val_loss: 20.7697
Epoch 89/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0674
Epoch 00089: val_loss improved from 20.76968 to 20.76824, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 953us/sample - loss: 20.5871 - val_loss: 20.7682
Epoch 90/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8788
Epoch 00090: val_loss improved from 20.76824 to 20.76766, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 763us/sample - loss: 20.5858 - val_loss: 20.7677
Epoch 91/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6628
Epoch 00091: val_loss did not improve from 20.76766
121/121 [==============================] - 0s 615us/sample - loss: 20.5846 - val_loss: 20.7685
Epoch 92/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2138
Epoch 00092: val_loss did not improve from 20.76766

Epoch 00092: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 628us/sample - loss: 20.5836 - val_loss: 20.7688
Epoch 93/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6392
Epoch 00093: val_loss improved from 20.76766 to 20.76764, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 876us/sample - loss: 20.5823 - val_loss: 20.7676
Epoch 94/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1687
Epoch 00094: val_loss improved from 20.76764 to 20.76546, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 819us/sample - loss: 20.5808 - val_loss: 20.7655
Epoch 95/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5273
Epoch 00095: val_loss improved from 20.76546 to 20.76420, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 893us/sample - loss: 20.5795 - val_loss: 20.7642
Epoch 96/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6036
Epoch 00096: val_loss improved from 20.76420 to 20.76375, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 809us/sample - loss: 20.5781 - val_loss: 20.7638
Epoch 97/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4062
Epoch 00097: val_loss did not improve from 20.76375
121/121 [==============================] - 0s 637us/sample - loss: 20.5771 - val_loss: 20.7642
Epoch 98/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7837
Epoch 00098: val_loss improved from 20.76375 to 20.76353, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 859us/sample - loss: 20.5756 - val_loss: 20.7635
Epoch 99/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5784
Epoch 00099: val_loss improved from 20.76353 to 20.76344, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 768us/sample - loss: 20.5741 - val_loss: 20.7634
Epoch 100/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.5529
Epoch 00100: val_loss improved from 20.76344 to 20.76182, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 20.5730 - val_loss: 20.7618
Epoch 101/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9283
Epoch 00101: val_loss improved from 20.76182 to 20.76111, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 914us/sample - loss: 20.5717 - val_loss: 20.7611
Epoch 102/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6474
Epoch 00102: val_loss improved from 20.76111 to 20.75933, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 20.5704 - val_loss: 20.7593
Epoch 103/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3997
Epoch 00103: val_loss improved from 20.75933 to 20.75694, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 781us/sample - loss: 20.5690 - val_loss: 20.7569
Epoch 104/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.3785
Epoch 00104: val_loss improved from 20.75694 to 20.75654, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 819us/sample - loss: 20.5677 - val_loss: 20.7565
Epoch 105/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6064
Epoch 00105: val_loss improved from 20.75654 to 20.75536, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 956us/sample - loss: 20.5666 - val_loss: 20.7554
Epoch 106/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8904
Epoch 00106: val_loss improved from 20.75536 to 20.75333, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 828us/sample - loss: 20.5655 - val_loss: 20.7533
Epoch 107/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2397
Epoch 00107: val_loss improved from 20.75333 to 20.74972, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 778us/sample - loss: 20.5640 - val_loss: 20.7497
Epoch 108/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1198
Epoch 00108: val_loss improved from 20.74972 to 20.74560, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 761us/sample - loss: 20.5635 - val_loss: 20.7456
Epoch 109/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0042
Epoch 00109: val_loss improved from 20.74560 to 20.74170, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 861us/sample - loss: 20.5624 - val_loss: 20.7417
Epoch 110/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.8301
Epoch 00110: val_loss improved from 20.74170 to 20.73992, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 800us/sample - loss: 20.5618 - val_loss: 20.7399
Epoch 111/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5774
Epoch 00111: val_loss improved from 20.73992 to 20.73835, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 850us/sample - loss: 20.5607 - val_loss: 20.7383
Epoch 112/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1356
Epoch 00112: val_loss did not improve from 20.73835
121/121 [==============================] - 0s 572us/sample - loss: 20.5593 - val_loss: 20.7390
Epoch 113/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1858
Epoch 00113: val_loss did not improve from 20.73835
121/121 [==============================] - 0s 551us/sample - loss: 20.5573 - val_loss: 20.7400
Epoch 114/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7352
Epoch 00114: val_loss did not improve from 20.73835
121/121 [==============================] - 0s 549us/sample - loss: 20.5557 - val_loss: 20.7406
Epoch 115/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.4844
Epoch 00115: val_loss did not improve from 20.73835
121/121 [==============================] - 0s 671us/sample - loss: 20.5541 - val_loss: 20.7402
Epoch 116/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4415
Epoch 00116: val_loss did not improve from 20.73835
121/121 [==============================] - 0s 624us/sample - loss: 20.5529 - val_loss: 20.7408
Epoch 117/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8154
Epoch 00117: val_loss did not improve from 20.73835
121/121 [==============================] - 0s 633us/sample - loss: 20.5519 - val_loss: 20.7388
Epoch 118/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4954
Epoch 00118: val_loss improved from 20.73835 to 20.73498, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 719us/sample - loss: 20.5506 - val_loss: 20.7350
Epoch 119/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7966
Epoch 00119: val_loss improved from 20.73498 to 20.73207, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 880us/sample - loss: 20.5488 - val_loss: 20.7321
Epoch 120/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.2901
Epoch 00120: val_loss improved from 20.73207 to 20.73075, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 827us/sample - loss: 20.5480 - val_loss: 20.7308
Epoch 121/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2309
Epoch 00121: val_loss improved from 20.73075 to 20.73042, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 804us/sample - loss: 20.5468 - val_loss: 20.7304
Epoch 122/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.6945
Epoch 00122: val_loss improved from 20.73042 to 20.72904, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 912us/sample - loss: 20.5456 - val_loss: 20.7290
Epoch 123/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6632
Epoch 00123: val_loss improved from 20.72904 to 20.72825, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 822us/sample - loss: 20.5443 - val_loss: 20.7283
Epoch 124/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6557
Epoch 00124: val_loss improved from 20.72825 to 20.72660, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 747us/sample - loss: 20.5431 - val_loss: 20.7266
Epoch 125/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7187
Epoch 00125: val_loss improved from 20.72660 to 20.72581, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 808us/sample - loss: 20.5424 - val_loss: 20.7258
Epoch 126/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.3263
Epoch 00126: val_loss improved from 20.72581 to 20.72497, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 748us/sample - loss: 20.5409 - val_loss: 20.7250
Epoch 127/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.3799
Epoch 00127: val_loss improved from 20.72497 to 20.72307, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 20.5392 - val_loss: 20.7231
Epoch 128/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4800
Epoch 00128: val_loss improved from 20.72307 to 20.72080, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 788us/sample - loss: 20.5383 - val_loss: 20.7208
Epoch 129/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1112
Epoch 00129: val_loss did not improve from 20.72080
121/121 [==============================] - 0s 555us/sample - loss: 20.5369 - val_loss: 20.7220
Epoch 130/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2330
Epoch 00130: val_loss did not improve from 20.72080
121/121 [==============================] - 0s 524us/sample - loss: 20.5355 - val_loss: 20.7229
Epoch 131/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4055
Epoch 00131: val_loss improved from 20.72080 to 20.72062, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 703us/sample - loss: 20.5339 - val_loss: 20.7206
Epoch 132/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4324
Epoch 00132: val_loss improved from 20.72062 to 20.71925, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 671us/sample - loss: 20.5326 - val_loss: 20.7193
Epoch 133/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.9237
Epoch 00133: val_loss did not improve from 20.71925
121/121 [==============================] - 0s 512us/sample - loss: 20.5313 - val_loss: 20.7204
Epoch 134/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0591
Epoch 00134: val_loss did not improve from 20.71925
121/121 [==============================] - 0s 500us/sample - loss: 20.5298 - val_loss: 20.7201
Epoch 135/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2612
Epoch 00135: val_loss did not improve from 20.71925
121/121 [==============================] - 0s 486us/sample - loss: 20.5285 - val_loss: 20.7203
Epoch 136/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9835
Epoch 00136: val_loss did not improve from 20.71925
121/121 [==============================] - 0s 488us/sample - loss: 20.5276 - val_loss: 20.7196
Epoch 137/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8440
Epoch 00137: val_loss did not improve from 20.71925
121/121 [==============================] - 0s 504us/sample - loss: 20.5264 - val_loss: 20.7199
Epoch 138/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.5570
Epoch 00138: val_loss improved from 20.71925 to 20.71891, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 668us/sample - loss: 20.5253 - val_loss: 20.7189
Epoch 139/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6557
Epoch 00139: val_loss improved from 20.71891 to 20.71584, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.5241 - val_loss: 20.7158
Epoch 140/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0517
Epoch 00140: val_loss improved from 20.71584 to 20.71404, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 979us/sample - loss: 20.5227 - val_loss: 20.7140
Epoch 141/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.8918
Epoch 00141: val_loss improved from 20.71404 to 20.71241, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 924us/sample - loss: 20.5218 - val_loss: 20.7124
Epoch 142/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0094
Epoch 00142: val_loss improved from 20.71241 to 20.70935, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 816us/sample - loss: 20.5208 - val_loss: 20.7094
Epoch 143/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0892
Epoch 00143: val_loss did not improve from 20.70935
121/121 [==============================] - 0s 601us/sample - loss: 20.5196 - val_loss: 20.7100
Epoch 144/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9193
Epoch 00144: val_loss improved from 20.70935 to 20.70870, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 766us/sample - loss: 20.5182 - val_loss: 20.7087
Epoch 145/200
 20/121 [===>..........................] - ETA: 0s - loss: 23.1619
Epoch 00145: val_loss improved from 20.70870 to 20.70690, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 864us/sample - loss: 20.5170 - val_loss: 20.7069
Epoch 146/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.4434
Epoch 00146: val_loss improved from 20.70690 to 20.70583, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 897us/sample - loss: 20.5147 - val_loss: 20.7058
Epoch 147/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9769
Epoch 00147: val_loss improved from 20.70583 to 20.70463, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 871us/sample - loss: 20.5134 - val_loss: 20.7046
Epoch 148/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1982
Epoch 00148: val_loss improved from 20.70463 to 20.70449, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 778us/sample - loss: 20.5120 - val_loss: 20.7045
Epoch 149/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.5775
Epoch 00149: val_loss improved from 20.70449 to 20.70125, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 857us/sample - loss: 20.5107 - val_loss: 20.7012
Epoch 150/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9725
Epoch 00150: val_loss improved from 20.70125 to 20.70007, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 832us/sample - loss: 20.5092 - val_loss: 20.7001
Epoch 151/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6899
Epoch 00151: val_loss improved from 20.70007 to 20.69854, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 927us/sample - loss: 20.5078 - val_loss: 20.6985
Epoch 152/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9239
Epoch 00152: val_loss did not improve from 20.69854
121/121 [==============================] - 0s 589us/sample - loss: 20.5067 - val_loss: 20.6997
Epoch 153/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0214
Epoch 00153: val_loss did not improve from 20.69854
121/121 [==============================] - 0s 584us/sample - loss: 20.5051 - val_loss: 20.7010
Epoch 154/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0342
Epoch 00154: val_loss did not improve from 20.69854
121/121 [==============================] - 0s 600us/sample - loss: 20.5042 - val_loss: 20.7017
Epoch 155/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.9491
Epoch 00155: val_loss did not improve from 20.69854
121/121 [==============================] - 0s 550us/sample - loss: 20.5029 - val_loss: 20.7006
Epoch 156/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0843
Epoch 00156: val_loss did not improve from 20.69854
121/121 [==============================] - 0s 645us/sample - loss: 20.5015 - val_loss: 20.6995
Epoch 157/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3660
Epoch 00157: val_loss improved from 20.69854 to 20.69783, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 848us/sample - loss: 20.5002 - val_loss: 20.6978
Epoch 158/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8280
Epoch 00158: val_loss improved from 20.69783 to 20.69461, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 819us/sample - loss: 20.4991 - val_loss: 20.6946
Epoch 159/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0308
Epoch 00159: val_loss improved from 20.69461 to 20.69154, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 831us/sample - loss: 20.4976 - val_loss: 20.6915
Epoch 160/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.2551
Epoch 00160: val_loss improved from 20.69154 to 20.69070, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 778us/sample - loss: 20.4963 - val_loss: 20.6907
Epoch 161/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3540
Epoch 00161: val_loss improved from 20.69070 to 20.68792, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 857us/sample - loss: 20.4954 - val_loss: 20.6879
Epoch 162/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0111
Epoch 00162: val_loss improved from 20.68792 to 20.68550, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 801us/sample - loss: 20.4936 - val_loss: 20.6855
Epoch 163/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2913
Epoch 00163: val_loss improved from 20.68550 to 20.68520, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 872us/sample - loss: 20.4925 - val_loss: 20.6852
Epoch 164/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8477
Epoch 00164: val_loss improved from 20.68520 to 20.68410, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 876us/sample - loss: 20.4910 - val_loss: 20.6841
Epoch 165/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3023
Epoch 00165: val_loss improved from 20.68410 to 20.68290, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 765us/sample - loss: 20.4898 - val_loss: 20.6829
Epoch 166/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3361
Epoch 00166: val_loss improved from 20.68290 to 20.68148, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 858us/sample - loss: 20.4884 - val_loss: 20.6815
Epoch 167/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1704
Epoch 00167: val_loss improved from 20.68148 to 20.68065, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 831us/sample - loss: 20.4871 - val_loss: 20.6806
Epoch 168/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3105
Epoch 00168: val_loss improved from 20.68065 to 20.67817, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 814us/sample - loss: 20.4860 - val_loss: 20.6782
Epoch 169/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7058
Epoch 00169: val_loss improved from 20.67817 to 20.67792, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 803us/sample - loss: 20.4848 - val_loss: 20.6779
Epoch 170/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9254
Epoch 00170: val_loss improved from 20.67792 to 20.67565, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 815us/sample - loss: 20.4833 - val_loss: 20.6757
Epoch 171/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0390
Epoch 00171: val_loss improved from 20.67565 to 20.67521, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 766us/sample - loss: 20.4819 - val_loss: 20.6752
Epoch 172/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0466
Epoch 00172: val_loss did not improve from 20.67521
121/121 [==============================] - 0s 598us/sample - loss: 20.4809 - val_loss: 20.6758
Epoch 173/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.1754
Epoch 00173: val_loss did not improve from 20.67521
121/121 [==============================] - 0s 628us/sample - loss: 20.4794 - val_loss: 20.6756
Epoch 174/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8254
Epoch 00174: val_loss improved from 20.67521 to 20.67481, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 835us/sample - loss: 20.4781 - val_loss: 20.6748
Epoch 175/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.9939
Epoch 00175: val_loss improved from 20.67481 to 20.67207, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 858us/sample - loss: 20.4768 - val_loss: 20.6721
Epoch 176/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8139
Epoch 00176: val_loss improved from 20.67207 to 20.67049, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 800us/sample - loss: 20.4758 - val_loss: 20.6705
Epoch 177/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9699
Epoch 00177: val_loss improved from 20.67049 to 20.66907, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 774us/sample - loss: 20.4744 - val_loss: 20.6691
Epoch 178/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6063
Epoch 00178: val_loss did not improve from 20.66907
121/121 [==============================] - 0s 615us/sample - loss: 20.4731 - val_loss: 20.6704
Epoch 179/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.7054
Epoch 00179: val_loss did not improve from 20.66907
121/121 [==============================] - 0s 572us/sample - loss: 20.4716 - val_loss: 20.6714
Epoch 180/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7596
Epoch 00180: val_loss did not improve from 20.66907
121/121 [==============================] - 0s 568us/sample - loss: 20.4710 - val_loss: 20.6729
Epoch 181/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7150
Epoch 00181: val_loss did not improve from 20.66907
121/121 [==============================] - 0s 535us/sample - loss: 20.4703 - val_loss: 20.6732
Epoch 182/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.2458
Epoch 00182: val_loss did not improve from 20.66907
121/121 [==============================] - 0s 550us/sample - loss: 20.4687 - val_loss: 20.6708
Epoch 183/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4424
Epoch 00183: val_loss improved from 20.66907 to 20.66832, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 754us/sample - loss: 20.4667 - val_loss: 20.6683
Epoch 184/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4808
Epoch 00184: val_loss improved from 20.66832 to 20.66536, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 794us/sample - loss: 20.4654 - val_loss: 20.6654
Epoch 185/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.7431
Epoch 00185: val_loss improved from 20.66536 to 20.66419, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 734us/sample - loss: 20.4641 - val_loss: 20.6642
Epoch 186/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1946
Epoch 00186: val_loss improved from 20.66419 to 20.66155, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 787us/sample - loss: 20.4629 - val_loss: 20.6616
Epoch 187/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.8728
Epoch 00187: val_loss improved from 20.66155 to 20.66006, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 792us/sample - loss: 20.4615 - val_loss: 20.6601
Epoch 188/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8348
Epoch 00188: val_loss improved from 20.66006 to 20.65742, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 806us/sample - loss: 20.4602 - val_loss: 20.6574
Epoch 189/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9611
Epoch 00189: val_loss improved from 20.65742 to 20.65534, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 812us/sample - loss: 20.4589 - val_loss: 20.6553
Epoch 190/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5639
Epoch 00190: val_loss improved from 20.65534 to 20.65334, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 787us/sample - loss: 20.4578 - val_loss: 20.6533
Epoch 191/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.3395
Epoch 00191: val_loss improved from 20.65334 to 20.65092, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 823us/sample - loss: 20.4574 - val_loss: 20.6509
Epoch 192/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6224
Epoch 00192: val_loss improved from 20.65092 to 20.64903, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 766us/sample - loss: 20.4563 - val_loss: 20.6490
Epoch 193/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.1333
Epoch 00193: val_loss improved from 20.64903 to 20.64649, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.4550 - val_loss: 20.6465
Epoch 194/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.7872
Epoch 00194: val_loss improved from 20.64649 to 20.64502, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 731us/sample - loss: 20.4534 - val_loss: 20.6450
Epoch 195/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.0648
Epoch 00195: val_loss improved from 20.64502 to 20.64396, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 747us/sample - loss: 20.4515 - val_loss: 20.6440
Epoch 196/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9561
Epoch 00196: val_loss improved from 20.64396 to 20.64234, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 20.4502 - val_loss: 20.6423
Epoch 197/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.9819
Epoch 00197: val_loss improved from 20.64234 to 20.64229, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 746us/sample - loss: 20.4490 - val_loss: 20.6423
Epoch 198/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.2378
Epoch 00198: val_loss improved from 20.64229 to 20.64032, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 803us/sample - loss: 20.4475 - val_loss: 20.6403
Epoch 199/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4612
Epoch 00199: val_loss improved from 20.64032 to 20.63690, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 766us/sample - loss: 20.4462 - val_loss: 20.6369
Epoch 200/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4070
Epoch 00200: val_loss improved from 20.63690 to 20.63382, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2017/cpf_CRPS_FINETUNE_val_2016_test_2017.ckpt
121/121 [==============================] - 0s 777us/sample - loss: 20.4457 - val_loss: 20.6338
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_val_2017_test_2018.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ae0dfcd0cc0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ae0dfcda828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfcdacc0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0dfcdaba8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfcf60b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfcd9940> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0dfd0bdd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfd0bd30> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ae0dfd12320> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd40278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd40ac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd4a828> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ae0dfd5b4a8> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ae0dfd5b400> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ae0dfd68e10> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd68cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfd83908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd83b70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfd83cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd8bac8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ae0dfd8bc50> False
Model: "model_6"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_4 (ZeroPadding2D (None, 72, 60, 6)    0           input_5[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_4[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_4 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_4[0][0]                      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_4 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_4[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 71, 57, 2)    578         conv2d_2[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ae0dfcd0cc0> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ae0dfcda828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfcdacc0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0dfcdaba8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfcf60b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfcd9940> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae0dfd0bdd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfd0bd30> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ae0dfd12320> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd40278> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd40ac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd4a828> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ae0dfd5b4a8> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ae0dfd5b400> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ae0dfd68e10> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd68cf8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfd83908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd83b70> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae0dfd83cf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfd8bac8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ae0dfd8bc50> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0ded87550> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0dfdc8438> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
Train on 121 samples, validate on 121 samples
Epoch 1/200
 20/121 [===>..........................] - ETA: 3s - loss: 152.9299
Epoch 00001: val_loss improved from inf to 78.18808, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 1s 8ms/sample - loss: 120.9448 - val_loss: 78.1881
Epoch 2/200
 20/121 [===>..........................] - ETA: 0s - loss: 77.6926
Epoch 00002: val_loss improved from 78.18808 to 26.13039, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 758us/sample - loss: 57.2673 - val_loss: 26.1304
Epoch 3/200
 20/121 [===>..........................] - ETA: 0s - loss: 25.0342
Epoch 00003: val_loss did not improve from 26.13039
121/121 [==============================] - 0s 610us/sample - loss: 25.4619 - val_loss: 28.2086
Epoch 4/200
 20/121 [===>..........................] - ETA: 0s - loss: 28.1826
Epoch 00004: val_loss improved from 26.13039 to 24.72781, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 740us/sample - loss: 29.7171 - val_loss: 24.7278
Epoch 5/200
 20/121 [===>..........................] - ETA: 0s - loss: 27.7042
Epoch 00005: val_loss improved from 24.72781 to 21.33662, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 741us/sample - loss: 23.9421 - val_loss: 21.3366
Epoch 6/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.6774
Epoch 00006: val_loss did not improve from 21.33662
121/121 [==============================] - 0s 627us/sample - loss: 21.8550 - val_loss: 21.5920
Epoch 7/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.0286
Epoch 00007: val_loss improved from 21.33662 to 20.68357, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 797us/sample - loss: 21.1058 - val_loss: 20.6836
Epoch 8/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.0614
Epoch 00008: val_loss improved from 20.68357 to 20.49373, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 771us/sample - loss: 21.0528 - val_loss: 20.4937
Epoch 9/200
 20/121 [===>..........................] - ETA: 0s - loss: 21.7560
Epoch 00009: val_loss improved from 20.49373 to 20.46170, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 716us/sample - loss: 20.6845 - val_loss: 20.4617
Epoch 10/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4733
Epoch 00010: val_loss improved from 20.46170 to 20.27144, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 785us/sample - loss: 20.5788 - val_loss: 20.2714
Epoch 11/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.3768
Epoch 00011: val_loss improved from 20.27144 to 20.17619, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 701us/sample - loss: 20.4582 - val_loss: 20.1762
Epoch 12/200
 20/121 [===>..........................] - ETA: 0s - loss: 22.5255
Epoch 00012: val_loss improved from 20.17619 to 20.08649, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 763us/sample - loss: 20.3521 - val_loss: 20.0865
Epoch 13/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.3492
Epoch 00013: val_loss improved from 20.08649 to 19.98616, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 784us/sample - loss: 20.2523 - val_loss: 19.9862
Epoch 14/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.5664
Epoch 00014: val_loss improved from 19.98616 to 19.81523, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 804us/sample - loss: 20.1733 - val_loss: 19.8152
Epoch 15/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9863
Epoch 00015: val_loss improved from 19.81523 to 19.77108, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 770us/sample - loss: 20.0530 - val_loss: 19.7711
Epoch 16/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.1440
Epoch 00016: val_loss improved from 19.77108 to 19.62816, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 795us/sample - loss: 19.9066 - val_loss: 19.6282
Epoch 17/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.5187
Epoch 00017: val_loss improved from 19.62816 to 19.55952, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 798us/sample - loss: 19.8047 - val_loss: 19.5595
Epoch 18/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6761
Epoch 00018: val_loss did not improve from 19.55952
121/121 [==============================] - 0s 585us/sample - loss: 19.6887 - val_loss: 19.5829
Epoch 19/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6331
Epoch 00019: val_loss improved from 19.55952 to 19.37301, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 831us/sample - loss: 19.6486 - val_loss: 19.3730
Epoch 20/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.1078
Epoch 00020: val_loss did not improve from 19.37301
121/121 [==============================] - 0s 588us/sample - loss: 19.5554 - val_loss: 19.3902
Epoch 21/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2574
Epoch 00021: val_loss improved from 19.37301 to 19.23171, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 823us/sample - loss: 19.4781 - val_loss: 19.2317
Epoch 22/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0695
Epoch 00022: val_loss improved from 19.23171 to 19.18473, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 888us/sample - loss: 19.3892 - val_loss: 19.1847
Epoch 23/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2077
Epoch 00023: val_loss improved from 19.18473 to 19.17965, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 894us/sample - loss: 19.3247 - val_loss: 19.1796
Epoch 24/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.9262
Epoch 00024: val_loss improved from 19.17965 to 19.08957, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 1ms/sample - loss: 19.2496 - val_loss: 19.0896
Epoch 25/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.7632
Epoch 00025: val_loss improved from 19.08957 to 18.97832, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 840us/sample - loss: 19.2243 - val_loss: 18.9783
Epoch 26/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.7113
Epoch 00026: val_loss did not improve from 18.97832
121/121 [==============================] - 0s 755us/sample - loss: 19.1947 - val_loss: 19.0028
Epoch 27/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9862
Epoch 00027: val_loss did not improve from 18.97832

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
121/121 [==============================] - 0s 661us/sample - loss: 19.1176 - val_loss: 18.9851
Epoch 28/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9188
Epoch 00028: val_loss improved from 18.97832 to 18.91816, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 865us/sample - loss: 19.0977 - val_loss: 18.9182
Epoch 29/200
 20/121 [===>..........................] - ETA: 0s - loss: 17.5093
Epoch 00029: val_loss improved from 18.91816 to 18.90408, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 874us/sample - loss: 19.0536 - val_loss: 18.9041
Epoch 30/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.3602
Epoch 00030: val_loss improved from 18.90408 to 18.86454, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 789us/sample - loss: 19.0601 - val_loss: 18.8645
Epoch 31/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4350
Epoch 00031: val_loss did not improve from 18.86454
121/121 [==============================] - 0s 643us/sample - loss: 19.0116 - val_loss: 18.9037
Epoch 32/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.3404
Epoch 00032: val_loss did not improve from 18.86454

Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
121/121 [==============================] - 0s 701us/sample - loss: 19.0405 - val_loss: 18.8864
Epoch 33/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.1011
Epoch 00033: val_loss improved from 18.86454 to 18.83768, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 797us/sample - loss: 18.9940 - val_loss: 18.8377
Epoch 34/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.2122
Epoch 00034: val_loss did not improve from 18.83768
121/121 [==============================] - 0s 649us/sample - loss: 18.9803 - val_loss: 18.8413
Epoch 35/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.0359
Epoch 00035: val_loss did not improve from 18.83768

Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
121/121 [==============================] - 0s 632us/sample - loss: 18.9733 - val_loss: 18.8484
Epoch 36/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.4050
Epoch 00036: val_loss did not improve from 18.83768
121/121 [==============================] - 0s 600us/sample - loss: 18.9643 - val_loss: 18.8438
Epoch 37/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.4944
Epoch 00037: val_loss did not improve from 18.83768

Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
121/121 [==============================] - 0s 588us/sample - loss: 18.9621 - val_loss: 18.8395
Epoch 38/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4459
Epoch 00038: val_loss improved from 18.83768 to 18.83678, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2018/cpf_CRPS_FINETUNE_val_2017_test_2018.ckpt
121/121 [==============================] - 0s 784us/sample - loss: 18.9614 - val_loss: 18.8368
Epoch 39/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.1479
Epoch 00039: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 662us/sample - loss: 18.9603 - val_loss: 18.8384
Epoch 40/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.8337
Epoch 00040: val_loss did not improve from 18.83678

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
121/121 [==============================] - 0s 689us/sample - loss: 18.9588 - val_loss: 18.8406
Epoch 41/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.0929
Epoch 00041: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 651us/sample - loss: 18.9577 - val_loss: 18.8419
Epoch 42/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.9302
Epoch 00042: val_loss did not improve from 18.83678

Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-05.
121/121 [==============================] - 0s 631us/sample - loss: 18.9570 - val_loss: 18.8413
Epoch 43/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.3480
Epoch 00043: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 621us/sample - loss: 18.9563 - val_loss: 18.8412
Epoch 44/200
 20/121 [===>..........................] - ETA: 0s - loss: 19.6499
Epoch 00044: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 636us/sample - loss: 18.9557 - val_loss: 18.8402
Epoch 45/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.8578
Epoch 00045: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 693us/sample - loss: 18.9552 - val_loss: 18.8390
Epoch 46/200
 20/121 [===>..........................] - ETA: 0s - loss: 20.6026
Epoch 00046: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 598us/sample - loss: 18.9546 - val_loss: 18.8368
Epoch 47/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.7178
Epoch 00047: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 665us/sample - loss: 18.9542 - val_loss: 18.8379
Epoch 48/200
 20/121 [===>..........................] - ETA: 0s - loss: 18.4910
Epoch 00048: val_loss did not improve from 18.83678
121/121 [==============================] - 0s 595us/sample - loss: 18.9537 - val_loss: 18.8369
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
Epoch 00048: early stopping
#################################################
testing: ['/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc']
validatiing: ['/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc']
#################################################
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
...gathering data...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/validate/F024_WY_2017_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/train/F024_WY_2016_500mb_Clean.nc"...
Reading data from: "/glade/scratch/wchapman/Reforecast/F024/test/F024_WY_2018_500mb_Clean.nc"...
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
==================================================================================================
Total params: 35,506
Trainable params: 35,442
Non-trainable params: 64
__________________________________________________________________________________________________
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
/glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### Loading old model weights name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_val_2018_test_2016.ckpt
########### layers frozen ###########
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ae126465908> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ae12646dac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae12646deb8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae12646ddd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae12646d9e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264768d0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae12648ceb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae12648cb00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ae126493400> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264c20b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264c2ba8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264ca908> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ae1264de588> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ae1264de4e0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ae1264eaef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264eadd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae1265039e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae126503c50> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae126503dd8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae12650eba8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ae12650ed30> False
Model: "model_8"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 71, 57, 6)]  0                                            
__________________________________________________________________________________________________
zero_padding2d_5 (ZeroPadding2D (None, 72, 60, 6)    0           input_6[0][0]                    
__________________________________________________________________________________________________
downsampling_0_conv_0 (Conv2D)  (None, 72, 60, 16)   880         zero_padding2d_5[0][0]           
__________________________________________________________________________________________________
downsampling_0_batchnorm_0 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_0[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_0 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_0[0][0] 
__________________________________________________________________________________________________
downsampling_0_conv_1 (Conv2D)  (None, 72, 60, 16)   2320        downsampling_0_activation_0[0][0]
__________________________________________________________________________________________________
downsampling_0_batchnorm_1 (Bat (None, 72, 60, 16)   64          downsampling_0_conv_1[0][0]      
__________________________________________________________________________________________________
downsampling_0_activation_1 (Ac (None, 72, 60, 16)   0           downsampling_0_batchnorm_1[0][0] 
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 36, 30, 16)   0           downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
bottleneck_skip_0 (Conv2D)      (None, 36, 30, 32)   4640        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
bottleneck_skip_1 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_0[0][0]          
__________________________________________________________________________________________________
bottleneck_skip_2 (Conv2D)      (None, 36, 30, 32)   9248        bottleneck_skip_1[0][0]          
__________________________________________________________________________________________________
add_5 (Add)                     (None, 36, 30, 32)   0           bottleneck_skip_0[0][0]          
                                                                 bottleneck_skip_1[0][0]          
                                                                 bottleneck_skip_2[0][0]          
__________________________________________________________________________________________________
upsampling_0_conv_trans_0 (Conv (None, 72, 60, 16)   2064        add_5[0][0]                      
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 60, 32)   0           upsampling_0_conv_trans_0[0][0]  
                                                                 downsampling_0_activation_1[0][0]
__________________________________________________________________________________________________
upsampling_0_conv_0 (Conv2D)    (None, 72, 60, 16)   4624        concatenate_5[0][0]              
__________________________________________________________________________________________________
upsampling_0_activation_0 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_0[0][0]        
__________________________________________________________________________________________________
upsampling_0_conv_1 (Conv2D)    (None, 72, 60, 16)   2320        upsampling_0_activation_0[0][0]  
__________________________________________________________________________________________________
upsampling_0_activation_1 (Acti (None, 72, 60, 16)   0           upsampling_0_conv_1[0][0]        
__________________________________________________________________________________________________
linear (Conv2D)                 (None, 72, 60, 2)    34          upsampling_0_activation_1[0][0]  
__________________________________________________________________________________________________
cropping2d_5 (Cropping2D)       (None, 71, 57, 2)    0           linear[0][0]                     
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 71, 57, 32)   608         cropping2d_5[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 57, 2)    578         conv2d_4[0][0]                   
==================================================================================================
Total params: 36,692
Trainable params: 1,186
Non-trainable params: 35,506
__________________________________________________________________________________________________
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x2ae126465908> False
<tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x2ae12646dac8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae12646deb8> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae12646ddd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae12646d9e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264768d0> False
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x2ae12648ceb8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae12648cb00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x2ae126493400> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264c20b8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264c2ba8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264ca908> False
<tensorflow.python.keras.layers.merge.Add object at 0x2ae1264de588> False
<tensorflow.python.keras.layers.convolutional.Conv2DTranspose object at 0x2ae1264de4e0> False
<tensorflow.python.keras.layers.merge.Concatenate object at 0x2ae1264eaef0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae1264eadd8> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae1265039e8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae126503c50> False
<tensorflow.python.keras.layers.core.Activation object at 0x2ae126503dd8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae12650eba8> False
<tensorflow.python.keras.layers.convolutional.Cropping2D object at 0x2ae12650ed30> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae0df5a08d0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x2ae12654b518> True
########### Saving new model name: /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
Train on 242 samples, validate on 121 samples
Epoch 1/200
 20/242 [=>............................] - ETA: 13s - loss: 91.4269140/242 [================>.............] - ETA: 0s - loss: 68.9899 
Epoch 00001: val_loss improved from inf to 28.81300, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 2s 7ms/sample - loss: 55.4750 - val_loss: 28.8130
Epoch 2/200
 20/242 [=>............................] - ETA: 0s - loss: 30.9466180/242 [=====================>........] - ETA: 0s - loss: 28.2633
Epoch 00002: val_loss improved from 28.81300 to 20.61927, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 580us/sample - loss: 27.1160 - val_loss: 20.6193
Epoch 3/200
 20/242 [=>............................] - ETA: 0s - loss: 19.1221200/242 [=======================>......] - ETA: 0s - loss: 21.1783
Epoch 00003: val_loss improved from 20.61927 to 20.16825, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 547us/sample - loss: 21.1482 - val_loss: 20.1683
Epoch 4/200
 20/242 [=>............................] - ETA: 0s - loss: 18.7802200/242 [=======================>......] - ETA: 0s - loss: 20.1828
Epoch 00004: val_loss improved from 20.16825 to 20.02835, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 543us/sample - loss: 20.1620 - val_loss: 20.0284
Epoch 5/200
 20/242 [=>............................] - ETA: 0s - loss: 20.4743200/242 [=======================>......] - ETA: 0s - loss: 19.6480
Epoch 00005: val_loss improved from 20.02835 to 19.71172, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 527us/sample - loss: 19.7834 - val_loss: 19.7117
Epoch 6/200
 20/242 [=>............................] - ETA: 0s - loss: 19.7488120/242 [=============>................] - ETA: 0s - loss: 19.2623
Epoch 00006: val_loss improved from 19.71172 to 19.39552, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 797us/sample - loss: 19.4014 - val_loss: 19.3955
Epoch 7/200
 20/242 [=>............................] - ETA: 0s - loss: 18.6629200/242 [=======================>......] - ETA: 0s - loss: 19.1203
Epoch 00007: val_loss improved from 19.39552 to 19.22737, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 551us/sample - loss: 19.1313 - val_loss: 19.2274
Epoch 8/200
 20/242 [=>............................] - ETA: 0s - loss: 19.3536200/242 [=======================>......] - ETA: 0s - loss: 18.8602
Epoch 00008: val_loss improved from 19.22737 to 19.07620, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 574us/sample - loss: 18.9772 - val_loss: 19.0762
Epoch 9/200
 20/242 [=>............................] - ETA: 0s - loss: 18.9771180/242 [=====================>........] - ETA: 0s - loss: 19.1873
Epoch 00009: val_loss improved from 19.07620 to 18.93516, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 580us/sample - loss: 18.8631 - val_loss: 18.9352
Epoch 10/200
 20/242 [=>............................] - ETA: 0s - loss: 16.6408200/242 [=======================>......] - ETA: 0s - loss: 18.6125
Epoch 00010: val_loss improved from 18.93516 to 18.83361, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 558us/sample - loss: 18.7337 - val_loss: 18.8336
Epoch 11/200
 20/242 [=>............................] - ETA: 0s - loss: 18.3338200/242 [=======================>......] - ETA: 0s - loss: 18.8775
Epoch 00011: val_loss improved from 18.83361 to 18.74702, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 552us/sample - loss: 18.7093 - val_loss: 18.7470
Epoch 12/200
 20/242 [=>............................] - ETA: 0s - loss: 18.5492200/242 [=======================>......] - ETA: 0s - loss: 18.6157
Epoch 00012: val_loss improved from 18.74702 to 18.71039, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 635us/sample - loss: 18.6784 - val_loss: 18.7104
Epoch 13/200
 20/242 [=>............................] - ETA: 0s - loss: 18.0862180/242 [=====================>........] - ETA: 0s - loss: 18.7706
Epoch 00013: val_loss improved from 18.71039 to 18.65418, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 613us/sample - loss: 18.5345 - val_loss: 18.6542
Epoch 14/200
 20/242 [=>............................] - ETA: 0s - loss: 17.6589180/242 [=====================>........] - ETA: 0s - loss: 18.5834
Epoch 00014: val_loss improved from 18.65418 to 18.57419, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 604us/sample - loss: 18.4826 - val_loss: 18.5742
Epoch 15/200
 20/242 [=>............................] - ETA: 0s - loss: 19.9410200/242 [=======================>......] - ETA: 0s - loss: 18.4396
Epoch 00015: val_loss improved from 18.57419 to 18.54132, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 590us/sample - loss: 18.4003 - val_loss: 18.5413
Epoch 16/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1202180/242 [=====================>........] - ETA: 0s - loss: 18.0223
Epoch 00016: val_loss improved from 18.54132 to 18.45195, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 609us/sample - loss: 18.3449 - val_loss: 18.4520
Epoch 17/200
 20/242 [=>............................] - ETA: 0s - loss: 19.3553200/242 [=======================>......] - ETA: 0s - loss: 18.1415
Epoch 00017: val_loss improved from 18.45195 to 18.39770, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 559us/sample - loss: 18.3041 - val_loss: 18.3977
Epoch 18/200
 20/242 [=>............................] - ETA: 0s - loss: 17.6916180/242 [=====================>........] - ETA: 0s - loss: 18.1493
Epoch 00018: val_loss improved from 18.39770 to 18.33808, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 647us/sample - loss: 18.2627 - val_loss: 18.3381
Epoch 19/200
 20/242 [=>............................] - ETA: 0s - loss: 19.9816200/242 [=======================>......] - ETA: 0s - loss: 18.1802
Epoch 00019: val_loss did not improve from 18.33808
242/242 [==============================] - 0s 477us/sample - loss: 18.2830 - val_loss: 18.4178
Epoch 20/200
 20/242 [=>............................] - ETA: 0s - loss: 17.9928180/242 [=====================>........] - ETA: 0s - loss: 18.1945
Epoch 00020: val_loss improved from 18.33808 to 18.28504, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 624us/sample - loss: 18.2217 - val_loss: 18.2850
Epoch 21/200
 20/242 [=>............................] - ETA: 0s - loss: 18.4646180/242 [=====================>........] - ETA: 0s - loss: 18.3808
Epoch 00021: val_loss did not improve from 18.28504
242/242 [==============================] - 0s 465us/sample - loss: 18.1590 - val_loss: 18.3201
Epoch 22/200
 20/242 [=>............................] - ETA: 0s - loss: 19.5537200/242 [=======================>......] - ETA: 0s - loss: 18.0834
Epoch 00022: val_loss improved from 18.28504 to 18.17452, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 523us/sample - loss: 18.1698 - val_loss: 18.1745
Epoch 23/200
 20/242 [=>............................] - ETA: 0s - loss: 17.1508180/242 [=====================>........] - ETA: 0s - loss: 18.3734
Epoch 00023: val_loss improved from 18.17452 to 18.16893, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 655us/sample - loss: 18.0855 - val_loss: 18.1689
Epoch 24/200
 20/242 [=>............................] - ETA: 0s - loss: 18.7397180/242 [=====================>........] - ETA: 0s - loss: 18.1046
Epoch 00024: val_loss improved from 18.16893 to 18.11510, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 619us/sample - loss: 18.0323 - val_loss: 18.1151
Epoch 25/200
 20/242 [=>............................] - ETA: 0s - loss: 18.1698200/242 [=======================>......] - ETA: 0s - loss: 18.1736
Epoch 00025: val_loss did not improve from 18.11510
242/242 [==============================] - 0s 443us/sample - loss: 18.0655 - val_loss: 18.1241
Epoch 26/200
 20/242 [=>............................] - ETA: 0s - loss: 17.8971180/242 [=====================>........] - ETA: 0s - loss: 18.4786
Epoch 00026: val_loss did not improve from 18.11510

Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
242/242 [==============================] - 0s 510us/sample - loss: 18.1636 - val_loss: 18.5628
Epoch 27/200
 20/242 [=>............................] - ETA: 0s - loss: 19.3593200/242 [=======================>......] - ETA: 0s - loss: 18.3068
Epoch 00027: val_loss improved from 18.11510 to 18.09424, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 551us/sample - loss: 18.1264 - val_loss: 18.0942
Epoch 28/200
 20/242 [=>............................] - ETA: 0s - loss: 18.4021180/242 [=====================>........] - ETA: 0s - loss: 17.9841
Epoch 00028: val_loss improved from 18.09424 to 18.06009, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 585us/sample - loss: 17.9693 - val_loss: 18.0601
Epoch 29/200
 20/242 [=>............................] - ETA: 0s - loss: 16.6085180/242 [=====================>........] - ETA: 0s - loss: 17.6664
Epoch 00029: val_loss improved from 18.06009 to 18.03530, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 555us/sample - loss: 17.9498 - val_loss: 18.0353
Epoch 30/200
 20/242 [=>............................] - ETA: 0s - loss: 17.7783200/242 [=======================>......] - ETA: 0s - loss: 17.8565
Epoch 00030: val_loss improved from 18.03530 to 18.01563, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 526us/sample - loss: 17.9234 - val_loss: 18.0156
Epoch 31/200
 20/242 [=>............................] - ETA: 0s - loss: 17.8543200/242 [=======================>......] - ETA: 0s - loss: 17.9363
Epoch 00031: val_loss improved from 18.01563 to 18.00691, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 590us/sample - loss: 17.9097 - val_loss: 18.0069
Epoch 32/200
 20/242 [=>............................] - ETA: 0s - loss: 19.5921180/242 [=====================>........] - ETA: 0s - loss: 17.9022
Epoch 00032: val_loss did not improve from 18.00691
242/242 [==============================] - 0s 483us/sample - loss: 17.8946 - val_loss: 18.0086
Epoch 33/200
 20/242 [=>............................] - ETA: 0s - loss: 18.0301200/242 [=======================>......] - ETA: 0s - loss: 17.8785
Epoch 00033: val_loss improved from 18.00691 to 18.00323, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 547us/sample - loss: 17.8946 - val_loss: 18.0032
Epoch 34/200
 20/242 [=>............................] - ETA: 0s - loss: 18.1686200/242 [=======================>......] - ETA: 0s - loss: 17.7656
Epoch 00034: val_loss improved from 18.00323 to 17.99128, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 738us/sample - loss: 17.8877 - val_loss: 17.9913
Epoch 35/200
 20/242 [=>............................] - ETA: 0s - loss: 17.1153180/242 [=====================>........] - ETA: 0s - loss: 17.6812
Epoch 00035: val_loss improved from 17.99128 to 17.96477, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 958us/sample - loss: 17.8923 - val_loss: 17.9648
Epoch 36/200
 20/242 [=>............................] - ETA: 0s - loss: 19.1318180/242 [=====================>........] - ETA: 0s - loss: 17.6871
Epoch 00036: val_loss improved from 17.96477 to 17.94743, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 539us/sample - loss: 17.8692 - val_loss: 17.9474
Epoch 37/200
 20/242 [=>............................] - ETA: 0s - loss: 17.5160200/242 [=======================>......] - ETA: 0s - loss: 17.8537
Epoch 00037: val_loss improved from 17.94743 to 17.94655, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 558us/sample - loss: 17.8541 - val_loss: 17.9465
Epoch 38/200
 20/242 [=>............................] - ETA: 0s - loss: 17.7243200/242 [=======================>......] - ETA: 0s - loss: 18.0265
Epoch 00038: val_loss did not improve from 17.94655
242/242 [==============================] - 0s 493us/sample - loss: 17.8489 - val_loss: 17.9697
Epoch 39/200
 20/242 [=>............................] - ETA: 0s - loss: 19.1123180/242 [=====================>........] - ETA: 0s - loss: 17.7177
Epoch 00039: val_loss improved from 17.94655 to 17.93787, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 638us/sample - loss: 17.8546 - val_loss: 17.9379
Epoch 40/200
 20/242 [=>............................] - ETA: 0s - loss: 17.5319200/242 [=======================>......] - ETA: 0s - loss: 17.8789
Epoch 00040: val_loss improved from 17.93787 to 17.91982, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 574us/sample - loss: 17.8644 - val_loss: 17.9198
Epoch 41/200
 20/242 [=>............................] - ETA: 0s - loss: 17.1060180/242 [=====================>........] - ETA: 0s - loss: 17.8611
Epoch 00041: val_loss improved from 17.91982 to 17.90862, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 565us/sample - loss: 17.8530 - val_loss: 17.9086
Epoch 42/200
 20/242 [=>............................] - ETA: 0s - loss: 16.5165200/242 [=======================>......] - ETA: 0s - loss: 17.8003
Epoch 00042: val_loss improved from 17.90862 to 17.90565, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 523us/sample - loss: 17.8257 - val_loss: 17.9057
Epoch 43/200
 20/242 [=>............................] - ETA: 0s - loss: 19.1008200/242 [=======================>......] - ETA: 0s - loss: 18.1176
Epoch 00043: val_loss improved from 17.90565 to 17.89732, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 528us/sample - loss: 17.8252 - val_loss: 17.8973
Epoch 44/200
 20/242 [=>............................] - ETA: 0s - loss: 17.8928180/242 [=====================>........] - ETA: 0s - loss: 18.0559
Epoch 00044: val_loss improved from 17.89732 to 17.89071, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 537us/sample - loss: 17.8098 - val_loss: 17.8907
Epoch 45/200
 20/242 [=>............................] - ETA: 0s - loss: 18.4174220/242 [==========================>...] - ETA: 0s - loss: 17.8724
Epoch 00045: val_loss improved from 17.89071 to 17.87306, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 506us/sample - loss: 17.8138 - val_loss: 17.8731
Epoch 46/200
 20/242 [=>............................] - ETA: 0s - loss: 16.5912200/242 [=======================>......] - ETA: 0s - loss: 17.7020
Epoch 00046: val_loss improved from 17.87306 to 17.86133, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 586us/sample - loss: 17.8301 - val_loss: 17.8613
Epoch 47/200
 20/242 [=>............................] - ETA: 0s - loss: 18.2129200/242 [=======================>......] - ETA: 0s - loss: 17.9605
Epoch 00047: val_loss did not improve from 17.86133
242/242 [==============================] - 0s 441us/sample - loss: 17.8526 - val_loss: 17.9779
Epoch 48/200
 20/242 [=>............................] - ETA: 0s - loss: 18.2582220/242 [==========================>...] - ETA: 0s - loss: 17.7138
Epoch 00048: val_loss did not improve from 17.86133

Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.
242/242 [==============================] - 0s 428us/sample - loss: 17.8253 - val_loss: 17.9254
Epoch 49/200
 20/242 [=>............................] - ETA: 0s - loss: 19.4675220/242 [==========================>...] - ETA: 0s - loss: 17.8360
Epoch 00049: val_loss did not improve from 17.86133
242/242 [==============================] - 0s 420us/sample - loss: 17.8141 - val_loss: 17.8672
Epoch 50/200
 20/242 [=>............................] - ETA: 0s - loss: 18.0957180/242 [=====================>........] - ETA: 0s - loss: 17.4351
Epoch 00050: val_loss improved from 17.86133 to 17.84912, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 589us/sample - loss: 17.7918 - val_loss: 17.8491
Epoch 51/200
 20/242 [=>............................] - ETA: 0s - loss: 17.0546200/242 [=======================>......] - ETA: 0s - loss: 17.7709
Epoch 00051: val_loss improved from 17.84912 to 17.83702, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 568us/sample - loss: 17.7726 - val_loss: 17.8370
Epoch 52/200
 20/242 [=>............................] - ETA: 0s - loss: 17.0927200/242 [=======================>......] - ETA: 0s - loss: 17.6972
Epoch 00052: val_loss did not improve from 17.83702
242/242 [==============================] - 0s 471us/sample - loss: 17.7656 - val_loss: 17.8506
Epoch 53/200
 20/242 [=>............................] - ETA: 0s - loss: 17.9726200/242 [=======================>......] - ETA: 0s - loss: 17.7830
Epoch 00053: val_loss improved from 17.83702 to 17.83064, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 575us/sample - loss: 17.7576 - val_loss: 17.8306
Epoch 54/200
 20/242 [=>............................] - ETA: 0s - loss: 17.5035200/242 [=======================>......] - ETA: 0s - loss: 17.8487
Epoch 00054: val_loss did not improve from 17.83064
242/242 [==============================] - 0s 475us/sample - loss: 17.8028 - val_loss: 17.9102
Epoch 55/200
 20/242 [=>............................] - ETA: 0s - loss: 16.8624180/242 [=====================>........] - ETA: 0s - loss: 17.9137
Epoch 00055: val_loss improved from 17.83064 to 17.82719, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 617us/sample - loss: 17.7775 - val_loss: 17.8272
Epoch 56/200
 20/242 [=>............................] - ETA: 0s - loss: 18.2504200/242 [=======================>......] - ETA: 0s - loss: 17.5682
Epoch 00056: val_loss did not improve from 17.82719
242/242 [==============================] - 0s 457us/sample - loss: 17.7671 - val_loss: 17.8336
Epoch 57/200
 20/242 [=>............................] - ETA: 0s - loss: 18.1039200/242 [=======================>......] - ETA: 0s - loss: 17.8840
Epoch 00057: val_loss improved from 17.82719 to 17.81938, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 529us/sample - loss: 17.7483 - val_loss: 17.8194
Epoch 58/200
 20/242 [=>............................] - ETA: 0s - loss: 16.3728200/242 [=======================>......] - ETA: 0s - loss: 17.7651
Epoch 00058: val_loss did not improve from 17.81938
242/242 [==============================] - 0s 463us/sample - loss: 17.7532 - val_loss: 17.8353
Epoch 59/200
 20/242 [=>............................] - ETA: 0s - loss: 18.0015220/242 [==========================>...] - ETA: 0s - loss: 17.7571
Epoch 00059: val_loss did not improve from 17.81938

Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.
242/242 [==============================] - 0s 422us/sample - loss: 17.7477 - val_loss: 17.8238
Epoch 60/200
 20/242 [=>............................] - ETA: 0s - loss: 16.9537200/242 [=======================>......] - ETA: 0s - loss: 17.7318
Epoch 00060: val_loss did not improve from 17.81938
242/242 [==============================] - 0s 455us/sample - loss: 17.7463 - val_loss: 17.8263
Epoch 61/200
 20/242 [=>............................] - ETA: 0s - loss: 17.3650200/242 [=======================>......] - ETA: 0s - loss: 17.8125
Epoch 00061: val_loss did not improve from 17.81938

Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.
242/242 [==============================] - 0s 477us/sample - loss: 17.7410 - val_loss: 17.8226
Epoch 62/200
 20/242 [=>............................] - ETA: 0s - loss: 17.6109200/242 [=======================>......] - ETA: 0s - loss: 17.6548
Epoch 00062: val_loss did not improve from 17.81938
242/242 [==============================] - 0s 446us/sample - loss: 17.7400 - val_loss: 17.8228
Epoch 63/200
 20/242 [=>............................] - ETA: 0s - loss: 17.4679220/242 [==========================>...] - ETA: 0s - loss: 17.7218
Epoch 00063: val_loss did not improve from 17.81938

Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.
242/242 [==============================] - 0s 420us/sample - loss: 17.7389 - val_loss: 17.8201
Epoch 64/200
 20/242 [=>............................] - ETA: 0s - loss: 17.1977200/242 [=======================>......] - ETA: 0s - loss: 17.6333
Epoch 00064: val_loss improved from 17.81938 to 17.81906, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 542us/sample - loss: 17.7381 - val_loss: 17.8191
Epoch 65/200
 20/242 [=>............................] - ETA: 0s - loss: 18.7201200/242 [=======================>......] - ETA: 0s - loss: 17.5650
Epoch 00065: val_loss did not improve from 17.81906
242/242 [==============================] - 0s 466us/sample - loss: 17.7375 - val_loss: 17.8198
Epoch 66/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1641200/242 [=======================>......] - ETA: 0s - loss: 17.7046
Epoch 00066: val_loss improved from 17.81906 to 17.81661, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 563us/sample - loss: 17.7375 - val_loss: 17.8166
Epoch 67/200
 20/242 [=>............................] - ETA: 0s - loss: 18.3128200/242 [=======================>......] - ETA: 0s - loss: 17.8394
Epoch 00067: val_loss did not improve from 17.81661
242/242 [==============================] - 0s 447us/sample - loss: 17.7371 - val_loss: 17.8169
Epoch 68/200
 20/242 [=>............................] - ETA: 0s - loss: 16.8715200/242 [=======================>......] - ETA: 0s - loss: 17.7556
Epoch 00068: val_loss did not improve from 17.81661

Epoch 00068: ReduceLROnPlateau reducing learning rate to 1e-05.
242/242 [==============================] - 0s 451us/sample - loss: 17.7373 - val_loss: 17.8187
Epoch 69/200
 20/242 [=>............................] - ETA: 0s - loss: 17.3750200/242 [=======================>......] - ETA: 0s - loss: 17.5544
Epoch 00069: val_loss did not improve from 17.81661
242/242 [==============================] - 0s 450us/sample - loss: 17.7374 - val_loss: 17.8185
Epoch 70/200
 20/242 [=>............................] - ETA: 0s - loss: 18.4970220/242 [==========================>...] - ETA: 0s - loss: 17.7556
Epoch 00070: val_loss improved from 17.81661 to 17.81353, saving model to /glade/scratch/wchapman/Reforecast/models/NN_CRPS/F024/StartingYear2011/CNN_2016/cpf_CRPS_FINETUNE_val_2018_test_2016.ckpt
242/242 [==============================] - 0s 531us/sample - loss: 17.7375 - val_loss: 17.8135
Epoch 71/200
 20/242 [=>............................] - ETA: 0s - loss: 17.6879180/242 [=====================>........] - ETA: 0s - loss: 18.0168
Epoch 00071: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 467us/sample - loss: 17.7369 - val_loss: 17.8156
Epoch 72/200
 20/242 [=>............................] - ETA: 0s - loss: 18.1568220/242 [==========================>...] - ETA: 0s - loss: 17.7794
Epoch 00072: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 447us/sample - loss: 17.7367 - val_loss: 17.8148
Epoch 73/200
 20/242 [=>............................] - ETA: 0s - loss: 19.6511180/242 [=====================>........] - ETA: 0s - loss: 17.8874
Epoch 00073: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 473us/sample - loss: 17.7364 - val_loss: 17.8145
Epoch 74/200
 20/242 [=>............................] - ETA: 0s - loss: 17.1720200/242 [=======================>......] - ETA: 0s - loss: 17.7542
Epoch 00074: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 457us/sample - loss: 17.7365 - val_loss: 17.8171
Epoch 75/200
 20/242 [=>............................] - ETA: 0s - loss: 16.5181220/242 [==========================>...] - ETA: 0s - loss: 17.8542
Epoch 00075: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 427us/sample - loss: 17.7363 - val_loss: 17.8193
Epoch 76/200
 20/242 [=>............................] - ETA: 0s - loss: 17.8970200/242 [=======================>......] - ETA: 0s - loss: 17.6725
Epoch 00076: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 443us/sample - loss: 17.7364 - val_loss: 17.8214
Epoch 77/200
 20/242 [=>............................] - ETA: 0s - loss: 18.9876200/242 [=======================>......] - ETA: 0s - loss: 17.7113
Epoch 00077: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 456us/sample - loss: 17.7360 - val_loss: 17.8197
Epoch 78/200
 20/242 [=>............................] - ETA: 0s - loss: 16.7876200/242 [=======================>......] - ETA: 0s - loss: 17.6442
Epoch 00078: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 449us/sample - loss: 17.7358 - val_loss: 17.8181
Epoch 79/200
 20/242 [=>............................] - ETA: 0s - loss: 19.5063180/242 [=====================>........] - ETA: 0s - loss: 17.8593
Epoch 00079: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 486us/sample - loss: 17.7354 - val_loss: 17.8157
Epoch 80/200
 20/242 [=>............................] - ETA: 0s - loss: 16.1324200/242 [=======================>......] - ETA: 0s - loss: 17.7082
Epoch 00080: val_loss did not improve from 17.81353
242/242 [==============================] - 0s 463us/sample - loss: 17.7354 - val_loss: 17.8149
Epoch 00080: early stopping
done
